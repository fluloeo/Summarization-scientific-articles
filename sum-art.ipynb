{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/fluloeo/sum-art?scriptVersionId=290819556\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nimport ast\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-08T18:05:48.86075Z","iopub.execute_input":"2026-01-08T18:05:48.861006Z","iopub.status.idle":"2026-01-08T18:05:50.651274Z","shell.execute_reply.started":"2026-01-08T18:05:48.860977Z","shell.execute_reply":"2026-01-08T18:05:50.650333Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/articles/df_dict_test.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"**Роман мне скинул чанки, сделанные парсером, я очень доволен!!**","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/articles/df_dict_test.csv')\ndf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T18:05:54.581566Z","iopub.execute_input":"2026-01-08T18:05:54.581883Z","iopub.status.idle":"2026-01-08T18:05:54.718268Z","shell.execute_reply.started":"2026-01-08T18:05:54.581829Z","shell.execute_reply":"2026-01-08T18:05:54.717531Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"            id                                              title  \\\n0   1602.04402  balanced truncation of linear time-invariant s...   \n1   1611.01462  tying word vectors and word classifiers: a los...   \n2   1611.04496  multi-view recurrent neural acoustic word embe...   \n3   1808.00560  compressible spectral mixture kernels with spa...   \n4   2111.00405  limitations of the macaulay matrix approach fo...   \n..         ...                                                ...   \n95  2307.14341  virtual mirrors: non-line-of-sight imaging bey...   \n96  2307.14354  learned gridification for efficient point clou...   \n97  2307.14362  learnable wavelet neural networks for cosmolog...   \n98  2307.14392  human-centric scene understanding for 3d large...   \n99  2309.03177  3d object positioning using differentiable mul...   \n\n                                             abstract  \\\n0   this paper discusses model order reduction of ...   \n1   recurrent neural networks have been very succe...   \n2   recent work has begun exploring neural acousti...   \n3   spectral mixture (sm) kernels comprise a power...   \n4   recently chen and gao~\\cite{chengao2017} propo...   \n..                                                ...   \n95  non-line-of-sight (nlos) imaging methods are c...   \n96  neural operations that rely on neighborhood in...   \n97  convolutional neural networks (cnns) have been...   \n98  human-centric scene understanding is significa...   \n99  this article describes a multi-modal method us...   \n\n                                            dict_test  \n0   {'I. INTRODUCTION AND PROBLEM FORMULATIONS': '...  \n1   {'INTRODUCTION': \"Neural network models have r...  \n2   {'INTRODUCTION': 'Word embeddings-continuous-v...  \n3   {'Introduction': \"Gaussian processes (GPs) con...  \n4   {'Introduction': 'Solving systems of multivari...  \n..                                                ...  \n95  {'Computed image of': 'T-shaped object from a ...  \n96  {'Introduction': 'Point clouds provide sparse ...  \n97  {'Introduction': 'The process of extracting in...  \n98  {'Introduction': 'Human-centric scene understa...  \n99  {'I. INTRODUCTION': \"Differentiable rendering ...  \n\n[100 rows x 4 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>title</th>\n      <th>abstract</th>\n      <th>dict_test</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1602.04402</td>\n      <td>balanced truncation of linear time-invariant s...</td>\n      <td>this paper discusses model order reduction of ...</td>\n      <td>{'I. INTRODUCTION AND PROBLEM FORMULATIONS': '...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1611.01462</td>\n      <td>tying word vectors and word classifiers: a los...</td>\n      <td>recurrent neural networks have been very succe...</td>\n      <td>{'INTRODUCTION': \"Neural network models have r...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1611.04496</td>\n      <td>multi-view recurrent neural acoustic word embe...</td>\n      <td>recent work has begun exploring neural acousti...</td>\n      <td>{'INTRODUCTION': 'Word embeddings-continuous-v...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1808.00560</td>\n      <td>compressible spectral mixture kernels with spa...</td>\n      <td>spectral mixture (sm) kernels comprise a power...</td>\n      <td>{'Introduction': \"Gaussian processes (GPs) con...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2111.00405</td>\n      <td>limitations of the macaulay matrix approach fo...</td>\n      <td>recently chen and gao~\\cite{chengao2017} propo...</td>\n      <td>{'Introduction': 'Solving systems of multivari...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>2307.14341</td>\n      <td>virtual mirrors: non-line-of-sight imaging bey...</td>\n      <td>non-line-of-sight (nlos) imaging methods are c...</td>\n      <td>{'Computed image of': 'T-shaped object from a ...</td>\n    </tr>\n    <tr>\n      <th>96</th>\n      <td>2307.14354</td>\n      <td>learned gridification for efficient point clou...</td>\n      <td>neural operations that rely on neighborhood in...</td>\n      <td>{'Introduction': 'Point clouds provide sparse ...</td>\n    </tr>\n    <tr>\n      <th>97</th>\n      <td>2307.14362</td>\n      <td>learnable wavelet neural networks for cosmolog...</td>\n      <td>convolutional neural networks (cnns) have been...</td>\n      <td>{'Introduction': 'The process of extracting in...</td>\n    </tr>\n    <tr>\n      <th>98</th>\n      <td>2307.14392</td>\n      <td>human-centric scene understanding for 3d large...</td>\n      <td>human-centric scene understanding is significa...</td>\n      <td>{'Introduction': 'Human-centric scene understa...</td>\n    </tr>\n    <tr>\n      <th>99</th>\n      <td>2309.03177</td>\n      <td>3d object positioning using differentiable mul...</td>\n      <td>this article describes a multi-modal method us...</td>\n      <td>{'I. INTRODUCTION': \"Differentiable rendering ...</td>\n    </tr>\n  </tbody>\n</table>\n<p>100 rows × 4 columns</p>\n</div>"},"metadata":{}}],"execution_count":2},{"cell_type":"markdown","source":"*Пример для одной статьи:*","metadata":{}},{"cell_type":"code","source":"data_dict = ast.literal_eval(df['dict_test'].iloc[0])\nprint(*data_dict.keys(),sep='\\n')\nprint(f'Количество чанков: {len(data_dict.keys())}',end='\\n\\n')\n\nfor key, value in data_dict.items():\n    print(key,end='\\n\\n')\n    print(value,end='\\n\\n')\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T18:29:22.638784Z","iopub.execute_input":"2026-01-08T18:29:22.639589Z","iopub.status.idle":"2026-01-08T18:29:22.645401Z","shell.execute_reply.started":"2026-01-08T18:29:22.639558Z","shell.execute_reply":"2026-01-08T18:29:22.644687Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"I. INTRODUCTION AND PROBLEM FORMULATIONS\nDRAFT\nII. FUNDAMENTAL TOOL\nIV. FREQUENCY-DEPENDENT BALANCED TRUNCATION OVER KNOWN FREQUENCY-INTERVALS\nTheorem 4.4 (Interval-type Frequency-dependent Balanced Truncation):\n, n\nV. EXAMPLES\nIndexes computation formula\nBerlin/Heidelberg, Germany\nКоличество чанков: 9\n\nI. INTRODUCTION AND PROBLEM FORMULATIONS\n\nWe study model order reduction for linear time-invariant continuous-time systems where A ∈ C n×n , B ∈ C n×m , C ∈ C p×n , D ∈ C p×m , x(t) ∈ C n is the state vector, u(t) ∈ C m is the input signal, y(t) ∈ C p is the output signal. Modeling of complex physical processes often leads to large order n. The corresponding high storage requirements and expensive computations make it very difficult to simulate, optimize or even design such large scale systems - . In this case model order reduction (MOR) plays an important role. It consists in approximating the system (1) by a reduced-order system: G r (ω) : 1 Max Planck Institute for Dynamics of Complex Technical Systems, Sandtorstraße 1, 39106 Magdeburg, Germany. * Corresponding author: benner@mpi-magdeburg.mpg.de 2 School of Mechatronic Engineering and Automation, Shanghai University, Shanghai, 200072, P. R. China. where A r ∈ C r×r , B r ∈ C n×m , C r ∈ C p×n , D r ∈ C p×m with r < n. Balanced truncation is a well grounded and the most commonly used model order reduction scheme . The standard form is the so-called Lyapunov balanced truncation, which was first introduced in the systems and control literature by Moore . The prominent advantages of balanced truncation is that it preserves stability and provides an a priori known error bound over the entire-frequency range. In detail, it gives a upper bound of the following entire-frequency (EF) type approximation performance index function In many practical applications, the operating frequency of input signal belongs to a fully or partially known finite-frequency range such as a limited interval (i.e. ω ∈ [̟ 1 , ̟ 2 ]). For those cases, the reduced-order model is only needed to capture the input-output behavior of the original system for input signals with admissible frequency. Correspondingly, good in-band approximation performance is more expected, while the out-band approximation performance might be neglected - . In other words, the objective of finite-frequency (FF) model order reduction is only to minimize the following finite-frequency type performance index function: Since the standard balanced truncation is intrinsically frequency-independent, hereby we will call it as frequency-independent balanced truncation (FIBT) in the sequel, it cannot be used to further improve the in-band approximation performance with pre-known frequency information. To enhance the approximation performance over pre-specified frequency range, several balancingrelated approaches have been developed. Some famous and popular ones include: (1) Singular perturbation approximation (SPA). SPA is a companion balancing-related method of the standard FIBT and is first introduced by Liu and Anderson . Although FIBT and SPA gives same entire-frequency type error bound, the characteristics of them are contrary to each other. The reduced systems generated by FIBT generally have a smaller error at high frequencies, and tend to be larger at low frequencies. In contrast, SPA generally leads to good approximation performance at frequencies around ω = 0 by forcing the transfer function of full order model and reduced order model to be matched exactly at ω = 0 (i.e G(0) = G r (0)). Therefore, SPA is particularly suited for solving model reduction problems in the cases that ω = 0 is pre-known as the dominating operating frequency point ( [10]). To further make the a flexible tradeoff between the local approximation performance over low-frequency ranges and the global approximation performance over entire frequency range, generalized SPA algorithm has been developed by introducing a userdefined adjustable scalar (see Obinata and Anderson ). DRAFT Frequency-weighted balanced truncation (FWBT). In the fields of system analysis and control theory, frequency weighting functions is a conventional tool which has been widely applied for solving various analysis and synthesis problems with pre-known frequency information. For finite-frequency model order reduction problems, utilizing the frequency weighting technique and combing it with the standard balanced truncation method also is very prevailing. During the last three decades, many frequency weighted balanced reduction approaches have been developed (see Enns ; Zhou ; Sreeram ; Ghafoor and Sreeram ; Houlis and Sreeram ; Wang et al ; Sreeram et al and the references therein). The common procedure of FWBT is build frequency-weighted model first by introducing input/out frequency weighted transfer functions and then apply the standard FIBT on the weighted model. Indeed, good frequency-specific approximation performance may be obtained if the selected weighting function is an appropriate one. However, the design iterations to search for an appropriate weighting transfer function can be tedious and time consuming. Besides, FWBT also suffers from the drawback of the increased order of the weighted plant model. (3) Frequency-limited Grammians balanced truncation (FGBT). It was first introduced by Gawronski and Juang in . This methodology stems from the consideration of extending the definition of standard Gramians to the frequency-limited case and then applying the standard balanced truncation procedures to the frequency-limited Gramians ( [21] ). As has been pointed out in , FGBT may be invalid in some cases as the solutions of the \"frequency-limited Lyapunov equations\" cannot be guaranteed to be positive semi-definite, and it provides no error bound. To overcome those drawbacks, several modified FGBT schemes providing error bound have been proposed (Gugercin and Antoulas ; Gahfoor and Sreeram ) A common feature of the those existing finite-frequency balancing-related approaches is that they continue to use entire-frequency type index (3) to evaluate the actually concerned finite-frequency approximation performance (See Table ).\n\nDRAFT\n\nThe task is to build reduced model of order 3 approximating the frequency domain dynamic behaviors of the original model well in the neighborhood of ̟ = 0. Among the existing balancingrelated methods, the (generalized) SPA is the most suitable one for coping with this kind of model reduction problems. At the same time, our proposed SF-type FDBT method can also be applied for this kind of problems. The sigma plots of error systems generated by generalized SPA and SFtype FDBT are depicted in Fig. and Fig. , respectively. As Fig. and Fig. shown, both of them could gives rise to small approximation error around ̟ = 0. Moreover, one can make a tradeoff between the local approximation performance and global approximation performance by adjusting the the user-defined parameter (ρ for generalized SPA and ǫ for SF-type FDBT). In this example the generalized SPA and the SF-type FDBT performs very similar with each other, however, huge variety on their performance may occurred in some cases (see example 3 in the below, in which only the SF-type FDBT is effective). Among the existing balancing-related methods, FGBT is the exact one developed for solving such interval-type finite-frequency model reduction problems. Our proposed interval-type FDBT is also aimed to solve this kind of problems. We will show the differences between them by this example. The sigma plot of error models and the corresponding error bound are given in the Fig. -Fig. , by which the most striking difference on the type of error bounds can be illustrated. The FGBT provides error bound over entire-frequency range, in contrast, the intervaltype only provides error bound over the pre-specified frequency interval. Since it is assumed that the operating frequencies belong to the given intervals, the interval-type error bounds are adequate for approximation performance estimation. Compared with the standard FIBT, both the FGBT and the interval-type FDBT are effective in improving the approximation performance over specified DRAFT frequency interval. At the same time, the interval-type FDBT has the advantage that it gives rise to better approximation performance and smaller error bound simultaneously. As referred to in Remark 4, the interval-type FDBT always provides small error bound as long as the size of frequency interval is small enough. To show this, a randomization experiment was carried out. We randomly generate 100 stable systems with order 4. (The off-diagonal elements of matrix A and each element of the matrices B, C, D are obtained with a zero mean and unitary DRAFT variance normal distribution, the diagonal element of matrix A are obtained with mean -5.5 and variance 4.5). To compare the average performance between FGBT and interval-type FDBT, several indices are defined in Table .\n\nII. FUNDAMENTAL TOOL\n\nThe Kalman-Yakubovich-Popov (KYP) Lemma is a cornerstone in system and control theory. In fact, the EF-type error bound provided by the standard FIBT can be proofed and interpreted with the aid of KYP Lemma . In , Iwasaki and Hara successfully generalized the KYP Lemma from entire-frequency case to finite-frequency cases. The Generalized KYP Lemma plays a fundamental role in our developed and it is included here. Lemma 2.1 , Generalized KYP lemma): Consider a continuous-time system (1), the following statements are equivalent: (1) The frequency domain inequality (2) There exist symmetric matrices P and Q of appropriate dimensions, satisfying Q > 0 and G ǫ̟ (ω) : DRAFT where ǫ > 0 is a user-specified scalar. It should be pointed out that ǫ should be a scalar satisfying the condition: ǫ = −(̟ − λ i ) to ensure the invertibility of (ǫI + ̟I − A), where λ i , i = 1, ..., n denote the eigenvalues of the matrix A. Proposition 3.2: For a given system (1), the corresponding SF-type frequency-dependent extended system (7) can be obtained by applying a particular Moebius transformation as follows: Proposition 3.3: The following statements are true: a). If the original system (1) is Hurwitz stable and ǫ > 0, then the corresponding SF-type frequencydependent extended system is stable. b). Given the original system (1) is unstable and denote the unstable eigenvalues of A as λ + i , i = 1, ..., n u , then the corresponding SF-type frequency-dependent extended system is stable if the value of ǫ satisfying 0 < ǫ < min(ǫ + i ), i = 1, ..., n u , where ǫ + i = (̟ − Im(λ i )) 2 /Re(λ i ) + Re(λ i ). Proof: a). Let us denote λ i , i = 1, 2..., n, and λ ǫi (̟), i = 1, 2..., n as the eigenvalues of the matrices A and A ǫ (̟), respectively. According to the mapping between A and A ǫ (̟) given in , we know that Noticing that Re(λ i ) < 0 if the system G(ω) is stable, then the following inequalities hold if ǫ > 0. Thus the proof is completed. b). Denote λ + ǫi (̟), i = 1, ..., n u as the eigenvalues of A ǫ (̟) mapped from λ + i , i.e. then it can be concluded that Re(λ + ǫi (̟)) < 0, i = 1, ..., n u for all ǫ satisfying 0 < ǫ < min(ǫ + i ), i = 1, ..., n u , according to the computational formula . Thus the proof is completed. Definition 3.4 (SF-type Frequency-dependent Lyapunov Equations): Given a linear continuoustime system (1) and one of its corresponding Hurwitz stable SF-type frequency-dependent extended systems , then the following two Lyapunov equation DRAFT are defined as SF-type frequency-dependent controllability and observability Lyapunov equations of the continuous-time system . Furthermore, the solutions W cǫ (̟) and W oǫ (̟) will be referred to as SF-type frequency-dependent controllability and observability Gramians of the continuous-time system (1). Definition 3.5 (SF-type Frequency-dependent Balanced Realization): Given a linear continuoustime system (1) and one of its Hurwitz stable SF-type frequency-dependent extended systems , the corresponding SF-type frequency-dependent controllability and observability Gramians are equal and diagonal, i.e. the following Lyapunov equations simultaneously hold, then this particular realization will be referred to as a SF-type frequencydependent balanced realization Proposition 3.6: Suppose the given system ( ) is stable and let W c , W o , Σ denote its standard controllability and observability and balanced Gramian matrices, then the following statements are true: Proof: a). It is well known that the standard controllability and observability Gramian matrices W c , W o of system (1) satisfy the following standard frequency-independent Lyapunov equations: Post-and-pre multiply the SF-type frequency-dependent Lyapunov equations ( ) by ǫ −1 (ǫI + ̟I − A), then we have Furthermore, the following equations can be derived by subtracting the equations ( ) from ( ) It is easily to conclude that DRAFT Thus the proof is completed. b). The SF-type frequency-dependent Lyapunov equations ( ) can be rewritten as: thus one can conclude that: Thus the proof is completed. 3). It can be easily observed that the ̟-dependent matrices A ̟ , B ̟ , C ̟ will recover A, B, C as ǫ → ∞, i.e. Then it is trivial to conclude that Theorem 3.7 (SF-type Frequency-dependent Balanced Truncation): Given a linear continuoustime system (1) and the pre-known dominating operating frequency point ω = ̟, then for any one of its Hurwitz stable SF-type frequency-dependent extended systems given in SF-type frequency-dependent balanced realization with respect to the SF-type frequency-dependent Gramian and given by: where Z r = [I r×r 0 r×(n−r) ]. Furthermore, the truncated model G r (ω) possesses the following DRAFT properties: 1). The approximation error between the original system model ( ) and the truncated r th reduced model at the given frequency point ω = ̟ satisfies the following SF-type error bound: 2). The approximation error between the original system model ( ) and the truncated r th reduced model ( ) over entire frequency range satisfies the following EF-type error bound: where G rǫ̟ (ω) : Proof: 1). The detailed proof for r = n − 1 case will be provided in the sequel, and the r = n − 2, ...1 cases can be easily completed step by step. The error system model between the original high-order system model G(ω) and the truncated From the error system E n (ω), we can construct a dilated system E n (ω) as follow: DRAFT where B dn , C dn , D 12 dn , D 21 dn , D 22 dn are auxiliary 'dilated' matrices, and those matrices are constructed as follows: Defining the Lyapunov variable Q en = Q * en ≥ 0 and P en = P en as follows: Substituting the above constructed Lyapunov variable Q en , P en into the following SF-type matrix inequality suggested by the Generalized KYP Lemma, Combing the balanced SF-type frequency-dependent Lyapunov equations , one can derive the following equations: DRAFT where According to the Generalized KYP Lemma, the dilate error systems E n (ω) satisfying DRAFT therefore the error system E n (ω) satisfying This completes the SF-type error bound for the r = n − 1 case. The remainder of the proof for the r = n − 2, ...1 cases can be easily completed in a reciprocal way. 2). From ( ) and ( ), it can be concluded that the SF-type frequency-dependent extended system G rǫ̟ (ω) of reduced system G r (ω) can be obtained by applying the standard FIBT algorithm for G ǫ̟ (ω). Therefore, we have Noting that Using triangle inequality we get This completes the proof of entire-frequency error bound . Based on above preliminaries and results, we now at the stage to present the SF-type frequencydependent balanced truncation algorithm (see Algorithm 1). Remark 3.8: According to Proposition 3, the SF-type error bound can be regulated to an arbitrary small value by decreasing the parameter ǫ, in other word, arbitrary approximation accuracy at the given frequency point ω = ̟ can be achieved. To make the approximation performance over the neighboring intervals (ω ∈ [̟ − δ, ̟ + δ]) be satisfactory, the value of parameter ǫ should be selected carefully. One possible way to pick an appropriate value of ǫ is to plot the curves of SFtype error bound and EF-type error bound with respect to the parameter ǫ, then one can choose a proper value ǫ * which make the SF-type and EF-type error bound be traded off against each other. Furthermore, it is suggested to adopt the value of ǫ be smaller than ǫ * if there exists an estimation ( δ) on the size of the uncertain frequency interval (δ). The smaller δ is, the smaller value of ǫ could be. DRAFT Algorithm 1 SF-type FDBT Input: Full-order model (A, B, C, D), frequency (̟), user-defined parameter ǫ and the order of reduced model (r), Step 1. Solve the SF-type frequency-dependent Lyapunov equations (9) Step 2. Get the SF-type frequency-dependent balanced realization of the given system by coordinate transformation: where T ǫ (̟) is a matrix that simultaneously diagonalize the matrices W cǫ (̟) and W oǫ (̟), i.e., Compute the reduced-order model as: Output: Reduced-order model (A r , B r , C r , D r ) Remark 3.9: For the sake of theoretical completeness, the SF-type FDBT approach is developed in a complex setting. The original system matrices and the reduced system matrices are allowed to be complex. In many applications, only real systems are of practical interest. With real model restriction, the proposed SF-type FDBT can only be applied in the case that ̟ = 0. It is easy to find that the involved matrices W cǫ (̟), W oǫ (̟), T ǫ (̟) and the generated reduced model A r , B r , C r , D r are all real if the original system is real and the frequency point is ̟ = 0. In the framework of balancing related methods, the proposed SF-type FDBT is not the only way for solving model order reduction problems assuming the dominating frequency is ̟ = 0. As referred to in Section I, SPA is also regarded as an effective way for improving the approximation performance over lowfrequency ranges. However, it should be noticed that the underlying mechanisms and the algorithms of SPA and SF-type FDBT are totally different. Which one will performs better on low-frequency approximation accuracy improvement depends on the given original system model. From the results of Example 3 in Section 5, to say the least, the proposed SF-type FDBT can be viewed as a new non-trivial alternative option besides SPA. Remark 3.10: It is well-known that the conventional balanced truncation methods (such as the DRAFT above mentioned FIBT, SPA, FWBT and FGBT) are developed for stable systems. To make those methods applicable for unstable system, some techniques like stable part and unstable part decomposition should be combined [27] . According to Proposition 2, one can always find a stable SF-type frequency-dependent extended system by choosing a proper ǫ, even if the given original system is unstable. Thus, the SF-type FDBT can be used for coping with model reduction of unstable systems directly. The corresponding cost is that it cann't guarantee the generated reduced model is stable even if the original system is stable.\n\nIV. FREQUENCY-DEPENDENT BALANCED TRUNCATION OVER KNOWN FREQUENCY-INTERVALS\n\nIn this section, we present our results for the cases that the operating frequency belongs to a pre-known limited interval, i.e. ω ∈ [̟ 1 , ̟ 2 ]. We will present some related definitions first and then show the related results and the interval-type frequency-dependent balanced truncation algorithm. where are defined as interval-type frequency-dependent controllability and observability Lyapunov equa-DRAFT tions of the continuous-time system . Furthermore, the solutions will be referred to as interval-type frequency-dependent controllability and observability Gramians of the continuous-time system (1) Definition 4.3 (Interval-type Frequency-dependent Balanced Realization): Given a linear continuoustime system (1) and a pre-specified frequency interval (ω ∈ [̟ 1 , ̟ 2 ]), the corresponding intervaltype frequency-dependent controllability and observability Gramians are equal and diagonal, i.e. the following Lyapunov equations simultaneously hold, then this particular realization will be referred to as interval-type frequencydependent balanced realization.\n\nTheorem 4.4 (Interval-type Frequency-dependent Balanced Truncation):\n\nGiven a linear continuoustime system (1) with a pre-specified frequency interval (ω ∈ [̟ 1 , ̟ 2 ]), and assume the system is given in interval-type frequency-dependent balanced realization with respect to the interval-type frequency-dependent Gramian: and where Z r = [I r×r 0 r×(n−r) ]. Furthermore, the truncated model G r (ω) possesses the following properties: 1). If the original system is stable then the reduced system is stable. 2). The approximation error between the original system model (1) and the truncated r th reduced model (43) over the given frequency interval (ω ∈ [̟ 1 , ̟ 2 ]) satisfies the following interval-type DRAFT error bound: where and 3). The approximation error between the original system model ( ) and the truncated r th reduced model (43) over entire frequency range satisfies the following EF-type error bound: where G r̟ 1 ,̟ 2 (ω) represents the corresponding interval-type frequency-dependent extended system of reduced system G r (ω), i.e. G r̟1,̟2 (ω) : where Proof: 1) It can be easily completed by the similar procedure adopted in the proof of stability preservation for classic FIBT . 2). Similar with the proof of SF-type error bound provided in Theorem 1, only the sketch of the proof for r = n − 1 case will be given below. We abuse notation a little bit for simplification. The error system E n (ω) between the original DRAFT system model G(ω) and the (n − 1) th order reduced model G n−1 (ω) can be represented by: Based on the error system E n (ω), one can construct a structure-preserving dilated system E n (ω) as follows: where B en , B dn , C en , C dn , N en are defined as ( )-( ). Now, if one choose two symmetrical Lyapunov variables Q en = Q * en ≥ 0 and P en = P * en as follows: According to the Generalized KYP Lemma, the dilated error system E n (ω) satisfies Therefore the error system satisfying the following inequality This completes the proof of interval-type error bound (44) for the r = n−1 case, the r = n−2, ..., 1 cases can be fulfilled step by step. 3). Similar with proof of EF-type error bound provided by SF-type FDBT, the proof of EF-type error bound (51) provided by interval-type FDBT can be completed in the same way. Proposition 4.5: the the following statements are true: DRAFT a). lim\n\n, n\n\nProof: a). It can be easily observed that From the interval-type frequency-dependent Lyapunov equation ( ), we know that which means lim b). Similar with the above proof, we have and Then lim furthermore, one can conclude that there exists a scalar µ < ∞ such that the following inequality holds since the convergence of matrices C ei ,N ei and B ei in cases that ̟ d → 0 are norm bounded. Thus the proof is completed. DRAFT The following equation holds for arbitrarily given invertible matrix T ∈ C n×n Proof: Lets consider the square of matrices of the left side and right side in (66), we have The above equation means that there exist matrices U, V such that where U is the matrix whose columns are eigenvectors of and V is the diagonal matrix whose diagonal elements are the corresponding eigenvalues. Furthermore, one get This completes the proof. With the above preparations, the corresponding interval-type FDBT algorithm (Algorithm 2) can be presented as follows. Remark 4.7: Compared with other balancing-related approaches, the most distinctive feature of the proposed interval-type FDBT method is that it gives an interval-type error bound (44). To the best of our knowledge, it is the first time to provide such an interval-type error bound using the interval-type index (4) in the model order reduction research areas. In particular, as revealed by Proposition 4, the interval-type error bound (44) always tends to be zero while the interval size tends to zero. This property means that the interval-type FDBT generally will gives rise to good in-band approximation performance while provides better in-band error bound simultaneously as DRAFT Algorithm 2 Interval-type FDBT Input: Full-order model (A, B, C, D), Frequency interval (̟ 1 , ̟ 2 ), order of reduced model (r). Step 1. Solve the interval-type frequency-dependent Lyapunov equations (41) Step 2. Get the frequency-dependent realization of the given system by coordinate transformation: where T (̟ 1 , ̟ 2 ) is a matrix that simultaneously diagonalize the matrices W c (̟ 1 , ̟ 2 ) and W o (̟ 1 , ̟ 2 ), i.e., Step 3. Compute the reduced-order model as: Output: Reduced-order model long as the size of frequency interval is small enough. Although the interval-type error bound may be increasing quickly with respect to the size of frequency interval. The interval-type error bound and its property are still appealing from a theoretical viewpoint. Remark 4.8: Again, the interval-type FDBT is also presented in a general form, i.e. the system matrices are allowed to be complex or real and the frequency interval might be symmetrical or asymmetrical. It can be easily verified that the interval-type FDBT will generate real reduced models for real full models if the given frequency interval is symmetrical (i.e ̟ 1 = −̟ 2 ). For applications with real system parameter restriction in asymmetrical frequency interval cases (ω ∈ [̟ 1 , ̟ 2 ]), the interval-type FDBT can also be applied in a conservative way by modifying the frequency as\n\nV. EXAMPLES\n\nExample 5.1: Lets consider a LTI system (1) with the following parameter matrices: Here we assume that the frequency of input signal belongs to an uncertain interval around ̟ = 0.\n\nIndexes computation formula\n\nErr(̟ l , r, FDBT) In Table , ̟ l represents the upper bound of the symmetrical frequency interval, r is the order of reduced model, G l Dr (ω), G l Sr (ω), G l Gr (ω), G l Ir (ω) represent the reduced models of order r generated by interval-type FDBT, SPA, FGBT and the classic FIBT for the l th random model, respectively. Fig. and Fig. display the experiment results on the these indices. Fig. validated that the interval-type error bound provided by interval-type FDBT generally is smaller than the EF-type error bound generated by FIBT and FDBT for the cases that the intervalsize is small enough (about ̟ < 1 in this experiment). Although the advantage on the error bound is restricted for small interval-size cases, it is suggested to take the interval-type FDBT as a feasible option even for medium interval-size cases. According to our experiment, the interval-type FDBT DRAFT generally also gives rise to better in-band approximation performance than FIBT and FGBT for medium interval-size cases (see Fig. for details). As has been pointed out in , approximating the ladder circuit is quite difficult in the framework of balancing related model order reduction approaches since neither the Hankel nor the singular values decay to any extent. In particular, its dynamic behavior over low frequency ranges is too complex to be well approximated due to the special distribution of its poles and zeros. Here we are interested to approximate this circuit in the following cases: Case I: the frequency of input signal belongs to a unknown neighborhood of dominating operating frequency point (̟ = 0). Case II: the frequency of input signal is known to be within the interval (ω ∈ [−0.5, +0.5]). At first, lets consider the case I and apply FIBT and generalized SPA to build reduced models. The frequency response of full model and reduced model of order 181 are shown in Fig. . As indicated by the visual inspections of the frequency response of the reduced vs. the full system from Fig. , the standard FIBT is failed to approximate the dynamic behaviors around ω = 0 even the order of reduced model is 181. Besides, it is surprising and remarkable that the generalized SPA method also failed here. Although the generalized SPA approach generally leads to good approximation performance around ω = 0, it is incapable to cope with this example. Now, lets resort to the proposed SF-type FDBT for dealing with the model reduction problem in case I. Our experiment results show that good approximants can be generated via SF-type FDBT as long as the order of reduced system is larger than 50. The frequency response of the full system and th order RLC Ladder Circuit system and the 181 th order approximants obtained via FIBT and SPA frequency ω 201 th order original RLC Ladder Circuit system 181 th order reduced system via FIBT 181 th order reduced system via generalized SPA (ρ=0) 181 th order reduced system via generalized SPA (ρ=1) 181 th order reduced system via generalized SPA (ρ=10) 181 th order reduced system via generalized SPA (ρ=100)\n\nBerlin/Heidelberg, Germany\n\nSpringer-Verlag 2005 45 2005 SIAM, Philadelphia 2008 Springer-Verlag Berlin, Heidelberg eeding of Joint 48th IEEE Conference on Decision and Control and 28th Chinese Control Conference 2009 77 8 2004 83 1 2010 26 1 1981 50 4 1989 6 2013 eeding of IEEE International Symposium on Computer-Aided Control System Design 2000 eeding of 23rd Conference on Decision and Control Las Vegas, NV, USA 1984 40 10 1995 12 1 1989 44 9 1999 86 5 2013 130 6 2008 54 5 2009 21 2 1990 127 3 2005 99 2014 55 9 2008 1996 Prentice-Hall Upper Saddle River, NJ 28 1 1996 50 1 2005 2013 Max Planck Institute Magdeburg Preprint MPIMD/13-02 9 3 1999 2005. 2005 54 4 2005\n\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"!pip install -U bitsandbytes accelerate transformers -q\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nimport torch\nos.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\"\n)\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-4B-Instruct-2507\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen3-4B-Instruct-2507\",\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    trust_remote_code=True\n)\n\ntorch.cuda.empty_cache()\n!pip install langchain==0.0.208  -q\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n!pip install rouge-score evaluate -q\nfrom rouge_score import rouge_scorer\nimport evaluate\nrouge = evaluate.load('rouge')\n\n!pip install bert-score -q\nfrom bert_score import BERTScorer\n\nscorer = BERTScorer(lang=\"en\", model_type=\"bert-base-multilingual-cased\")\n\n!pip install longdocfactscore -q\nimport nltk\nnltk.download('punkt_tab')\nnltk.download('punkt')\nfrom longdocfactscore.ldfacts import LongDocFACTScore\nldfacts_scorer = LongDocFACTScore(device=device)\nfrom tqdm.notebook import tqdm\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T18:07:36.868062Z","iopub.execute_input":"2026-01-08T18:07:36.868612Z","iopub.status.idle":"2026-01-08T18:10:38.267784Z","shell.execute_reply.started":"2026-01-08T18:07:36.868584Z","shell.execute_reply":"2026-01-08T18:10:38.266788Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m380.9/380.9 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hcuda\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"269a35bdc8784900a378415bd06b9cd2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a28bbb244c641749756129bcde80c84"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d153348f07f496f97bd5e5ab5752756"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4e62cbf900e4e33bc84b7dc3fe49f3e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/727 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"edddf4895fc84a5e9feb76e2341b0262"}},"metadata":{}},{"name":"stderr","text":"2026-01-08 18:08:20.891349: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1767895701.241712      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1767895701.331492      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1767895702.415966      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767895702.415992      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767895702.415995      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767895702.415998      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e897c9ba5d948b68635ce8102c7ae79"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0123f74db6a14b8c8df9eb65c6bb0031"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00003.safetensors:   0%|          | 0.00/3.96G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"efa2b2713f6940f39a169e51147e5b3a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00003.safetensors:   0%|          | 0.00/3.99G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"939101277c3f4edf8b139fd9b14d4fd2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00003.safetensors:   0%|          | 0.00/99.6M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a17d4faa14a246cd9da3133326a0949e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d1f0d31def25403dafe2a58d4571c31b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/238 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e17517ec4d744c878c5feaf62be5b52b"}},"metadata":{}},{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.1/155.1 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m100.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m72.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m94.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.26.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\nsigstore 4.1.0 requires pydantic<3,>=2, but you have pydantic 1.10.26 which is incompatible.\nsigstore-models 0.0.5 requires pydantic>=2.11.7, but you have pydantic 1.10.26 which is incompatible.\nsigstore-rekor-types 0.0.18 requires pydantic[email]<3,>=2, but you have pydantic 1.10.26 which is incompatible.\nydata-profiling 4.18.0 requires pydantic<3,>=2, but you have pydantic 1.10.26 which is incompatible.\nxmanager 0.7.1 requires sqlalchemy==1.2.19, but you have sqlalchemy 2.0.45 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngoogle-colab 1.0.0 requires jupyter-server==2.14.0, but you have jupyter-server 2.12.5 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\njaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\nthinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\nthinc 8.3.6 requires pydantic<3.0.0,>=2.0.0, but you have pydantic 1.10.26 which is incompatible.\nopencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\nopencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\njax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\nlangchain-core 0.3.79 requires pydantic<3.0.0,>=2.7.4, but you have pydantic 1.10.26 which is incompatible.\ncudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ngradio 5.49.1 requires pydantic<2.12,>=2.0, but you have pydantic 1.10.26 which is incompatible.\nalbumentations 2.0.8 requires pydantic>=2.9.2, but you have pydantic 1.10.26 which is incompatible.\nbigframes 2.26.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nopencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\npytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\ngoogle-genai 1.45.0 requires pydantic<3.0.0,>=2.0.0, but you have pydantic 1.10.26 which is incompatible.\npydantic-settings 2.11.0 requires pydantic>=2.7.0, but you have pydantic 1.10.26 which is incompatible.\nmcp 1.18.0 requires pydantic<3.0.0,>=2.11.0, but you have pydantic 1.10.26 which is incompatible.\npylibcudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c4d6d4d592e46948d6fd504a5fcf4ec"}},"metadata":{}},{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"44ff0757623c43aea8566f60efa621b8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"933dd01ddd3c44ebb784f5c22e071f0c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1bea91803086483f855c015fca457801"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de59fca16ef046b5b9f191985b9fb492"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/714M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"888a51efd4144e3ebb2a5a62263c495c"}},"metadata":{}},{"name":"stderr","text":"[nltk_data] Downloading package punkt_tab to /usr/share/nltk_data...\n[nltk_data]   Package punkt_tab is already up-to-date!\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7bd1dcbb6fa242fdbfe9e34bf236d5cb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b4acfef409745699dadde5e41d959f6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eea250a91bcb44c5b33cd6ea90cee047"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e10ff56b2681423e9d22e6a515081189"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8666627a5c734bd7841e767c407ccc40"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d424ac765ee3422385aea3a0f306db68"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/399 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"31380722c881444fa074d52b09b7ef95"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a51c73e7248248cfb654c0b2dc0050a4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"485466a955c741e38cedb82c359030e7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"104056d7cbfe44c086755cbdde93b185"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ade638136ab5468ea89bb3978802d354"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab1ee75fe5984ae39e0edbfc96f37ee6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f85db40bfdf24d2abc571e0d0165f3ba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9acde1e5947b4a4f880260ea01f4096a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"36fd72f0bc814981b9a41f748bdf67ab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4065bb341f64a03badd2a441175b61e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5285afcffd414c70849d73d6875f832e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/1.02G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"12ef640f6be54d2c82c973491e2c64a1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.02G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ecc7caec1da4d15ae00d2a11c1796e0"}},"metadata":{}}],"execution_count":9},{"cell_type":"markdown","source":"Функция для того чтобы отправить инструкцию модели","metadata":{}},{"cell_type":"code","source":"def qwen(article, prompt, max_tokens):\n    torch.cuda.empty_cache()\n    final_prompt = f\"{prompt}:\\n\\n{article}\"\n    messages = [{\"role\": \"user\", \"content\": final_prompt}]\n    model.eval()\n    with torch.no_grad():\n        inputs = tokenizer.apply_chat_template(\n            messages,\n            add_generation_prompt=True,\n            tokenize=True,\n            return_dict=True,\n            return_tensors=\"pt\",\n            truncation=False\n        ).to(model.device)\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=max_tokens,\n            do_sample=False\n        )\n    final_summary = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:], skip_special_tokens=True)\n    return final_summary","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T18:22:36.409112Z","iopub.execute_input":"2026-01-08T18:22:36.409431Z","iopub.status.idle":"2026-01-08T18:22:36.414926Z","shell.execute_reply.started":"2026-01-08T18:22:36.409402Z","shell.execute_reply":"2026-01-08T18:22:36.414206Z"}},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":"Функция для обработки чанков статьи, встроенный text_splitter можно отключить","metadata":{}},{"cell_type":"code","source":"def summarize(article, prompt_0, prompt_1,chunk_size=1500, max_tokens_0=700, max_tokens_1=700, text_splitter = True):\n    torch.cuda.empty_cache()\n    if text_splitter:\n        text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n            tokenizer=tokenizer,\n            chunk_size=chunk_size,\n            chunk_overlap=chunk_size//20,\n            separators=[\"\\n\\n\", \"\\n\", \". \"],\n            keep_separator=True\n        )\n        article_chunks = text_splitter.split_text(article)\n    else:\n        article_chunks = article\n    summaries = []\n    for i, chunk in tqdm(enumerate(article_chunks), total=len(article_chunks)):\n        try:\n            chunk_summary = qwen(article=chunk, prompt=prompt_0, max_tokens=max_tokens_0)\n            print(f\"\\nSummary chunk number {i}\")\n            print(chunk_summary)\n        except torch.cuda.OutOfMemoryError as e:\n            print(f\"⚠️ Ошибка нехватки памяти CUDA, слишком длинный чанк: {str(e)[:100]}...\")\n            break\n\n        summaries.append(chunk_summary)\n        torch.cuda.empty_cache()\n    combined_summary = \"\\n\".join(summaries)\n    \n    try:\n        final_summary = qwen(article=combined_summary, prompt=prompt_1, max_tokens=max_tokens_1)\n    except torch.cuda.OutOfMemoryError as e:\n        print(f\"⚠️ Ошибка нехватки памяти CUDA при итоговой суммаризации: {str(e)[:100]}...\")\n    return final_summary","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T18:43:50.395031Z","iopub.execute_input":"2026-01-08T18:43:50.39555Z","iopub.status.idle":"2026-01-08T18:43:50.402107Z","shell.execute_reply.started":"2026-01-08T18:43:50.395522Z","shell.execute_reply":"2026-01-08T18:43:50.401371Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"rouge_list = []\nbert_list = []\nldfacts_list = []\nsummaries_list = []\n\ndef to_full_text(data_dict):\n    text = ''\n    for key, value in data_dict.items(): \n        text += f\"{key}\\n\\n{value}\\n\\n\"\n    return text\n\n\nfor i in range(5):\n    print(f\"\\n=== Статья {i} ===\")\n    #извлекаем чанки, абстракт, названия разделов\n    article_dict = ast.literal_eval(df['dict_test'].iloc[i]) #словарь для i-той статьи \n    chunked_article = list(article_dict.values()) #список чанков\n    chapters_list = list(article_dict.keys()) #список глав\n    print('Список разделов')\n    display(pd.DataFrame(chapters_list))\n    abstract = df['abstract'].iloc[i] \n    article = to_full_text(article_dict) #полный текст статьи с ключами и значениями словаря\n    \n    print(f\"Токенов в статье: {len(tokenizer.encode('\\n'.join(chunked_article)))}\")\n    prompt_0 = \"You're a science editor. Briefly summarize this fragment of the scientific text in original language. Do not add information that is not in the source texts\"\n    prompt_1 = \"You're a science editor. Based on the following summaries of the parts of the article, create a single, coherent and concise summary of the entire scientific article in original language, highlighting the common goal, methods, key results and conclusion. Do not add information that is not in the source texts\"\n    \n    summary_full = summarize(chunked_article, prompt_0 = prompt_0, prompt_1 = prompt_1, max_tokens_0 = 500, max_tokens_1 = 700, text_splitter = False)\n    print(f'\\nSummary article number {i}')\n    print(summary_full)\n    summaries_list.append({\n        'article_id': i,\n        'original_text': article,\n        'abstract': abstract,\n        'summary': summary_full\n    })\n\n    results = rouge.compute(\n    predictions=[summary_full],\n    references=[article],\n    use_stemmer=True\n    )\n    print(\"ROUGE Metrics:\")\n\n    rouge_dict = {}\n    for key, value in results.items():\n        print(f\"{key}: {value:.4f}\")\n        rouge_dict[key] = value\n    rouge_list.append(rouge_dict)\n\n    _, _, F1 = scorer.score([summary_full], [abstract])\n    bert_f1 = F1.item()\n    print(f\"\\nBERTScore: F1 = {bert_f1:.4f}\")\n    bert_list.append(bert_f1)\n\n    ldfacts_sum = ldfacts_scorer.score_src_hyp_long([article], [summary_full])\n    ldfacts_abs = ldfacts_scorer.score_src_hyp_long([article], [abstract])\n    print(f\"LongDocFACTScore for sum: {ldfacts_sum[0]}\")\n    print(f\"LongDocFACTScore for abstract: {ldfacts_abs[0]}\")\n    ldfacts_list.append({\n        'for_summary': ldfacts_sum[0],\n        'for_abstract': ldfacts_abs[0]\n    })","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T18:44:03.831242Z","iopub.execute_input":"2026-01-08T18:44:03.831831Z","iopub.status.idle":"2026-01-08T19:26:21.205967Z","shell.execute_reply.started":"2026-01-08T18:44:03.831805Z","shell.execute_reply":"2026-01-08T19:26:21.205291Z"}},"outputs":[{"name":"stdout","text":"\n=== Статья 0 ===\nСписок разделов\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"                                                   0\n0           I. INTRODUCTION AND PROBLEM FORMULATIONS\n1                                              DRAFT\n2                               II. FUNDAMENTAL TOOL\n3  IV. FREQUENCY-DEPENDENT BALANCED TRUNCATION OV...\n4  Theorem 4.4 (Interval-type Frequency-dependent...\n5                                                , n\n6                                        V. EXAMPLES\n7                        Indexes computation formula\n8                         Berlin/Heidelberg, Germany","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>I. INTRODUCTION AND PROBLEM FORMULATIONS</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>DRAFT</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>II. FUNDAMENTAL TOOL</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>IV. FREQUENCY-DEPENDENT BALANCED TRUNCATION OV...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Theorem 4.4 (Interval-type Frequency-dependent...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>, n</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>V. EXAMPLES</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Indexes computation formula</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Berlin/Heidelberg, Germany</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":"Токенов в статье: 6903\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/9 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"54df005f5e0e436cbbc42f150df6341e"}},"metadata":{}},{"name":"stdout","text":"\nSummary chunk number 0\nWe study model order reduction for linear time-invariant continuous-time systems with state vector \\( x(t) \\in \\mathbb{C}^n \\), input \\( u(t) \\in \\mathbb{C}^m \\), and output \\( y(t) \\in \\mathbb{C}^p \\), described by matrices \\( A \\in \\mathbb{C}^{n\\times n} \\), \\( B \\in \\mathbb{C}^{n\\times m} \\), \\( C \\in \\mathbb{C}^{p\\times n} \\), \\( D \\in \\mathbb{C}^{p\\times m} \\). Large system orders \\( n \\) lead to high computational and storage costs, making simulation, optimization, and design challenging. Model order reduction (MOR) is thus essential to approximate the system by a reduced-order model \\( G_r(j\\omega) \\) with \\( r < n \\). Balanced truncation, particularly Lyapunov balanced truncation introduced by Moore, is widely used due to its stability preservation and a priori error bounds over the entire frequency range. However, in practical applications where input signals operate within a known finite frequency range \\( \\omega \\in [\\omega_1, \\omega_2] \\), the objective shifts to minimizing a finite-frequency performance index. Standard balanced truncation (called frequency-independent balanced truncation, FIBT) is not suited for this, as it provides entire-frequency error bounds and does not optimize in-band performance. To improve in-band approximation, balancing-related methods have been developed: (1) Singular perturbation approximation (SPA), which matches the transfer functions at \\( \\omega = 0 \\), offering good low-frequency performance; (2) Frequency-weighted balanced truncation (FWBT), which uses frequency weighting functions to improve frequency-specific performance but requires iterative design and increases model order; (3) Frequency-limited Grammians balanced truncation (FGBT), which extends standard Gramians to finite frequencies but may fail due to non-positive semi-definite solutions and lacks error bounds; modified FGBT schemes provide error bounds. A common feature of these finite-frequency approaches is that they still use entire-frequency performance indices to evaluate actual finite-frequency approximation performance.\n\nSummary chunk number 1\nThe task is to construct a reduced-order model of order 3 that well approximates the frequency-domain dynamic behavior of the original model near ω = 0. Among balancing-related methods, generalized SPA is the most suitable for this problem, and the proposed SF-type FDBT method is also applicable. Sigma plots of the error systems from generalized SPA and SF-type FDBT are shown in Fig. and Fig., respectively, both of which exhibit small approximation errors near ω = 0. The local and global approximation performance can be balanced by adjusting the user-defined parameter (ρ for generalized SPA and ε for SF-type FDBT). In this example, generalized SPA and SF-type FDBT perform similarly, though significant differences may occur in other cases (e.g., Example 3, where only SF-type FDBT is effective). Among balancing-related methods, FGBT is the exact method developed for interval-type finite-frequency model reduction, while the proposed interval-type FDBT is also designed for such problems. The key difference lies in the type of error bounds: FGBT provides bounds over the entire frequency range, whereas interval-type FDBT provides bounds only over a pre-specified frequency interval. Since operating frequencies are assumed to lie within the given interval, the interval-type error bounds are sufficient for performance estimation. Compared to standard FIBT, both FGBT and interval-type FDBT improve approximation performance over a specified frequency interval, with interval-type FDBT offering better approximation and smaller error bounds simultaneously. As noted in Remark 4, interval-type FDBT provides small error bounds as long as the frequency interval size is sufficiently small. This is demonstrated via a randomization experiment involving 100 stable systems of order 4, with system matrices generated using normal distributions. Several performance indices are defined in Table to compare the average performance between FGBT and interval-type FDBT.\n\nSummary chunk number 2\nThe Kalman-Yakubovich-Popov (KYP) Lemma is a fundamental tool in system and control theory, enabling the proof and interpretation of error bounds in frequency-domain methods. Iwasaki and Hara generalized the KYP Lemma to finite-frequency cases, forming the Generalized KYP Lemma, which is used here to analyze stability and error bounds in frequency-dependent system extensions. For a continuous-time system, the lemma establishes equivalence between a frequency-domain inequality and the existence of symmetric positive definite matrices satisfying a matrix inequality involving a user-defined scalar ǫ, which must satisfy ǫ = −(j̟ − λ_i) to ensure invertibility of (ǫI + j̟I − A), where λ_i are eigenvalues of matrix A. A Moebius transformation generates an SF-type frequency-dependent extended system. If the original system is Hurwitz stable and ǫ > 0, or if it is unstable and 0 < ǫ < min(ǫ⁺_i), the extended system is stable. SF-type frequency-dependent Lyapunov equations are defined for controllability and observability Gramians, and a balanced realization is defined when these Gramians are equal and diagonal. Propositions show that as ǫ → ∞, the extended system recovers the original system. The SF-type frequency-dependent balanced truncation method reduces system order at a given frequency ̟, with approximation error bounds derived via a dilated error system and the Generalized KYP Lemma. The error bounds are frequency-dependent at ̟ and extend to the entire frequency range via standard FIBT. The algorithm involves solving Lyapunov equations, performing coordinate transformation, and truncating the system. The method works for unstable systems by selecting appropriate ǫ, though the reduced model may not be stable. The approach is valid in complex settings, and under real system and ̟ = 0 assumptions, all matrices remain real. It is distinct from SPA and other balancing methods, offering a new alternative for model order reduction, especially at low frequencies.\n\nSummary chunk number 3\nWe present results for systems with operating frequency in a known interval ω ∈ [̟₁, ̟₂]. First, we define interval-type frequency-dependent controllability and observability Lyapunov equations for continuous-time systems. The solutions are called interval-type frequency-dependent controllability and observability Gramians. Definition 4.3: Given a linear continuous-time system (1) and a pre-specified frequency interval ω ∈ [̟₁, ̟₂], if the interval-type frequency-dependent controllability and observability Gramians are equal and diagonal, and the corresponding Lyapunov equations hold simultaneously, then the realization is referred to as interval-type frequency-dependent balanced realization.\n\nSummary chunk number 4\nGiven a linear continuous-time system (1) with a pre-specified frequency interval (ω ∈ [̟₁, ̟₂]), and assuming the system is given in interval-type frequency-dependent balanced realization with respect to the interval-type frequency-dependent Gramian, where Zᵣ = [Iᵣ×ᵣ 0ᵣ×(n−r)]. The truncated r-th reduced model Gᵣ(jω) has the following properties:  \n1) If the original system is stable, the reduced system is also stable.  \n2) The approximation error over the given frequency interval satisfies an interval-type DRAFT error bound, where and  \n3) The approximation error over the entire frequency range satisfies an EF-type error bound, where Gᵣ̟₁,̟₂(jω) is the interval-type frequency-dependent extended system of the reduced model Gᵣ(jω), defined as:  \nProof:  \n1) Stability preservation follows by a procedure similar to that in the proof of stability preservation for classic FIBT.  \n2) Similar to the proof of SF-type error bound in Theorem 1, only the sketch is given for the r = n−1 case. Using a simplified notation, the error system Eₙ(jω) between the original DRAFT model G(jω) and the (n−1)th-order reduced model Gₙ₋₁(jω) is represented as: Based on this error system, a structure-preserving dilated system Eₙ(jω) is constructed with Bₑₙ, B𝒹ₙ, Cₑₙ, C𝒹ₙ, Nₑₙ defined as in ( )–( ). By choosing two symmetrical Lyapunov variables Qₑₙ = Q*ₑₙ ≥ 0 and Pₑₙ = P*ₑₙ, and applying the Generalized KYP Lemma, the dilated error system satisfies certain conditions, leading to the error system satisfying the inequality, thus proving the interval-type error bound (44) for r = n−1; the cases r = n−2, ..., 1 follow similarly.  \n3) The proof of EF-type error bound (51) follows the same approach as in the proof of EF-type error bound for SF-type FDBT.  \n\nProposition 4.5: The following statements hold: DRAFT  \na). lim\n\nSummary chunk number 5\nProof: It is observed that from the interval-type frequency-dependent Lyapunov equation, the limit implies lim. Similarly, under the same conditions, lim, and thus there exists a scalar µ < ∞ such that the inequality holds due to the norm boundedness of matrices C ei, N ei, and B ei as ̟ d → 0. This completes the proof. For an invertible matrix T ∈ C^n×n, consider the square of matrices on both sides of (66). This implies the existence of matrices U and V such that U consists of eigenvectors of and V is a diagonal matrix with corresponding eigenvalues. From this, it follows that the result is established. The interval-type FDBT algorithm (Algorithm 2) is then presented. Remark 4.7: The proposed interval-type FDBT method features an interval-type error bound (44), which is the first such bound in model order reduction using the interval-type index (4). As shown in Proposition 4, the error bound tends to zero as the interval size tends to zero, indicating good in-band approximation and a reliable error bound. Algorithm 2: Input: Full-order model (A, B, C, D), frequency interval (̟₁, ̟₂), reduced model order r. Step 1: Solve the interval-type frequency-dependent Lyapunov equations (41). Step 2: Obtain the frequency-dependent realization via coordinate transformation: where T(̟₁, ̟₂) simultaneously diagonalizes W_c(̟₁, ̟₂) and W_o(̟₁, ̟₂). Step 3: Compute the reduced-order model as: Output: Reduced-order model. The method provides good in-band approximation when the frequency interval size is small enough, though the error bound may increase with interval size. The error bound and its properties are theoretically significant. Remark 4.8: The interval-type FDBT is presented in general form, allowing complex or real system matrices and symmetrical or asymmetrical frequency intervals. It generates real reduced models for real full models when the frequency interval is symmetrical (̟₁ = −̟₂). For real system parameter constraints in asymmetrical intervals (ω ∈ [̟₁, ̟₂]), the method can be applied conservatively by modifying the frequency.\n\nSummary chunk number 6\nWe consider an LTI system (1) with the given parameter matrices, assuming the input signal frequency lies within an uncertain interval centered at ̟ = 0.\n\nSummary chunk number 7\nErr(̟ l , r, FDBT) — in the table, ̟ l denotes the upper bound of the symmetrical frequency interval, r is the order of the reduced model, and G l Dr (ω), G l Sr (ω), G l Gr (ω), G l Ir (ω) represent reduced models of order r generated by interval-type FDBT, SPA, FGBT, and classic FIBT, respectively, for the l-th random model. Figures show experimental results on these indices. The interval-type error bound provided by interval-type FDBT is generally smaller than the EF-type error bound from FIBT and FDBT when the interval size is small (about ̟ < 1). Although this advantage is limited to small interval sizes, interval-type FDBT is suggested as a feasible option even for medium interval sizes. Experimental results show that interval-type FDBT generally provides better in-band approximation performance than FIBT and FGBT for medium interval sizes. Approximating a ladder circuit is difficult in model order reduction methods due to insufficient decay of Hankel or singular values, especially in low-frequency ranges due to complex pole and zero distribution. The study focuses on two cases: Case I — input signal frequency in an unknown neighborhood of the dominant operating frequency (̟ = 0); Case II — input signal frequency known to lie in [−0.5, +0.5]. For Case I, FIBT and generalized SPA are applied to build reduced models of order 181. Visual inspection of frequency responses shows that standard FIBT fails to approximate dynamics near ω = 0, and generalized SPA also fails despite generally good performance near ω = 0. The proposed SF-type FDBT is applied; experimental results show that good approximations can be generated as long as the reduced model order exceeds 50.\n\nSummary chunk number 8\nThe fragment lists publication details, including publishers, years, titles, and conference names, such as proceedings from IEEE Conference on Decision and Control, Chinese Control Conference, and IEEE International Symposium on Computer-Aided Control System Design, as well as books and preprints from various institutions and years. No additional information is provided beyond the listed data.\nSummary article number 0\nThe article addresses model order reduction (MOR) for linear time-invariant continuous-time systems with large orders, where computational and storage costs hinder simulation, optimization, and design. The primary goal is to construct a reduced-order model of order 3 that accurately approximates the system's dynamic behavior near ω = 0, particularly within a known finite frequency interval. Standard balanced truncation (FIBT) is inadequate for this finite-frequency objective, as it provides entire-frequency error bounds and does not optimize in-band performance. To improve finite-frequency approximation, balancing-related methods such as singular perturbation approximation (SPA), frequency-weighted balanced truncation (FWBT), and frequency-limited Grammians balanced truncation (FGBT) are considered. However, these methods either require iterative design, increase model order, or suffer from non-positive semi-definite Gramians and lack of error bounds.  \n\nThe study introduces an interval-type frequency-dependent balanced truncation (FDBT) method, which extends standard balanced truncation to finite frequency intervals. This method defines interval-type frequency-dependent controllability and observability Gramians, constructs a balanced realization via coordinate transformation, and derives both interval-type and entire-frequency error bounds. The reduced model preserves stability and provides a frequency-dependent error bound over the specified interval [̟₁, ̟₂], with the interval-type error bound tending to zero as the interval size decreases. The method is valid for both stable and unstable systems, works in complex settings, and yields real reduced models when applied to real systems with symmetric frequency intervals.  \n\nKey results show that interval-type FDBT outperforms standard FIBT and FGBT in in-band approximation, especially for medium-sized frequency intervals, with smaller error bounds and better performance near ω = 0. Experimental validation on random stable systems and a ladder circuit demonstrates that interval-type FDBT provides superior low-frequency approximation, particularly when the operating frequency is near zero. The method is distinct from SPA and other balancing approaches, offering a new, theoretically grounded alternative with explicit error bounds tailored to finite-frequency applications.  \n\nIn conclusion, the proposed interval-type FDBT method provides a robust, stable, and performance-optimized approach for reducing high-order systems with known finite-frequency operating ranges, enabling accurate and reliable approximation of dynamic behavior in practical applications.\nROUGE Metrics:\nrouge1: 0.1276\nrouge2: 0.0642\nrougeL: 0.0740\nrougeLsum: 0.1088\n\nBERTScore: F1 = 0.6902\nLongDocFACTScore for sum: -4.1922071530268745\nLongDocFACTScore for abstract: -5.1785284042358395\n\n=== Статья 1 ===\nСписок разделов\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"                                                    0\n0                                        INTRODUCTION\n1   BACKGROUND: RECURRENT NEURAL NETWORK LANGUAGE ...\n2                   AUGMENTING THE CROSS-ENTROPY LOSS\n3       THEORETICALLY DRIVEN REUSE OF WORD EMBEDDINGS\n4                                        RELATED WORK\n5                                         EXPERIMENTS\n6                       MODEL AND TRAINING HIGHLIGHTS\n7   EMPIRICAL VALIDATION FOR THE THEORY OF REUSING...\n8              RESULTS ON PTB AND WIKITEXT-2 DATASETS\n9                                 QUALITATIVE RESULTS\n10                                         CONCLUSION\n11              APPENDIX A MODEL AND TRAINING DETAILS\n12        B METRIC FOR CALCULATING SUBSPACE DISTANCES\n13                                               2001","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>INTRODUCTION</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>BACKGROUND: RECURRENT NEURAL NETWORK LANGUAGE ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>AUGMENTING THE CROSS-ENTROPY LOSS</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>THEORETICALLY DRIVEN REUSE OF WORD EMBEDDINGS</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>RELATED WORK</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>EXPERIMENTS</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>MODEL AND TRAINING HIGHLIGHTS</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>EMPIRICAL VALIDATION FOR THE THEORY OF REUSING...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>RESULTS ON PTB AND WIKITEXT-2 DATASETS</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>QUALITATIVE RESULTS</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>CONCLUSION</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>APPENDIX A MODEL AND TRAINING DETAILS</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>B METRIC FOR CALCULATING SUBSPACE DISTANCES</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>2001</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":"Токенов в статье: 5944\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/14 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"93e11690146f4bd89db326aafa1a029a"}},"metadata":{}},{"name":"stdout","text":"\nSummary chunk number 0\nNeural network models have made significant advances in NLP tasks like speech recognition, sentiment analysis, text summarization, and machine translation. Although recurrent neural networks excel at modeling long-range word dependencies, current RNNLMs rely on a classical classification framework with two key limitations: (1) no natural metric is assumed on output classes, despite the existence of a well-defined metric space in language modeling via word embeddings; and (2) inputs and outputs are treated as isolated, whereas in language modeling they reside in the same space, leading to inefficiency in model complexity and information utilization. To address these issues, we propose a novel loss framework comprising two interlinked improvements: (1) augmenting cross-entropy loss with a term that minimizes KL-divergence between model predictions and a target distribution derived from word embedding similarities; and (2) theoretically analyzing this loss leads to tying input and output matrices by reusing the input word embedding matrix as the output classification matrix. The framework is empirically validated on the Penn Treebank corpus, showing superior performance over conventional models, and confirmed on the Wikitext-2 dataset, demonstrating consistent gains across different datasets.\n\nSummary chunk number 1\nIn any variant of a recurrent neural network language model (RNNLM), the goal is to predict the next word at position t in a sequence of one-hot word tokens (y*₁, ..., y*ₙ) using a function f, without making assumptions about the specific structure of the recurrent units. The loss is primarily the cross-entropy between the model prediction distribution yₜ and the observed one-hot target y*ₜ. Since cross-entropy and Kullback-Leibler (KL) divergence are equivalent when the target is one-hot, the loss can be rewritten as KL divergence. The optimization process is interpreted as minimizing the distance between the model prediction distribution (yₜ) and the empirical target distribution (y*ₜ), which, with sufficient training data, approximates the actual target distribution. In the proposed framework, KL divergence is used instead of cross-entropy due to its intuitive interpretation as a distance between distributions, even though the two are not equivalent in this context.\n\nSummary chunk number 2\nWe propose augmenting the conventional cross-entropy loss with an additional term. The augmented loss includes a hyperparameter α and a modified prediction distribution ŷt, obtained by dividing the logits of the standard prediction distribution y_t by a temperature parameter τ. We define ỹt as a probability distribution estimating the true data distribution (conditioned on word history) such that Eỹt = Ey*t. The goal is to minimize the distribution distance between the prediction and this estimated true distribution. In an ideal case where the true distribution is known (ỹt = Ey*t), stochastic gradient descent updates the logits based on all class labels with non-zero conditional probability, with step sizes proportional to ỹt,i. This update is less noisy than standard training since the target distribution is exact and deterministic, leading to improved supervision unless all examples belong to a single class. A recent work applying this framework estimates ỹ using large models' predictions. We propose estimating ỹ by finding the target word vector u_t and computing the inner product of u_t with all other word vectors to obtain an unnormalized distribution, normalized via softmax with the same temperature τ. This yields a distribution measuring word vector similarity and assigning higher probabilities to similar words. The estimation of ỹ is iterative and initially uninformative, but as training progresses, it is expected to better capture word statistics and provide a more accurate estimate of the true data distribution.\n\nSummary chunk number 3\nWe theoretically analyze the augmented loss under a specific setting: input embedding dimension equals RNN hidden state dimension (dₓ = dₕ), bias b = 0 (yₜ = W hₜ), only augmented loss is used, training loss is zero, and temperature τ is large. At high τ, the augmented loss matches the model’s logits to those of the informative labels ỹ. Using first-order approximation of the exponential function and assuming average zero inner product between uₜ and lⱼ, we show that the loss drives the model logits toward ỹ’s logits. Given zero training loss, gradients vanish, and under full-rank assumptions, the column spaces of W and Lᵀ are equal. Introducing matrix A such that W = LᵀA, we show that the output projection can be realized via the embedding matrix transpose and a linear mapping h → Ah. This reveals that the augmented loss implicitly constrains the output probability space to a subspace defined by the embedding matrix. We propose making this mechanism explicit by setting W = Lᵀ and b = 0 during training, which reduces network size and eliminates redundant work by the augmented loss.\n\nSummary chunk number 4\nSince their introduction, various improvements have been proposed for RNNLMs, such as different dropout methods, novel recurrent units, and pointer networks, but none addressed the loss structure. To the best of our knowledge, our work is the first to introduce a new loss framework. Our approach is related to prior work that estimates a more informed data distribution and augments the conventional loss with KL divergence between model prediction and estimated data distributions; however, that work estimates the data distribution by training large networks and applying it to smaller networks, whereas we improve learning through knowledge transfer within the same network in a self-contained manner. The present work is based on a report published in . We have also encountered a concurrent preprint where authors reuse the word embedding matrix in output projection to improve language modeling, but their approach is purely empirical without theoretical justification. The idea of using shared input and output representations has been explored before, with some language models interpreted as neural networks with shared embeddings; however, such sharing was implicitly built into these models rather than proposed as an improvement over a baseline. Consequently, the potential for improvement via shared representations was not explicitly pursued.\n\nSummary chunk number 5\nIn our experiments, we use the Penn Treebank (PTB) corpus and the Wikitext-2 dataset. PTB is a standard benchmark dataset consisting of 923k training, 73k validation, and 82k test words. The version used selects the most frequent 10k words for the vocabulary, with the rest replaced by an <unk> token. Wikitext-2 is a recent alternative to PTB, containing 2,088k training, 217k validation, and 245k test tokens, with a vocabulary of 33,278 words. Compared to PTB, Wikitext-2 is roughly twice as large in dataset size and three times larger in vocabulary.\n\nSummary chunk number 6\nWe base our baseline model on an LSTM language model. It consists of a 2-layer LSTM with equal hidden units per layer, using three network sizes: small (200 units), medium (650 units), and large (1500 units). Training is performed using stochastic gradient descent with a variant of the dropout method. Further training details are provided in section A of the appendix. The baseline is referred to as variational dropout LSTM (VD-LSTM).\n\nSummary chunk number 7\nIn Section 4, it was shown that the chosen loss augmentation scheme constrains the output projection matrix to be close to the input embedding matrix without explicitly setting W = L^T. To validate this, a 20,000-word sequence from the PTB training set was used to train a 2-layer LSTM with 300 units, using loss augmentation with a parameter β that controls the proportion of augmented loss. The input embedding matrix rows were constrained to have unit norm to ensure stable training. After training, the distance between the subspaces spanned by the input embedding matrix L and the output projection matrix W was measured using a residual norm metric, where 0 indicates identical subspaces and 1 indicates orthogonality. Experiments show that as β increases from 0 to 1, the subspace distance drops rapidly from nearly 1 to around 0.06. When β = 1, the distance remains low even at low temperatures (τ = 2), with further reduction at higher temperatures. These results confirm that the loss augmentation drives W to align with L^T, suggesting that explicitly setting W = L^T is not merely regularization but an optimal choice in the framework. The performance gains from combining the two proposed improvements are empirically unresolved and are addressed in the next section.\n\nSummary chunk number 8\nTo evaluate the effectiveness of proposed improvements, four models are trained for each network size: (1) VD-LSTM, (2) VD-LSTM + augmented loss (AL), (3) VD-LSTM + reused embeddings (RE), and (4) V D-LSTM + AL and RE (REAL). Validation perplexities on the PTB corpus for small (panel a) and large (panel b) networks are shown. All models with AL, RE, or both outperform the baseline. Table compares final validation and test perplexities on PTB and Wikitext-2. For both datasets, AL and RE individually improve performance, with REAL achieving the best results. AL provides greater gains for small networks, as small models are inflexible and benefit from more informative data distribution via AL. For larger datasets, RE outperforms AL, indicating that enforcing proximity between input embedding and output projection spaces is more effective in large models. RE reduces model size while preserving representational capacity. The best model (VD-LSTM+REAL) surpasses previous work using conventional frameworks, including large ensembles. VD-RHN+RE achieves the best performance, improving VD-RHN by 2.5 in perplexity. Small models retrained with initialization from first session show improved performance for non-baseline models. Results on equally sized partitions of Wikitext-2 (1044K and 929K) show AL outperforms RE despite threefold larger embedding size, supporting that AL improves information extraction from data. Table compares performance across models on PTB and Wikitext-2 partitions, consistent with PTB results in terms of training set size.\n\nSummary chunk number 9\nA key feature of the framework is the explicit mechanism that assigns word probabilities based not only on observed output statistics but also on metric similarity between words. In the Penn Treebank, this leads to lower <unk> token probabilities compared to the baseline network, because <unk> is an aggregated token not closely aligned with specific words in embedding space. The same effect is observed for frequent words like \"a\", \"an\", and \"the\", which are not closely related to specific words. Additionally, the model assigns higher probabilities to words nearby the target in embedding space, sometimes predicting semantically related words even when the target is not accurately captured. Examples from the PTB test set are provided in Table 4, comparing 1500-unit VD-LSTM and VD-LSTM +REAL. The prediction performance of VD-LSTM +REAL is similar to that of VD-LSTM +RE for the large network.\n\nSummary chunk number 10\nWe introduce a novel loss framework for language modeling that uses the metric structure of word embeddings to generate a more informed data distribution than one-hot targets, improving learning. Theoretically, the framework allows reusing the input embedding matrix in the output projection layer, reducing trainable parameters. Empirically, we validate this framework on Penn Treebank and Wikitext-2, showing it outperforms conventional methods, with the simple reuse of embeddings sufficient for large networks. The improvements are not limited to vanilla language modeling and apply to tasks like neural machine translation, speech recognition, and text summarization, offering performance gains and reduced parameter count—especially beneficial for large vocabularies.\n\nSummary chunk number 11\nTraining begins with a learning rate of 1, decaying at constant rates of 5, 10, and 1 for small, medium, and large networks respectively, with decay rates of 0.9 and 0.97 for small/medium and large networks. Backpropagation unrolls the network for 35 steps on both PTB and Wikitext-2 datasets. Gradient clipping rescales gradients using global norm: 5 for small and medium, 6 for large networks. Dropout is applied as in Gal, using the same mask across unrolled steps; hidden state dropout masks are tied and shared across layer propagation and input to next layer. Dropout is not applied in the input embedding layer; the same dropout probability is used for inputs and hidden states. For PTB, dropout probabilities are 0.7, 0.5, and 0.35 for small, medium, and large networks; for Wikitext-2, they are 0.8 and 0.6 for small and medium networks. When using augmented loss (AL), temperature τ is set to 20. Empirically, setting α (augmented loss weight) as α = γτ works well; γ is set between 0.5–0.8 for PTB and 1.0–1.5 for Wikitext-2. No sudden performance deterioration is observed with moderate variations in τ or α.\n\nSummary chunk number 12\nThe section describes a metric for computing the distance between two matrices X and Y based on the principle angles between their column subspaces. The method involves three steps: (1) obtaining orthonormal matrices U and V such that span(U) = span(X) and span(V) = span(Y) via QR decomposition; (2) computing the projection S = U U^T V of V onto U, and the residual R = V − S; (3) defining the distance metric as d, where d² = (1/C) trace(R^T R), with C being the number of columns. This distance is expressed as d² = (1/C) Σ sin²(θ_i), where θ_i are the principal angles between the subspaces, corresponding to the singular values of U^T V. The metric is nonnegative, zero only when span(X) = span(Y), symmetric in U and V, and ranges between 0 and 1.\n\nSummary chunk number 13\nThe fragment refers to \"Recurrent highway networks,\" a topic associated with research publications and preprints from various years (1973, 1993, 1997, 2007, 2010, 2013, 2014, 2015, 2016), including arXiv preprints and conference papers such as at the 24th International Conference on Machine Learning (2013), Interspeech (2016), and others. It lists specific dates, conference names, and authors (e.g., Jan Cernocký), but no detailed content is provided.\nSummary article number 1\nThe article proposes a novel loss framework for recurrent neural network language models (RNNLMs) that addresses two key limitations of conventional models: the absence of a natural metric on output classes and the treatment of inputs and outputs as isolated. The framework introduces two interlinked improvements: (1) an augmented cross-entropy loss that minimizes the Kullback-Leibler divergence between model predictions and a target distribution derived from word embedding similarities, thereby leveraging the intrinsic metric structure of language embeddings; and (2) a theoretical and empirical mechanism that ties the output projection matrix to the input embedding matrix by reusing the input embedding matrix as the output classification matrix, reducing model complexity and improving information efficiency. The method is validated on the Penn Treebank and Wikitext-2 datasets, where models incorporating both improvements outperform baseline RNNLMs, with the combined approach (REAL) achieving superior performance, especially in large models where embedding reuse yields significant gains in parameter efficiency and accuracy. The results demonstrate that the proposed framework enhances prediction quality by assigning higher probabilities to semantically similar words and reducing reliance on the <unk> token, while also enabling a reduction in trainable parameters through shared representations. The findings confirm that the loss augmentation improves learning by providing more informative supervision, and that explicit reuse of input embeddings is both theoretically justified and empirically effective, offering a scalable and efficient improvement for language modeling and related NLP tasks.\nROUGE Metrics:\nrouge1: 0.0863\nrouge2: 0.0356\nrougeL: 0.0539\nrougeLsum: 0.0717\n\nBERTScore: F1 = 0.7024\nLongDocFACTScore for sum: -5.553670024871826\nLongDocFACTScore for abstract: -4.481736040115356\n\n=== Статья 2 ===\nСписок разделов\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"                                                  0\n0                                      INTRODUCTION\n1                                      OUR APPROACH\n2   MULTI-VIEW LEARNING OF ACOUSTIC WORD EMBEDDINGS\n3             RECURRENT NEURAL NETWORK ARCHITECTURE\n4                                      RELATED WORK\n5                           EXPERIMENTS AND RESULTS\n6                                              DATA\n7           MODEL DETAILS AND HYPERPARAMETER TUNING\n8                   EFFECTS OF DIFFERENT OBJECTIVES\n9                                         Objective\n10                                           Method\n11                            WORD SIMILARITY TASKS\n12              VISUALIZATION OF LEARNED EMBEDDINGS\n13                                       CONCLUSION\n14                                  ACKNOWLEDGMENTS\n15     Published as a conference paper at ICLR 2017\n16                            A ADDITIONAL ANALYSIS\n17                                     Architecture\n18                                       2014. 2014","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>INTRODUCTION</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>OUR APPROACH</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>MULTI-VIEW LEARNING OF ACOUSTIC WORD EMBEDDINGS</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>RECURRENT NEURAL NETWORK ARCHITECTURE</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>RELATED WORK</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>EXPERIMENTS AND RESULTS</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>DATA</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>MODEL DETAILS AND HYPERPARAMETER TUNING</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>EFFECTS OF DIFFERENT OBJECTIVES</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Objective</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>Method</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>WORD SIMILARITY TASKS</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>VISUALIZATION OF LEARNED EMBEDDINGS</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>CONCLUSION</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>ACKNOWLEDGMENTS</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>Published as a conference paper at ICLR 2017</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>A ADDITIONAL ANALYSIS</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>Architecture</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>2014. 2014</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":"Токенов в статье: 6253\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/19 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"107221d9b1274c67945afb34a136ba4c"}},"metadata":{}},{"name":"stdout","text":"\nSummary chunk number 0\nWord embeddings—continuous vector representations of words—are widely used in natural language processing. They can be learned via spectral methods or neural networks and are often composed to represent phrases, sentences, or documents. While most embeddings capture semantic meaning, few represent the acoustic characteristics of words. This work introduces a multi-view approach to learn embeddings that capture how words sound, using both acoustic and orthographic sequences. By jointly learning embeddings from matched and mismatched acoustic-orthographic pairs using contrastive losses, the method enables direct comparison between acoustic and orthographic embeddings. The approach improves performance in acoustic word discrimination and shows promising results in cross-view tasks and word similarity, offering a more effective alternative to sequence-based methods like dynamic time warping.\n\nSummary chunk number 1\nIn this section, we first introduce our approach for learning acoustic word embeddings in a multiview setting, following a brief review of related methods to provide context. We then discuss the specific neural network architecture used, based on bidirectional long short-term memory (LSTM) networks.\n\nSummary chunk number 2\nPrevious approaches to acoustic word embeddings have focused on single-view classification, where acoustic segments are labeled with words and a classifier is trained to predict labels. This method uses paired data {(x_i, y_i)} and optimizes a classification loss (e.g., cross-entropy) over a shared embedding network f and classifier h. However, this approach struggles with large label sets and unseen words due to insufficient training data. An alternative uses Siamese networks with triplet supervision (\"x₁ is similar to x₂, not to x₃\"), enforcing distance constraints between embeddings to learn discriminative features without explicit labels. This approach handles rare labels better and uses more training examples via triplet generation. The text notes that existing methods treat word labels as discrete classes, ignoring phonetic similarities. To address this, the authors propose a multi-view approach, treating acoustic segments and character sequences as complementary views of pronunciation. They use a multi-view contrastive loss, embedding both views into a common space and enforcing that the distance between a segment and its correct character sequence is smaller than that with a mismatched sequence. The objective includes a margin parameter and considers various triplet constructions. A cost-sensitive variant is introduced, where the margin scales with the edit distance between character sequences (measured by Levenshtein distance), with a threshold t_max and maximum margin m_max. This ensures embeddings respect word similarity. The method is implemented using cosine distance and explores combinations of different objectives.\n\nSummary chunk number 3\nBoth views use sequential inputs, so we implement functions f and g with recurrent neural networks, specifically long-short term memory (LSTM) networks. Recurrent neural networks are state-of-the-art for speech tasks like speech recognition, and LSTM-based acoustic word embeddings achieved the best results in one of our experiments. As shown in Figure , f and g are implemented using multi-layer stacked bidirectional LSTMs. The inputs can be any frame-level acoustic feature representation and character vector representation from the orthographic input. At each layer, two LSTM cells process the input sequence from left to right and from right to left. At intermediate layers, the outputs of the two LSTMs at each time step are concatenated to form the input sequence for the next layer. At the top layer, the last time step outputs of the two LSTMs are concatenated to form a fixed-dimensional embedding of the view, which is then used to compute cosine distances in the objective function.\n\nSummary chunk number 4\nThere is no prior work on multi-view learning of acoustic and character-based word embeddings. Acoustic word embeddings have recently been explored through approaches such as representing speech segments as fixed-dimensional vectors using dynamic time warping (DTW) distances to template words, which outperformed raw DTW distances in word discrimination and were later applied to query-by-example tasks. However, DTW is computationally expensive and relies on non-learned parameters. Other studies used recurrent neural network (RNN) autoencoders to learn acoustic word embeddings, showing better performance than DTW in word discrimination. Autoencoders for both acoustic and written words, along with a model for comparing them, were applied to keyword search tasks. Evaluations of acoustic embeddings in downstream tasks like speech recognition and search are costly and often rely on word discrimination—determining if two speech segments correspond to the same word—as a proxy for query-by-example search. While word discrimination provides fixed word boundaries, prior work has extended these embeddings to non-word segments to improve query-by-example systems without known boundaries. The only prior study explicitly targeting acoustic similarity of character sequences used nearest-neighbor retrieval, phonetic/orthographic similarity, and homophone disambiguation. This text uses similar tasks and acoustic word discrimination for comparison with prior acoustic embedding work.\n\nSummary chunk number 5\nThe ultimate goal is to improve speech systems requiring word-level discrimination, such as speech recognition and query-by-example search. To efficiently evaluate model performance and focus on embedding content, three surrogate tasks are considered based on cosine distance between embeddings. First, acoustic word discrimination involves determining if two acoustic sequences correspond to the same word, using average precision (AP) as the performance metric. In a multi-view setup, embeddings of both acoustic words and character sequences are computed, enabling evaluation of written-spoken word comparisons, such as in spoken term detection. A second task, cross-view word discrimination, assesses whether an acoustic signal matches a written word by comparing embeddings and measuring performance with AP. A third task, word similarity, evaluates whether embedding distances reflect character edit distances, using rank correlation as the metric, applied to both acoustic and character sequence pairs. These tasks are used to evaluate embeddings, with initial exploration focused on acoustic word discrimination, followed by evaluation using cross-view discrimination and word similarity measures.\n\nSummary chunk number 6\nThe same experimental setup and data as in previous studies are used. The task and setup were developed by . Data comes from the Switchboard English conversational speech corpus. Spoken word segments last between 50 and 200 frames (0.5–2 seconds). The train/dev/test splits contain 9971/10966/11024 pairs of acoustic segments and character sequences, corresponding to 1687/3918/3390 unique words. For dev or test sets, all pairs are used to compute AP, resulting in approximately 60 million word pairs. In the acoustic view, the embedding model receives a sequence of 39-dimensional vectors (one per frame) of standard mel frequency cepstral coefficients (MFCCs) and their first and second derivatives. In the character sequence embedding model, the input is a sequence of 26-dimensional one-hot vectors representing each character of the word’s orthography.\n\nSummary chunk number 7\nWe experiment with different neural network architectures by varying the number of stacked LSTM layers, hidden units per layer, and the use of single- or bidirectional LSTM cells. A coarse grid search shows that 2-layer bidirectional LSTMs with 512 hidden units per direction per layer perform well on the acoustic word discrimination task, and this structure is fixed for subsequent experiments. The outputs of the top-layer LSTMs serve as the learned embedding for each view, which is 1024-dimensional when bidirectional LSTMs are used. During training, dropout is applied to the inputs of the acoustic view and between stacked layers for both views. For each training example, a negative example is required; a negative character label sequence is generated by uniformly sampling a word label different from the positive label, and negative acoustic feature sequences are uniformly sampled from differently labeled sequences in the training set. Negative label sampling is performed anew at the beginning of each epoch. Network weights are initialized with values uniformly sampled from [−0.05, 0.05]. The Adam optimizer is used with mini-batches of 20 acoustic segments, and the initial learning rate is tuned over {0.0001, 0.001}. Dropout rates are tuned over {0, 0.2, 0.4, 0.5}, with 0.4 typically performing best. The margin in the basic contrastive loss is tuned over {0.3, 0.4, 0.5, 0.6, 0.7}, with 0.4 and 0.5 yielding the best results. For object 0 with cost-sensitive margin, the maximum margin m_max is tuned over {0.5, 0.6, 0.7} and the threshold t_max over {9, 11, 13}. Each model is trained for up to 1000 epochs, and the model achieving the best AP on the development set is selected for evaluation on the test set.\n\nSummary chunk number 8\nWe presented four contrastive losses and potential combinations in Section 2.1. We explore the effects of these objectives on word discrimination tasks. Table shows the development set AP for acoustic and cross-view word discrimination achieved using the various objectives. We tuned the objectives for the acoustic discrimination task and used the corresponding converged models for the cross-view task. Among simple contrastive objectives, obj 0 and obj 2 (involving only cross-view distances) slightly outperform the others on acoustic word discrimination. The best-performing objective is the \"symmetrized\" objective obj 0 + obj 2, which significantly outperforms all individual objectives and the combination of the four. The cost-sensitive objective is very competitive but falls slightly short of the best performance. We note that a similar objective to our obj 0 + obj 2 was used by for the task of caption-image retrieval, where the authors essentially use all non-paired.\n\nSummary chunk number 9\nDev AP (acoustic) (cross-view)\n\nSummary chunk number 10\nThe table presents final test set average precision (AP) for different approaches to word discrimination. The baseline uses MFCC features with DTW, achieving AP = 0.214. Subsequent methods apply DTW to learned correspondence autoencoder or phone posterior features (AP = 0.469 and 0.497, respectively). Methods based on cosine similarity between acoustic word embeddings are not detailed here. The best-performing method is the multi-view LSTM using objectives obj 0 + obj 2, achieving AP = 0.806 (acoustic) and 0.892 (cross-view). The development set AP does not saturate after 1000 epochs, suggesting room for improvement. Previous work used acoustic embeddings only and did not address joint learning with the text view, thus not applicable to the cross-view task.\n\nSummary chunk number 11\nTable presents results on word similarity tasks, reporting Spearman's ρ (rank correlation) between embedding distances and orthographic edit distances (Levenshtein distance). Results are given for both acoustic and text embeddings. The cost-sensitive objective shows greater improvement on word similarity compared to fixed-margin, despite minimal gains on word discrimination tasks. Acoustic embeddings show a rank correlation of 0.226 with phonetic edit distances, and text embeddings show 0.241, which are close to their orthographic counterparts. The performance of text embeddings is not significantly better than that of acoustic embeddings, even though text embeddings have access to the full text input. This is attributed to the limited variety of similar word pairs in the training data, with fewer than 2% of unique word pairs having an edit distance below 5 characters. The embeddings do not learn to distinguish fine differences in character sequences due to insufficient data. Future work could involve training on datasets with more similar word pairs.\n\nSummary chunk number 12\nFigure presents a 2-dimensional t-SNE visualization of selected acoustic and character sequences from the development set, including both previously seen and previously unseen words. Previously seen words were selected uniformly at random from those appearing at least 15 times in the development set (unseen words are the only six meeting this criterion). The visualization shows that acoustic embeddings are tightly clustered and closely aligned with text embeddings, and that unseen words also cluster nearly as well as previously seen ones. Although the figure illustrates the relationships among multiple acoustic and text embeddings, the words are highly diverse, so no conclusions about word relationships can be drawn. Another visualization explores the text embeddings of development set words ending in \"-ly\", \"-ing\", and \"-tion\", confirming that related words are embedded close together, with suffix-sharing words forming well-defined clusters.\n\nSummary chunk number 13\nWe present an approach for jointly learning acoustic and orthographic word embeddings. This multi-view method improves acoustic embedding performance and enables the same embeddings to be used for both spoken and written queries. Various contrastive objectives are explored, including fixed-margin losses for separating word pairs and a cost-sensitive loss targeting orthographic edit distances. While losses perform similarly in word discrimination, the cost-sensitive loss enhances alignment between embedding and orthographic distances. Future directions include incorporating phonetic pronunciation knowledge in training and evaluation, and extending the approach to train on both word and non-word segments. Visualization via t-SNE shows character sequence embeddings for words with suffixes \"-ly\", \"-ing\", and \"-tion\".\n\nSummary chunk number 14\nThis research was supported by a Google Faculty Award and an NSF grant IIS-1321015. The authors' opinions do not necessarily reflect those of the funding agencies. The study used GPUs donated by NVIDIA Corporation. The authors thank Herman Kamper and Shane Settle for their assistance with the data and experimental setup.\n\nSummary chunk number 15\nA additional analysis explores the effect of network architectures on embedding models. Embeddings are learned using objective obj 0 and evaluated on acoustic and cross-view word discrimination tasks. All models are trained for 1000 epochs, except 1-layer unidirectional models, which converge after 500 epochs. Bidirectional LSTMs outperform unidirectional LSTMs, and two-layer bidirectional LSTMs are significantly better than one-layer ones. No significant improvement is observed with more than two layers. For subsequent experiments, the architecture is fixed at 2-layer bidirectional LSTMs per view. The table shows average precision (AP) on the development set for different architectures. Figure provides precision-recall curves for the best models and a scatter plot of cosine distances between acoustic embeddings and orthographic edit distances.\n\nSummary chunk number 16\nWe investigate the impact of network architectures on embedding models. Embeddings are learned using objective obj 0 and evaluated on acoustic and cross-view word discrimination tasks. Average precisions on the development set are reported in Table . All models are trained for 1000 epochs, except 1-layer unidirectional models, which converge after 500 epochs. Bidirectional LSTMs outperform unidirectional LSTMs, and two-layer LSTMs are significantly better than one-layer LSTMs. No significant improvement is observed with more than two layers. For subsequent experiments, the architecture is fixed at 2-layer bidirectional LSTMs per view.\n\nSummary chunk number 17\nThe table shows average precision (AP) values for acoustic and cross-view word discrimination tasks on the development set, using embeddings learned with objective obj 0 and different LSTM architectures. The results are presented for 1-layer unidirectional, 1-layer bidirectional, and 2-layer bidirectional models. In the figure, precision-recall curves for the best models and a scatter plot of cosine distances between acoustic embeddings and orthographic edit distances are provided.\n\nSummary chunk number 18\nThe fragment consists of a list of dates, conference names, and publication identifiers, likely indicating references or citations in a scientific text. It includes years and names of conferences such as Interspeech, ACL, IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU), IEEE Workshop on Spoken Language Technology (SLT), and others, along with preprints and journal volumes. No specific content or claim is summarized, as the text is a sequence of metadata without a coherent scientific statement.\nSummary article number 2\nThis work proposes a multi-view approach to learn acoustic word embeddings that capture the phonetic characteristics of words by jointly modeling acoustic and orthographic sequences. Unlike prior single-view methods that rely on classification or non-learned distance metrics like dynamic time warping, the proposed method uses bidirectional LSTM networks to learn fixed-dimensional embeddings from both acoustic features (e.g., MFCCs) and character sequences. A multi-view contrastive loss is introduced, enforcing that embeddings of matched acoustic-orthographic pairs are closer than those of mismatched pairs, enabling direct comparison between spoken and written forms. The method includes both fixed-margin and cost-sensitive contrastive objectives, with the latter adjusting the margin based on Levenshtein edit distance to better align embedding distances with orthographic similarity.  \n\nExperiments on the Switchboard corpus show that the multi-view approach significantly outperforms prior methods: the best model achieves 0.806 average precision (AP) in acoustic word discrimination and 0.892 in cross-view discrimination, surpassing DTW-based and autoencoder-based baselines. The cost-sensitive objective improves alignment with edit distances in word similarity tasks, though performance remains limited due to insufficient training data with similar word pairs. Despite this, both acoustic and orthographic embeddings show strong clustering behavior in t-SNE visualizations, with suffix-based words forming coherent groups. The study concludes that joint learning of acoustic and orthographic views enables more effective, discriminative, and transferable word embeddings, offering a superior alternative to sequence-based methods like DTW.\nROUGE Metrics:\nrouge1: 0.0856\nrouge2: 0.0391\nrougeL: 0.0515\nrougeLsum: 0.0743\n\nBERTScore: F1 = 0.7206\nLongDocFACTScore for sum: -4.167446494102478\nLongDocFACTScore for abstract: -4.3108125030994415\n\n=== Статья 3 ===\nСписок разделов\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"                                                    0\n0                                        Introduction\n1                         Background and related work\n2                                  Gaussian processes\n3                            Spectral mixture kernels\n4                                        Related work\n5                                          Motivation\n6   Spectral mixture kernel with dependency structure\n7     Modeling dependency structure using convolution\n8   Time and phase characterized Gaussian spectral...\n9       Time-and phase modulated dependency structure\n10  Spectral mixture with TP modulated dependency ...\n11             Interpretation of dependency structure\n12    Comparisons between the SMD and related kernels\n13  Structure adaptation for the spectral mixture ...\n14  Bootstrap-based hyperparameter initialization ...\n15  Algorithm 2: Bootstrap-based hyperparameter in...\n16  Compressed spectral mixture with dependency st...\n17       Sparse dependency structure and its behavior\n18                                        Experiments\n19                                   Model assessment\n20  Long range interpolation of monthly river flow...\n21  Scalable SMD on large scale multidimensional data\n22                                         Conclusion\n23                                     Acknowledgment\n24                                         Discussion\n25                                     Academic press","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Introduction</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Background and related work</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Gaussian processes</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Spectral mixture kernels</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Related work</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Motivation</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Spectral mixture kernel with dependency structure</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Modeling dependency structure using convolution</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Time and phase characterized Gaussian spectral...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Time-and phase modulated dependency structure</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>Spectral mixture with TP modulated dependency ...</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>Interpretation of dependency structure</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>Comparisons between the SMD and related kernels</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>Structure adaptation for the spectral mixture ...</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>Bootstrap-based hyperparameter initialization ...</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>Algorithm 2: Bootstrap-based hyperparameter in...</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>Compressed spectral mixture with dependency st...</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>Sparse dependency structure and its behavior</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>Experiments</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>Model assessment</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>Long range interpolation of monthly river flow...</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>Scalable SMD on large scale multidimensional data</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>Conclusion</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>Acknowledgment</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>Discussion</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>Academic press</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":"Токенов в статье: 8151\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/26 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aba105c5aa8a46039f5b0ccb64c4f8ef"}},"metadata":{}},{"name":"stdout","text":"\nSummary chunk number 0\nGaussian processes (GPs) are Bayesian nonparametric models that model functions using a Gaussian prior and compute the posterior distribution given observations. They can learn function approximations well with sufficient data and avoid overfitting with limited data. The choice of kernel function—reflecting the system’s autocovariance—is crucial for GP performance, as it determines the model’s representation ability and posterior behavior. Typically, kernel selection relies on expert knowledge and empirical analysis. While several base kernels and their combinations exist for applications like RSS-based modeling and traffic prediction, automatic and generalized kernel design is needed to reduce human intervention. The spectral mixture (SM) kernel, which models a signal’s spectral density with a sum of Gaussians, has been successfully applied in various domains. Recent advances focus on extending SM kernels rather than modeling latent dependencies between components. Variants such as the SM product (SMP), nonstationary SM (NSM), and grid SM (GSM) kernels enable modeling of image, spatial, and input-dependent data, but they do not explicitly represent dependencies between components. This paper introduces a new SM kernel with dependency structure (SMD), modeling inter-component dependencies via a generalized cross covariance derived from Bienaymé’s identity. A complex-valued Gaussian mixture model incorporating time and phase delays is used to characterize the spectral density. The resulting SMD kernel is positive definite and interpretable, with its spectral density, covariance, and sampling path providing insight into dependencies. A structure adaptation (SA) algorithm is proposed, including hyperparameter initialization via bootstrapping, component compression, and sparsification of dependencies, to improve learning efficiency and interpretability while preserving dependency structures. The SA algorithm prevents hyperparameter space expansion and retains meaningful dependencies. A dependency intensity measure γ_ij is introduced. The SMD kernel generalizes the original SM kernel: when only autocovariance is considered, it reduces to SM. The paper presents new contributions: (1) representation of dependency structure as cross covariance; (2) complex-valued GMM with time and phase delays for spectral density modeling; (3) interpretable and more expressive SMD kernel; (4) effective and interpretable SA algorithm; (5) evaluation of interpolation, extrapolation, scalability, dependency structures, compression ratio (CR), and sparsity ratio (SR) on synthetic and real datasets. The paper is structured as follows: Section 2 covers background and related work; Section 3 presents motivation; Section 4 introduces SMD; Section\n\nSummary chunk number 1\nIn this section, we review Gaussian processes, squared-exponential kernels, and related works.\n\nSummary chunk number 2\nA Gaussian process (GP) models a function map \\( y = f(x) + \\epsilon \\) with noise \\( \\epsilon \\), using a prior distribution defined by its mean function \\( m(x) \\) and covariance function \\( k(x, x') \\). The GP is expressed as \\( f(x) \\sim \\text{GP}(m(x), k(x, x')) \\). The covariance function, kernel, or kernel function are used interchangeably. Given a kernel function \\( k(x, x') \\) and training data \\( D = \\{X, y\\} \\), the predictive mean \\( \\tilde{y}^* \\) and variance \\( V[y^*] \\) for test data \\( X^* \\) are analytically derived from the posterior distribution \\( p(y^* | X^*, X, y) \\sim N(\\tilde{y}^*, V[y^*]) \\). The kernel has hyperparameters \\( \\Theta \\) that control model complexity. The GP model is optimized by minimizing the negative log-marginal likelihood \\( L_{\\text{NLML}} \\equiv -\\log p(y | X, \\Theta) \\), obtained by marginalizing over the latent function \\( f(X) \\).\n\nSummary chunk number 3\nThe SM kernel is derived using Bochner's Theorem, which states that a function k on R^d is a covariance function of a weakly stationary, mean-square continuous complex-valued process if and only if it can be expressed as k(τ) = ∫_{R^d} e^{2πi s^⊤ τ} dψ(s), where ψ is a positive finite measure. If ψ has a density k(s), then k is the SD (power spectrum) of the kernel. k and k form a Fourier transform pair, with F_{τ→s} and F_{s→τ} denoting the forward and inverse Fourier transforms, respectively. The SM kernel is originally constructed by approximating the underlying SD as a mixture of Q Gaussians in the frequency domain. It can approximate any stationary kernel with sufficient Gaussian components. The SM kernel is obtained via inverse Fourier transform as k_SM,i(τ) = F_{s→τ}[k_SM,i(s)](τ), where k_SM,i(τ) is the i-th SM component, and w_i, μ_i, and Σ_i are the signal magnitude, center frequency, and frequency bandwidth parameters, respectively.\n\nSummary chunk number 4\nThere is a rich literature on GPs related to SM kernel function design and analysis. This section focuses on the family of SM kernels and new variants. Similar to the compositional form of the SM kernel, additive GPs implicitly sum over one-dimensional base kernels. Advances include the SMP kernel with k SMP (τ |Θ) = ∏ᵢ=₁ ∏ₚ=₁ k SM (τₚ |Θₚ) and the NSM kernel, which replaces the exponential part of the SM kernel with a non-stationary Gibbs kernel and introduces input-dependent wᵢ(x) and µᵢ(x). Recently, an approach encoding simple dependency structures between SM components was proposed, but most existing variants assume independence among GP components and ignore possible dependency structures. A quantification of dependency structure in GPs was initially proposed, yet no further investigation into modeling such structures exists. Challenges in sparsifying dependency structure and modeling TP delays remain unsolved. One approach fixes µᵢ and Σᵢ of SM, optimizing only wᵢ, potentially leading to a sparse SM kernel. Another method uses a Lévy process to select the number of SM components, but selection is unstable and prone to overfitting. In summary, hyperparameter initialization, model compression, and dependency structure sparsity in SMD remain unexplored.\n\nSummary chunk number 5\nThis section presents a generalized SM kernel with a dependency structure. The original SM kernel is additive, and any function drawn from a GP with the SM kernel can be expressed as a sum of independent Gaussian processes. Using Bienaymé's identity, the generalized covariance is derived, where the autocovariance of each component is given by the weight times the SM kernel. However, the standard SM kernel assumes independence between components (Cov(f SM,i , f SM,j ) = 0 for i ≠ j), which is not justified. The paper instead considers a non-zero covariance between components as a general dependency structure, relaxing the independence assumption.\n\nSummary chunk number 6\nWe propose for the first time an extended SM kernel that includes dependency structure and its TP augmentations.\n\nSummary chunk number 7\nA stationary covariance function \\( k(x, x') \\) on \\( \\mathbb{R}^P \\) can be expressed in convolution form, where \\( \\tau \\equiv x - x' \\) and \\( * \\) denotes convolution. Convolution in the time domain corresponds to multiplication in the frequency domain, leading to the squared form of the i-th SM component, where \\( \\hat{g}_{SM,i}(s) \\) is the SD of the i-th SM basis component. For the dependency structure \\( \\text{Cov}(f_{SM,i}, f_{SM,j}) \\), cross-correlation between functions \\( f_{SM,i} \\) and \\( f_{SM,j} \\) is considered, equal to the convolution of their weighted kernels \\( w_i k_{SM,i} \\) and \\( w_j k_{SM,j} \\). However, directly using the equation yields a kernel that is the inverse Fourier transform of \\( w_i^2 \\phi_{SM,i}^2(s) \\) when \\( i = j \\), differing from the original SM component. To maintain compatibility between dependency structure and SM component when \\( i = j \\), the convolution between basis components \\( g_{SM,i}(\\tau) \\) and \\( g_{SM,j}(\\tau) \\) is used, where \\( g_{SM,i}(\\tau) = \\mathcal{F}^{-1}_{s \\to \\tau}[\\hat{g}_{SM,i}(s)](\\tau) \\). This allows accurate description of \\( \\text{Cov}(f_{SM,i}, f_{SM,j}) \\), which does not introduce additional parameters for the dependency structure.\n\nSummary chunk number 8\nIn signal processing, time delay differences between frequency components characterize their temporal relationship and affect signal shape. Due to the Fourier transform (FT), a signal in the frequency domain has a complex representation with magnitude and phase. Both magnitude and phase of the signal delay (SD) are relevant, as phase differences help understand interference and dependency structures among process components. However, existing methods only capture magnitude differences between signal delay components. Here, we introduce a TP parameterization for the signal delay (SD) of signal delay components (SM) to enhance its representation. Based on FT properties, shifting a signal k(τ) by time delay θ in the time domain corresponds to multiplying a complex exponential in the frequency domain: kθ(s) = e⁻²πiθs k(s). For a phase delay function kϕ(τ) with phase delay vector ϕ, its FT is kϕ(s) = e⁻²πiϕ k(s). For the SMD kernel, time delay θi and phase delay ϕi are directly embedded into the SD kSM,i(s), resulting in a complex-valued TP delay SD function.\n\nSummary chunk number 9\nConsidering the TP-modulated SD function in Eq. ( ) and adopting the squared form in Eq. ( ), we define ĝSMD,i (s) = k₁/² SMD,i (s). The corresponding SD with dependency structure is expressed with the following parameters: cross amplitude, cross Gaussian, cross time delay, and cross phase delay. The cross amplitude a_ij is a normalization constant depending only on the difference between components i and j. Without TP delays, the SD term in Eq. ( ) reduces to ki×j SMD,θ=0,ϕ=0 (s) ≜ ĝSM,i (s) • ĝSM,j (s). Remark 1: According to Eq. ( ), the closer the components are, the larger the weights w_ij, frequency µ_ij, and scale Σ_ij, and the greater the dependency structure in the SMD.\n\nSummary chunk number 10\nBased on the SM kernel, the dependency structure is defined via the inverse Fourier transform, where the Euclidean distance with time delay is used, and the normalization term c_ij = w_ij a_ij (independent of τ) incorporates cross weight and cross amplitude. c_ij reflects the maximum degree of dependency since the exponential term reaches its maximum value of 1. For an SM kernel with Q components, the dependency structure is derived from the symmetric properties of the symmetric distance (SD). The positive semidefinite (PSD) property of the SMD kernel implies that its SD, kSMD(s), is also PSD. For any finite set of non-zero complex vectors [z₁, ..., z_N]ᵀ ∈ C^N×P and s ∈ R^P, the PSD condition holds, thus the SMD kernel is PSD. The dependency structure is represented as Cov(f_SM,i, f_SM,j) = k_i×j SMD(τ). The dependency intensity is quantified by normalizing the structure, resulting in γ_ij ∈ [−1, 1]. When i = j, γ_ij = 1 if w_i k_SM,i(τ) > 0, and γ_ij = −1 if w_i k_SM,i(τ) < 0.\n\nSummary chunk number 11\nIn Fig. , the covariances, standard deviations (SDs), sampling paths, and posterior distributions in terms of amplitude, peak, and trend are shown for the SM (dashed red) and SMD (dashed blue) kernels. Without TP delays, subplots (b) and (g) show that the dependency structure reinforces the magnitudes of SD and covariance in SM but does not significantly alter the decaying behavior of covariance. The frequency domain dependency structure (subplot (g)) is modeled as the intersection (Gaussian, Eq. ( )) of two SM components. With TP delays, the dependency structure can reinforce or weaken both SDs and covariances of the original kernel. Subplots (c) and (e) show that the covariance range of SMD is extended and larger than SM due to time delay. The dependency structure significantly alters the magnitudes and shapes of SDs (subplots (h), (i), (j)) and covariance (subplots (c), (d), (e)), and changes the decaying behavior of covariance, further reducing predictive uncertainties (subplots (m), (n), (o)). Given six observations (black crosses), the learned posterior distribution and sampling paths are shown in subplots (k), (l), (m), (n), (o). Due to the dependency structure, the predictive confidence interval (CI) of SMD (blue shadow) is significantly tighter than that of SM (red shadow).\n\nSummary chunk number 12\nIn Fig. , the covariance differences between the SM and SMD are visualized, with each black solid link representing a covariance structure of the kernel. Circle q_i corresponds to f_SM,i, and the cross connection denotes Cov(f_SM,i, f_SM,j). The SM only considers the autocovariance Cov(f_SM,i, f_SM,i) and ignores inter-component dependencies. Table summarizes the hyperparameter differences between SMD and SM kernels in a P-dimensional input setting. In NSM, each SM hyperparameter is parameterized as a GP with a squared exponential kernel (e.g., weight w_i becomes w_i,x ∼ GP(0, k_SE(x, x′))), resulting in three times more hyperparameters than SM since w_i,x has three hyperparameters. Without TP delays, the hyperparameter space of SMD equals that of SM. Incorporating TP delays in SMD increases gradient computation complexity due to additional TP hyperparameters. Table: Comparison of SMD and other SM kernels in terms of hyperparameters and their counts. For initial large Q in SM and SMD, the number of components retained after compression, Qrest, is much smaller than Q.\n\nSummary chunk number 13\nThe SM kernel has many hyperparameters (size 3Q), complicating inference, learning, and interpretability of GPs. Issues such as hyperparameter initialization and selecting the number of kernel components hinder its use. The SMD kernel faces similar challenges, with dense dependency structures requiring sparsification. A structure adaptation (SA) algorithm is proposed to address these issues and enable efficient inference and interpretable structure discovery. The algorithm uses a standard maximum-likelihood approach for hyperparameter optimization in the pretrain and fine-training stages. It introduces two levels of sparsity: compression of original components and reduction of weak dependency structures. The algorithm proceeds by initializing hyperparameters through M training steps (steps 1–2), pruning unimportant components based on weights (steps 3–10), and sparsifying dependency structures by removing weak ones and fine-tuning the GP (steps 11–12).\n\nSummary chunk number 14\nThe learning of SMD kernels requires a good hyperparameter initialization in high-dimensional spaces. To improve initialization, the structure of empirical spectral densities (SDs) is exploited, leveraging the connection between SDs and kernels via Bochner's Theorem. However, empirical SDs are biased and contain noise and spurious peaks. To filter noise and false peaks, bootstrap sampling with replacement is applied to generate multiple spectral samples S*. A Gaussian mixture model (GMM) is then fitted to these samples to obtain a mixture of Q Gaussians, p(Θ|s) = Σ wi N(s; μi, Σi). A BHI algorithm (Algorithm 2) is proposed for SMD, using B = 100 bootstrap samples, which are generally sufficient for robust estimation. The hyperparameter estimates from this bootstrap procedure serve as initialization. Algorithm 2 is applied before optimization in the pretrain stage (step 1) of Algorithm 1. An illustration of Algorithm 2 is provided in Fig.\n\nSummary chunk number 15\nQinit, B = 100. Output: Hyperparameter initialization Θinit. Many small peaks in empirical SD are filtered in the bootstrap estimation.\n\nSummary chunk number 16\nSetting the number of components (Q) in SM and SMD kernels is challenging, as Q must be specified in advance and fixed during optimization. Inaccurate Q can cause overfitting (large Q) or underfitting (small Q), limiting the practicality of spectral kernels. To adaptively select components, minor ones are pruned based on their weights. In the monthly river flow dataset, components with weights below 1 have negligible amplitude and are considered less important. A pruning strategy (Steps 2–9 of Algorithm 1) removes such low-weight components, reducing the number of hyperparameters to 5Q_rest, where Q_rest is the remaining component count. The compression ratio (CR) is defined as α_CR = 1 − Q_rest/Q_init × 100%, measuring the pruning extent. In extreme cases, α_CR reaches 100% when all components are pruned (Q_rest = 0) or 0% when none are removed (Q_rest = Q_init).\n\nSummary chunk number 17\nWe investigate the sparsity and behavior of dependency structures in SMD. From Eq. (3) and Eq. ( ), there are Q² − Q dependency structures. For components far apart, the intersection of their SDs is near zero, indicating weak dependency. Closer values of µ_i, Σ_i, and w_i between components imply stronger dependency. A binary mask β_ij, determined by w_ij and a_ij, indicates whether to remove a dependency structure between components i and j. The resulting SMD has sparse dependency structures. Fig. shows the contribution of dependency structures to the final covariance even with zero TP delays. When θ ≠ 0 or ϕ ≠ 0, the covariance (in cyan) shifts and is centered at a different position (subplots (c), (d), and (e)). We define an SR (sparsity ratio) of the dependency structure as ×100%, to evaluate sparsity. Remark 3: α_SR is in [0, 1]; it is 1 if there is no significant dependency structure, and 0 if all dependency structures are large.\n\nSummary chunk number 18\nIn this section, the performance of the SMD kernel is thoroughly evaluated and compared with several state-of-the-art kernels on both synthetic and real-world datasets. The baseline kernels include linear (LIN), SE, polynomial (Poly), periodic (PER), rational quadratic (RQ), Matérn (MA), Gabor, and fractional Brownian motion covariance (FBM), as well as underdamped linear Langevin process covariance (ULL) and neural network (NN) kernels, all implemented in the GPML toolbox. The SM, NSM, and SMD kernels use the same number of components Q. In all plots, training data, testing data, SM predictions, SMD predictions, and confidence intervals are displayed in black, green, red, blue, and gray shadow, respectively.\n\nSummary chunk number 19\nWe evaluate the performance of GP models using multiple metrics: generalization performance, 95% confidence intervals to visualize prediction uncertainty, posterior correlation ρ_ij to quantify latent dependencies between SM components, γ_ij to indicate the intensity of learned dependency structure, and CR (α_CR) and SR (α_SR) of the SMD. We demonstrate the SMD's ability to capture delayed dependency structures in a synthetic signal generated from a GP with a hybrid kernel structure (f(x) ∼ GP(0, k_SMD(θ={0.1,0.3}, ϕ={0.1,0.3}) + k_SM)). The signal has Q = 2 components and is sampled over [-10, 10] with added noise. A time series of length 300 is created, with the middle 40% removed as missing testing data (green), and the rest used as training data (black). Both SMD and SM are configured with Q = 5 and identical initial hyperparameters w_i, µ_i, σ_i²; SMD hyperparameters θ_i and ϕ_i are initialized to zero. In Figure , the SMD outperforms SM in prediction accuracy and smoothness of confidence intervals, and better interpolates the missing block. SMD variants without TP delays, with only time delay, or with only phase delay fail to interpolate the missing block effectively. The SMD achieves the lowest MSE compared to SM.\n\nSummary chunk number 20\nInterpolation is evaluated for GP learning using the SMD kernel. The monthly river flow dataset from the Madison River (1923–1960) is used due to its complex patterns: short-term variations, seasonal trends, irregular long-term trends from lunar/solar positions, and noise. The dataset has 456 records; 30% (long-range middle part) is removed for testing, with the rest used for training. Results show both SMD and SM perform well in interpolation, but SMD achieves better performance and confidence. SMD outperforms SM in cross-validation (CR: 38.9% vs. lower) and removes more low-intensity dependencies (SR: 89.3%). Posterior cross covariance between SM components is computed, with ρ_ij indicating dependency; SM shows high and complex ρ_56, while SMD shows structured dependencies via γ_14, with no alignment between components due to separate optimization. For extrapolation, the yearly sunspot dataset (1700–2014) is used, with 315 records, 10% for extrapolation (last 10%), and 20% randomly sampled from first 90% for interpolation. Training uses 70%. Initial Q=20 components and zero time delays are used. SMD shows significant time-phase delay dependency (TP delay) between components 2 and 4, improving model size by 40.7% and dependency structure by 50.3%. Both SMD and SM interpolate missing values well with small CI, but SMD performs better in extrapolation. The results indicate that SMD can perform interpolation and extrapolation equally well for incomplete signals.\n\nSummary chunk number 21\nWe evaluate the scalable SMD on a large multidimensional abalone dataset using automatic relevance determination to remove irrelevant inputs. The FBM, ULL, and NSM kernels are not applicable for multidimensional data (P > 2). Exact Gaussian process inference is computationally expensive, with O(n³) complexity and O(n²) memory, due to the need to compute the inverse and determinant of K + σ²nI. We use stochastic variational inference for scalability. The CRs of both SM and SMD are above 30% due to the SA algorithm, which reduces the hyperparameter space. SMD with SA shows better CR than SM, possibly because SMD's dependency structure offers better representation. SMD requires fewer components than SM to model the underlying function due to its ability to capture latent dependencies. All SMDs show high sparsity in dependency structures (SR), indicating most are small and removable. However, due to at least three times more hyperparameters and overfitting risks, NSM performs poorly on synthetic signals and real-world datasets.\n\nSummary chunk number 22\nWe propose a novel SMD kernel that extends the SM kernel by incorporating TP delayed dependency structures. An interpretable SA algorithm is introduced to effectively initialize hyperparameters, compress components, and automatically obtain sparse dependency structures. Extensive experiments on synthetic and real-life datasets show that the SMD using the SA algorithm can learn TP delayed dependency structures between components and achieve more accurate interpolation and extrapolation, demonstrating significant benefits. Two issues remain: first, the initialization of TP parameters, currently set to zeros, requires more tailored and effective methods; second, sparse or efficient inference, a common challenge in GP methods, needs further improvement for SMD in large-scale data settings.\n\nSummary chunk number 23\nWe thank Elena Marchiori, Twan van Laarhoven, and Perry Groot for their feedback on an earlier version of this work.\n\nSummary chunk number 24\nThe results in Table show that the SMD (θ ≠ 0, ϕ ≠ 0) outperforms other baselines and other SMD variants (θ = 0 or ϕ = 0). Experimental analysis indicates that a GP with SMD kernel can effectively learn signals with dependency structures caused by physical interference. SM and SMDs using BHI perform better than those using random initialization (RI). The proposed BHI in the SA algorithm provides a good starting point for optimization in high-dimensional hyperparameter space.\n\nSummary chunk number 25\nThe fragment consists of dates, publication titles, conference names, and institutional affiliations, likely indicating references or citations in a scientific text. It does not contain a coherent narrative or summary and appears to be a list of publication years, conference names, journal names, publishers, and related details without contextual explanation or original content. Therefore, it cannot be summarized in a meaningful way as it lacks a structured scientific statement or argument.\nSummary article number 3\nThe article proposes a novel spectral mixture kernel with dependency structure (SMD) that extends the classical spectral mixture (SM) kernel by explicitly modeling inter-component dependencies through a generalized cross covariance derived from Bienaymé’s identity. Unlike existing SM variants that assume independence among components, the SMD kernel captures cross-covariances between Gaussian process components using a complex-valued Gaussian mixture model with time and phase delays, enabling the modeling of temporal and phase-dependent interactions. The kernel’s spectral density is parameterized via a complex-valued Fourier transform, incorporating time and phase delays to represent realistic signal dynamics. The resulting kernel is positive definite and interpretable, with its spectral properties providing insight into dependency structures. To address challenges in hyperparameter initialization, component selection, and interpretability, the authors introduce a structure adaptation (SA) algorithm that uses bootstrap-based initialization, component pruning based on component weights, and sparsification of weak dependency structures. This algorithm reduces the hyperparameter space, improves learning efficiency, and preserves meaningful dependencies. The SMD kernel generalizes the original SM kernel, reducing to it when no inter-component dependencies are considered. The method is evaluated on synthetic and real-world datasets—including a monthly river flow and yearly sunspot time series—showing that SMD outperforms SM and other baselines in interpolation, extrapolation, and prediction accuracy, with tighter confidence intervals and better handling of missing data. The model also demonstrates scalability on multidimensional data via variational inference, with significant compression (CR > 30%) and sparse dependency structures (SR > 89%). The key contributions are: (1) a formal representation of dependency structure via cross covariance; (2) a complex-valued GMM with time and phase delays for spectral modeling; (3) an interpretable and expressive SMD kernel; (4) an effective and interpretable SA algorithm; and (5) empirical validation of interpolation, extrapolation, scalability, and dependency sparsity. The study concludes that the SMD kernel enables more accurate and interpretable function approximation by explicitly modeling latent dependencies between components, particularly in time-delayed or physically coupled systems.\nROUGE Metrics:\nrouge1: 0.0957\nrouge2: 0.0447\nrougeL: 0.0547\nrougeLsum: 0.0794\n\nBERTScore: F1 = 0.7107\nLongDocFACTScore for sum: -4.881683869795366\nLongDocFACTScore for abstract: -4.574126243591309\n\n=== Статья 4 ===\nСписок разделов\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"                                                    0\n0                                        Introduction\n1            Macaulay linear systems and their tQLScn\n2                             Macaulay linear systems\n3   Lower bound on the truncated QLS condition num...\n4                    Comparison to brute-force search\n5   The Boolean Macaulay linear system and its tQLScn\n6                  The Boolean Macaulay matrix over C\n7   Definition 5.1. The Boolean Macaulay matrix B ...\n8                         Lower bound on the tQLScn κ\n9                     Details comparing running times\n10                                      The algorithm\n11                                         Discussion\n12                  B Bounds on binomial coefficients\n13                                   Acknowledgements\n14         A Simple proof of the unique solution case\n15                                                 11","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Introduction</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Macaulay linear systems and their tQLScn</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Macaulay linear systems</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Lower bound on the truncated QLS condition num...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Comparison to brute-force search</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>The Boolean Macaulay linear system and its tQLScn</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>The Boolean Macaulay matrix over C</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Definition 5.1. The Boolean Macaulay matrix B ...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Lower bound on the tQLScn κ</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Details comparing running times</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>The algorithm</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>Discussion</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>B Bounds on binomial coefficients</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>Acknowledgements</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>A Simple proof of the unique solution case</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>11</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":"Токенов в статье: 13917\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/16 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19a1fc58393541fda5ce3a32c9c11690"}},"metadata":{}},{"name":"stdout","text":"\nSummary chunk number 0\nSolving systems of multivariate polynomial equations over \\( \\mathbb{F}_2 \\) is NP-complete. This problem can be reduced to solving an exponentially large system of linear equations using the Macaulay matrix, which encodes the polynomials and their monomial multiples. The classical approach involves computing a Gröbner basis via triangularization of the Macaulay matrix. Chen and Gao proposed applying the HHL quantum linear system (QLS) algorithm to solve this system, avoiding explicit Gröbner basis computation. They demonstrated that the standard access requirements—efficient preparation of \\( |b\\rangle \\), extraction of the solution, and sparse well-conditioned matrix access—can be satisfied. However, the condition number \\( \\kappa \\) of the resulting matrix was not analyzed, and its exponential growth in worst-case instances suggests exponential time complexity for the quantum algorithm. The authors prove an exponential lower bound on \\( \\kappa \\) for Boolean polynomial systems, showing that the original HHL algorithm requires exponential time in the worst case. They also introduce a Grover-based brute-force algorithm that outperforms the quantum algorithm in cases with a unique solution or solutions of equal Hamming weight, achieving time \\( O(n^h) \\) versus quantum time \\( O(\\kappa) \\). A refined version of the algorithm uses a \"Boolean Macaulay matrix\" over \\( \\mathbb{C} \\), derived via Gaussian elimination, which is smaller and preserves the solution set. This leads to a lower bound of \\( \\Omega(2^{h/2}) \\) on the condition number, improved from \\( (3n)^{h/2} \\). The authors also define a truncated QLS condition number \\( \\kappa_b(A) \\), which bounds the performance of truncated algorithms and is also lower-bounded. These bounds rule out improvements via truncated QLS algorithms. The refined algorithm achieves a lower bound of \\( \\Omega(2^{h/2}) \\), which is polynomial when \\( h = \\Theta(\\log n) \\), leaving open the possibility of superpolynomial quantum speedup. The authors provide a self-contained proof of correctness using affine hashing and the Valiant-Vazirani method, reducing multi-solution systems to unique ones. They also present an improved solution extraction method, requiring \\( O(\\log n) \\) iterations instead of \\( O(n) \\), with total cost \\( O(n \\log n\n\nSummary chunk number 1\nWe define the Macaulay linear system of a set of polynomials F ⊆ C[x₁, ..., xₙ] and show that when F has a unique solution, the condition number of the matrix is Ω(nh), where h is the Hamming weight of the solution. This lower bound holds even when using max degree instead of total degree. As a result, the quantum algorithm proposed in the literature for solving polynomial equations via the QLS algorithm to solve a Macaulay linear system requires time Ω((3n)^(h/2)). If there are t different solutions with the same Hamming weight h, the lower bound on the condition number can decrease by at most a factor of √t. We also provide a formula for a lower bound on the condition number for any number of solutions and present computational evidence that this analytical lower bound is exponentially large in terms of the smallest Hamming weight among the solutions.\n\nSummary chunk number 2\nA well-known approach to solving polynomial systems involves linearizing them by introducing auxiliary variables, resulting in a linear system whose matrix is called the Macaulay matrix. Each row of the Macaulay matrix is labeled by a pair of polynomials (m, f) and contains the coefficient vector of the product mf, with rows corresponding to all f ∈ F and monomials m such that mf has degree at most d. Columns are labeled by monomials of degree at most d, ordered by a monomial ordering. The entry in row (m, f) and column m′ is the coefficient of m′ in mf. The degree can be interpreted as total degree or max degree (maximum degree of any variable), with the max degree version used in this text. The size of the Macaulay matrix depends on the selected degree; for m quadratic polynomials in n variables, the degree is at least n√m and at most cαn when m = αn. In the quantum setting, the goal is to compute monomials up to a certain degree. The paper provides an upper bound on the degree for any quadratic polynomials. Row operations on the matrix correspond to polynomial operations in the ideal ⟨F⟩ and preserve common roots. Classically, Gaussian elimination on the matrix yields the Gröbner basis, enabling solution recovery. In the quantum case, direct Gaussian elimination is not feasible; instead, Chen and Gao use the QLS algorithm to sample from nonzero solutions of the linear system M⃗y = ⃗b. It is assumed without loss of generality that ⃗b = [1 0]ᵀ, corresponding to the constant monomial. Chen and Gao set the max degree to 3n and show that if F has a unique solution, the linear system also has a unique solution. The QLS output state |ŷ⟩ corresponds to this unique solution, which can be measured to obtain the solution of F. Classically, the solving degree is at most n+2, allowing Gröbner basis computation via Gaussian elimination. However, for systems with multiple solutions, the affine subspace of linear system solutions has no known structure, and the QLS algorithm outputs a state corresponding to the smallest ℓ²-norm solution in this subspace. For such systems, extracting the solution of F from |ŷ⟩ remains non-trivial. Chen and Gao show that M is O(m • #F)-sparse\n\nSummary chunk number 3\nThis section provides a lower bound on the tQLScn κ⃗b(M) of a degree-d Macaulay linear system, which also implies a lower bound on the time complexity of Chen and Gao's algorithm. To establish this, it suffices to lower bound the ℓ₂-norm of the solution vector ⃗y = M + ⃗b. The analysis considers monomial solution vectors corresponding to binary assignments a, where valid monomials are those with exponents in {0,1,…,d}ⁿ and total degree at most d. The number of non-zero coordinates in the solution vector depends on the degree construction (max or total degree). It is shown that the affine subspace of solutions is spanned by monomial solution vectors under certain degree assumptions. Using lemmas on the ℓ₂-norm of vectors in affine hulls—both when solution vectors have equal Hamming weights (Lemma 4.3) and when they are convex combinations with minimal Hamming weight (Lemma 4.4)—a lower bound on ∥⃗y∥ is derived. A geometric lemma (Lemma 4.6) characterizes the shortest vector in an affine subspace via the Gram matrix and shows that the minimal ℓ₂-norm is bounded below by a quantity involving the Gram matrix and the minimal Hamming weight h of solutions. The resulting lower bound on tQLScn is expressed in terms of the (n−h+1)×(n−h+1) minor of the Gram matrix G, which depends on the degree type (max or total degree). The bound is shown to be exponentially large in h for large d, and numerically verified for d = 3n and h up to n = 300.\n\nSummary chunk number 4\nWe show that the lower bound for the running time of the quantum algorithm based on HHL is exponential in the Hamming weight of the unique Boolean solution, and provide strong evidence that this bound also holds when multiple solutions exist. Comparing with classical brute-force search and Grover's algorithm: if the solution's Hamming weight is known, classical search over all n-bit strings of that weight requires O(n^h) time, while Grover search achieves O(√(n^h)) evaluations. Without prior knowledge of the Hamming weight, classical iteration over increasing weights up to h ≤ n/2 requires at most O(√h · n^h) assignments, bounded similarly for decreasing weights when h > n/2. In the quantum case, naive iteration with Grover per weight yields O(h · n^h) complexity. A variant of Grover for unknown-sized search spaces requires only polynomial evaluations. Compared to Theorem 4.5, Grover performs at least as well as HHL when d + h ≥ n (up to lower-order corrections), and outperforms the Chen and Gao algorithm (with max degree d = 3n). When multiple solutions share the same Hamming weight, Theorem 4.5 implies no improvement over Grover search in condition number reduction. In the general case with solutions of different Hamming weights, we establish an exponential lower bound on tQLScn in terms of the smallest Hamming weight solution up to n = 300, suggesting that Chen and Gao's algorithm has a best-case complexity exponentially large in the minimal Hamming weight, making it unlikely to outperform brute-force Grover search.\n\nSummary chunk number 5\nWe present an equivalent but more efficient representation of the Macaulay matrix by exploiting the search for 0/1 solutions in C, yielding a smaller lower bound on the tQLScn of size Ω(2^h/2). Although the quantum algorithm's running time remains exponentially large for large Hamming weight solutions, for Hamming weight h = Θ(log n), this lower bound allows for a potential quasipolynomial speedup compared to classical brute-force search with running time O(n^h). The approach uses a sequence of upper bounds on Hamming weights, expanding the search space in each iteration by a bounded multiplicative factor in [c, C] ⊂ (1, ∞), enabling the claimed running time bound.\n\nSummary chunk number 6\nIn this section, the field polynomials \\( F_2 = \\{x_1^2 - x_1, \\dots, x_n^2 - x_n\\} \\) for the field \\( \\mathbb{F}_2 \\) are included together with the input polynomials \\( F_1 \\). Solving the system \\( F = F_1 \\cup F_2 \\) forces the roots to be effectively Boolean, even though the underlying field is \\( \\mathbb{C} \\). This enables replacement of all monomials with equivalent multilinear ones, leading to a reduced Macaulay matrix with a more compact form. This approach, previously used in finite fields where extra equations prevented solutions in field extensions, is adapted here for Boolean solutions over \\( \\mathbb{C} \\). Additionally, the Boolean structure of the solutions allows extraction of Boolean solutions by measuring the quantum state corresponding to solution vectors over \\( \\mathbb{C} \\). A monomial is mapped to its multilinear image via \\( \\psi \\). Lemma 5.3 shows that degrees higher than 1 are redundant, so the matrix only includes rows up to degree 1. The total degree is set to \\( d = n \\), the number of variables. Monomials are denoted by \\( m \\) and \\( m' \\), and multilinear monomials by \\( m, m', m'' \\).\n\nSummary chunk number 7\nThe matrix has rows labeled by pairs of polynomials (m, f), where m is a multilinear monomial and f ∈ F₁, such that ψ(mf) has degree at most d. Columns are labeled by multilinear monomials in x₁, ..., xₙ of degree at most d, ordered by a specified monomial ordering. The entry in row (m, f) and column m′ is the coefficient of m′ in ψ(mf). Compared to the Macaulay matrix, the Boolean Macaulay matrix excludes polynomials of maximum degree at least 2. It is obtained as a submatrix of the Macaulay matrix M of max degree d corresponding to F₁ ∪ F₂ after Gaussian row reduction. When F₁ = ∅, row operations on the Macaulay matrix of F₂ reduce it such that nonmultilinear monomials are transformed into multilinear ones via ψ, and rows corresponding to nonmultilinear monomials are eliminated. The resulting matrix has zero rows removed and the left block reduced to zero via row operations. The Boolean Macaulay matrix B is formed by including ψ(tf) for multilinear monomials t and f ∈ F₁, with row support size at most O(#F₁) and column sparsity bounded by at most 4m • #F₁. The linear system M⃗y = ⃗b is equivalent to the reduced system, and the matrix M is O(m • #F)-sparse, enabling efficient solution via QLS algorithms with runtime depending on the condition number of M.\n\nSummary chunk number 8\nLet \\( \\{x_n\\} \\) be given, and let \\( h \\) be the minimum Hamming weight of the \\( t \\) solutions \\( a_1, a_2, \\ldots, a_t \\). Let \\( \\vec{y}_1, \\vec{y}_2, \\ldots, \\vec{y}_t \\) be the corresponding solution vectors of the Boolean Macaulay linear system \\( M \\vec{y} = \\vec{b} \\) under the assignments \\( a_1, a_2, \\ldots, a_t \\) respectively. In this case, \\( \\|M\\| \\geq 1/2 \\) since \\( M \\) has at least one matrix element with absolute value at least \\( 1/2 \\). Analogously to Theorem 4.5, we get:\n\nSummary chunk number 9\nThe classical brute-force algorithm considers all $ n^j $ choices of locations for the 1's in the solution for each $ j \\leq h $, with a running time bounded by $ O(\\sqrt{h} n^h) $. Comparing this with the tQLScn lower bound, it is shown that the Gorver-enhanced brute-force search outperforms the Macaulay matrix approach when there is a unique solution and $ d = n $ (or $ d + h \\geq n $), indicating that the quantum algorithm achieves at most a quadratic speed-up. When $ d = 3n $ and following Chen and Gao’s suggestion on max degree, $ \\kappa_b(\\mathbf{M}) \\geq (3n)^{h/2} $, leading to: for $ h = \\Omega(\\sqrt{n}) $, classical brute force is faster than quantum; for $ h = O(\\sqrt{n}) $, the relative speed is unknown. In the Boolean case, with lower bound $ \\kappa_b(\\mathbf{M}) \\geq \\frac{1}{2}(2^h - 1) $, and for $ h = pn $ with $ p \\in (0, \\frac{1}{2}] $, a unique solution $ \\vec{y} = \\mathbf{M}^+ \\vec{b} $ exists for some $ d \\leq n $. The solution vector $ \\vec{y} $ corresponds to multilinear monomials of total degree at most $ d $, with a one-to-one correspondence between subsets of size at most $ d $ from $ U = \\{x_1, \\dots, x_n\\} $ and nonzero entries of $ \\vec{y} $. Given implicit access to matrix $ \\mathbf{M} $ and sparse vector $ \\vec{b} $, the QLS algorithm outputs $ \\vec{y} $ as quantum state $ |\\vec{y}\\rangle $, encoding nonzero entries. Since $ \\vec{y} $ is a 0/1 vector, $ |\\vec{y}\\rangle $ can be represented accordingly. As the only quantum operation is measurement in the computational basis, this corresponds to a classical coupon collector problem, where uniform random subsets are sampled. Thus, when $ 0 \\leq d \\leq \\lfloor |S|\n\nSummary chunk number 10\nWhen a set of polynomials F has a unique solution, Algorithm 1 finds it. If F has multiple solutions, Valiant-Vazirani reduction is applied to obtain a set F with a unique solution. Algorithm 1: Quantum linear system algorithm for F over C  \nInput:  \nStep 1: Apply a quantum linear system algorithm to the Boolean Macaulay linear system M⃗y = ⃗b of total degree n and obtain solution ⃗y in quantum state.  \nStep 2: Measure the quantum state |⃗y⟩ to get outcome |R⟩, then set all variables in R to 1.  \nStep 3: Repeat Steps 1 and 2 O(log n) times, then set all remaining variables a_j = 0.  \nStep 4: Return a.  \nCompared to Chen and Gao's algorithm, Algorithm 1 has two differences: (1) The Boolean Macaulay matrix size is m²n × 2ⁿ, leading to a smaller lower bound of tQLScn and enabling a potential superpolynomial speedup; Chen and Gao's matrix size is (m+n)(3n+1)ⁿ × (3n+1)ⁿ, resulting in a larger tQLScn lower bound that prevents quantum speedup. (2) In Algorithm 1, the polynomial system has a unique solution, so the Boolean Macaulay system remains unchanged across iterations, requiring only O(log n) iterations. Valiant-Vazirani reduction requires O(n) iterations to generate a unique solution with high probability, totaling O(n log n) iterations. In contrast, Chen and Gao's algorithm handles systems with any finite number of solutions, requiring system updates after each iteration and O(n) iterations.\n\nSummary chunk number 11\nThe Boolean Macaulay linear system approach provides a unified framework for studying quantum computation problems such as Factoring, Graph Isomorphism, and Learning with binary errors. The QLS algorithm for this framework is BQP-complete, and since Factoring is in BQP via Shor's algorithm, overcoming the condition number curse of the Macaulay matrix could extend quantum advantages to other problems. An analytical lower bound on the condition number decreases when polynomial systems have multiple solutions, but cryptographic systems typically have one or few solutions, suggesting the QLS algorithm cannot be effectively used to attack cryptosystems via the Macaulay matrix. Adding field equations increases the number of solutions exponentially but does not reduce the length of the shortest vector, as it remains an affine combination with new variables set to zero. Two main approaches to handle ill-conditioned QLSPs—truncated and preconditioned algorithms—are discussed; the lower bound rules out speedup from the truncated algorithm, while further study is needed on preconditioned variants like sparse approximate inverse or circulant preconditioners. The Boolean Macaulay system cannot be dequantized by classical methods due to its full column rank. A variant of the quantum coupon collector problem enables efficient solution extraction under correlated solution patterns, with potential generalizations for cryptographic applications. When the solution’s Hamming weight is logarithmic in the number of variables, a superpolynomial speedup over exhaustive search is not ruled out, though such cases are unlikely in finite field applications due to significant growth in variable count. Row operations on the Boolean Macaulay matrix preserve rank, leading to the conclusion that the system has a unique solution ŷ = M⁺⃗b.\n\nSummary chunk number 12\nIn this appendix, we derive standard bounds on binomial coefficients for completeness. First, we show that for h ≤ n/2, we use the upper bound on binomial coefficients given in [Juk11, Corollary 22.9].\n\nSummary chunk number 13\nA.G. thanks Rachel Player for inspiring discussions. J.L. thanks Eric R. Anschuetz, Yuan Su, Yu-Ao Chen, Xiao-Shan Gao, Sevag Gharibian, Antonio Blanca, Eunou Lee, Mahdi Belbasi, Mingming Chen, and Rachel Player for helpful discussions. Part of this work was conducted during visits to the Simons Institute for the Theory of Computing; the authors gratefully acknowledge its hospitality. J.D. was supported by NSF grant SaTC-1814221 and Taft Foundation. V.G. was supported by NSERC and CIFAR; IQC receives partial support from the Government of Canada and the Province of Ontario. A.G. was funded by Samsung Electronics Co., Ltd. for the project \"The Computational Power of Sampling on Quantum Computers\", by the Institute for Quantum Information and Matter (NSF Grant PHY-1733907), and by the EU's Horizon 2020 Marie Skłodowska-Curie program 891889-QuantOrder. S.H. was partially supported by National Science Foundation awards CCF-1618287, CNS-1617802, and CCF-1617710, and by a Vannevar Bush Faculty Fellowship from the US Department of Defense.\n\nSummary chunk number 14\nWe present a simple proof of the correctness of Algorithm 1 for Problem 3.2 when the system has a unique solution. Let \\( a = (a_1, \\dots, a_n) \\in \\{0, 1\\}^n \\) be the unique solution of a set of polynomials \\( F \\). Let \\( \\vec{y} = [a_1, \\dots, a_i, a_j, \\dots, a_n]^\\top \\) be the 0/1 solution vector corresponding to the multilinear monomials under assignment \\( a \\). We show that the Boolean Macaulay linear system \\( M\\vec{y} = \\vec{b} \\) has the unique solution \\( \\vec{y} = M^+ \\vec{b} \\) when \\( F \\) has a unique solution \\( a \\), because the matrix \\( M \\) has linearly independent columns. If \\( F \\) has more than one solution, the columns of \\( M \\) are linearly dependent, and the solutions of \\( M\\vec{y} = \\vec{b} \\) form a multidimensional affine subspace. By induction on degree \\( d \\), for each nontrivial multilinear monomial \\( X_\\beta \\), there exist polynomials \\( p_{i\\beta}, q_{j\\beta} \\) such that \\( X_\\beta - \\sum_{i} a_{\\beta_i} x_i \\in \\langle F \\rangle \\), using the base case \\( d = 1 \\) and the inductive hypothesis.\n\nSummary chunk number 15\nThe fragment lists various dates and events related to conferences, academic societies, and research publications, including the 15th Conference on the Theory of Quantum Computation, Communication, and Cryptography (TQC) in 2015, the 46th International Colloquium on Automata, Languages, and Programming (ICALP) in 2013 and 2019, the 52nd ACM Symposium on the Theory of Computing (STOC) in 2020, and references to publications and PhD theses from 1997, 1998, 2002, 2013, 2017, and 2018, as well as affiliations with KU Leuven and Microsoft Research.\nSummary article number 4\nThe article studies the problem of solving systems of multivariate polynomial equations over \\( \\mathbb{F}_2 \\), which is known to be NP-complete. The approach involves linearizing the system via the Macaulay matrix, which encodes the polynomials and their monomial products. The authors analyze the application of the HHL quantum linear system (QLS) algorithm to solve this linear system, aiming to avoid explicit Gröbner basis computation. They demonstrate that the standard requirements for QLS—efficient preparation of the input state, solution extraction, and sparse matrix access—are satisfied. However, they prove an exponential lower bound on the condition number \\( \\kappa \\) of the Macaulay matrix, specifically \\( \\Omega(n^h) \\) for a unique solution with Hamming weight \\( h \\), and show that this bound holds even under max degree. This implies that the HHL-based quantum algorithm requires exponential time in the worst case, with complexity \\( \\Omega((3n)^{h/2}) \\).\n\nThe authors introduce a refined \"Boolean Macaulay matrix\" over \\( \\mathbb{C} \\), derived via Gaussian elimination, which is smaller and preserves the solution set. This leads to a lower bound of \\( \\Omega(2^{h/2}) \\) on the condition number, improving upon the previous bound. When the Hamming weight \\( h = \\Theta(\\log n) \\), this bound is polynomial, leaving open the possibility of superpolynomial quantum speedup. They also define a truncated QLS condition number \\( \\kappa_b(\\mathbf{M}) \\), which is lower-bounded and rules out improvements via truncated algorithms. For systems with multiple solutions of equal Hamming weight, the condition number can be reduced by at most a factor of \\( \\sqrt{t} \\), where \\( t \\) is the number of solutions.\n\nThe authors compare the quantum approach to classical methods: brute-force search over all \\( n \\)-bit strings of weight \\( h \\) requires \\( O(n^h) \\) time, while Grover search achieves \\( O(\\sqrt{n^h}) \\). When the solution weight is unknown, classical iteration requires \\( O(\\sqrt{h} n^h) \\) assignments. In contrast, the quantum algorithm has complexity \\( O(\\kappa) \\), and the lower bound shows that it is outperformed by Grover when \\( d + h \\geq n \\). The Boolean Macaulay matrix approach enables efficient solution extraction via a modified algorithm with only \\( O(\\log n) \\) iterations, reducing the total cost to \\( O(n \\log n) \\), and allows for a potential superpolynomial speedup when \\( h = \\Theta(\\log n) \\).\n\nThe paper provides a self-contained proof of correctness using affine hashing and the Valiant-Vazirani reduction, reducing multi-solution systems to unique ones. It also shows that the solution vector is a 0/1 vector, enabling direct measurement and classical interpretation. The results establish that the quantum algorithm does not offer a speedup over classical brute-force search in general, due to the exponential lower bound on the condition number, and that the problem remains intractable for cryptographic applications where solutions are rare. The work also highlights the limitations of truncated and preconditioned QLS variants, and shows that the Boolean Macaulay system cannot be dequantized classically\nROUGE Metrics:\nrouge1: 0.0857\nrouge2: 0.0504\nrougeL: 0.0535\nrougeLsum: 0.0763\n\nBERTScore: F1 = 0.6749\nLongDocFACTScore for sum: -3.8184351544631157\nLongDocFACTScore for abstract: -4.2445260882377625\n","output_type":"stream"}],"execution_count":49},{"cell_type":"code","source":"print(\"\\n--- ROUGE Metrics (средние значения) ---\")\nfor key in rouge_list[0].keys():\n    avg = sum(r[key] for r in rouge_list) / len(rouge_list)\n    print(f\"{key}: {avg:.4f}\")\n\navg_bert = sum(bert_list) / len(bert_list)\nprint(f\"\\n--- BERTScore F1 (среднее): {avg_bert:.4f} ---\")\n\navg_sum = sum(l['for_summary'] for l in ldfacts_list) / len(ldfacts_list)\navg_abs = sum(l['for_abstract'] for l in ldfacts_list) / len(ldfacts_list)\n\nprint(f\"\\n--- LongDocFACTScore (средние) ---\")\nprint(f\"Для суммаризаций: {avg_sum:.4f}\")\nprint(f\"Для абстрактов: {avg_abs:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T19:27:53.239745Z","iopub.execute_input":"2026-01-08T19:27:53.240314Z","iopub.status.idle":"2026-01-08T19:27:53.246471Z","shell.execute_reply.started":"2026-01-08T19:27:53.24028Z","shell.execute_reply":"2026-01-08T19:27:53.245617Z"}},"outputs":[{"name":"stdout","text":"\n--- ROUGE Metrics (средние значения) ---\nrouge1: 0.0962\nrouge2: 0.0468\nrougeL: 0.0575\nrougeLsum: 0.0821\n\n--- BERTScore F1 (среднее): 0.6998 ---\n\n--- LongDocFACTScore (средние) ---\nДля суммаризаций: -4.5227\nДля абстрактов: -4.5579\n","output_type":"stream"}],"execution_count":50},{"cell_type":"markdown","source":"*С text_splitter:*","metadata":{}},{"cell_type":"markdown","source":"я забыл сохранить переменную summaries_list прежде чем запустить следующую ячейку :))))))","metadata":{}},{"cell_type":"code","source":"rouge_list = []\nbert_list = []\nldfacts_list = []\nsummaries_list = []\n\nfor i in range(5):\n    print(f\"\\n=== Статья {i} ===\")\n    #извлекаем чанки, абстракт, названия разделов\n    article_dict = ast.literal_eval(df['dict_test'].iloc[i]) #словарь для i-той статьи \n    chunked_article = list(article_dict.values()) #список чанков\n    # chapters_list = list(article_dict.keys()) #список глав\n    # print('Список разделов')\n    # display(pd.DataFrame(chapters_list))\n    abstract = df['abstract'].iloc[i] \n    article = to_full_text(article_dict) #полный текст статьи с ключами и значениями словаря\n    \n    print(f\"Токенов в статье: {len(tokenizer.encode('\\n'.join(chunked_article)))}\")\n    prompt_0 = \"You're a science editor. Briefly summarize this fragment of the scientific text in original language. Do not add information that is not in the source texts\"\n    prompt_1 = \"You're a science editor. Based on the following summaries of the parts of the article, create a single, coherent and concise summary of the entire scientific article in original language, highlighting the common goal, methods, key results and conclusion. Do not add information that is not in the source texts\"\n    \n    summary_full = summarize(article, prompt_0 = prompt_0, prompt_1 = prompt_1,chunk_size = 1500, max_tokens_0 = 500, max_tokens_1 = 700, text_splitter = True)\n    print(f'\\nSummary article number {i}')\n    print(summary_full)\n    summaries_list.append({\n        'article_id': i,\n        'original_text': article,\n        'abstract': abstract,\n        'summary': summary_full\n    })\n\n    results = rouge.compute(\n    predictions=[summary_full],\n    references=[article],\n    use_stemmer=True\n    )\n    print(\"ROUGE Metrics:\")\n\n    rouge_dict = {}\n    for key, value in results.items():\n        print(f\"{key}: {value:.4f}\")\n        rouge_dict[key] = value\n    rouge_list.append(rouge_dict)\n\n    _, _, F1 = scorer.score([summary_full], [abstract])\n    bert_f1 = F1.item()\n    print(f\"\\nBERTScore: F1 = {bert_f1:.4f}\")\n    bert_list.append(bert_f1)\n\n    ldfacts_sum = ldfacts_scorer.score_src_hyp_long([article], [summary_full])\n    ldfacts_abs = ldfacts_scorer.score_src_hyp_long([article], [abstract])\n    print(f\"LongDocFACTScore for sum: {ldfacts_sum[0]}\")\n    print(f\"LongDocFACTScore for abstract: {ldfacts_abs[0]}\")\n    ldfacts_list.append({\n        'for_summary': ldfacts_sum[0],\n        'for_abstract': ldfacts_abs[0]\n    })","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T19:30:42.38999Z","iopub.execute_input":"2026-01-08T19:30:42.390633Z","iopub.status.idle":"2026-01-08T20:01:45.359139Z","shell.execute_reply.started":"2026-01-08T19:30:42.390601Z","shell.execute_reply":"2026-01-08T20:01:45.358344Z"}},"outputs":[{"name":"stdout","text":"\n=== Статья 0 ===\nТокенов в статье: 6903\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/9 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a6fe7feb779f46e49de25127334a5125"}},"metadata":{}},{"name":"stdout","text":"\nSummary chunk number 0\nThe text introduces model order reduction (MOR) for linear time-invariant continuous-time systems with large order n, which poses challenges in simulation and design due to high computational cost. Balanced truncation is a standard MOR method that preserves stability and provides an a priori error bound over the entire frequency range. However, in practical applications where input signals operate within a known finite frequency range (e.g., ω ∈ [ω₁, ω₂]), the focus shifts to minimizing a finite-frequency performance index. Standard balanced truncation (called frequency-independent balanced truncation, FIBT) is not optimal for such cases. To improve in-band approximation, several balancing-related methods are discussed: (1) Singular perturbation approximation (SPA), which matches the transfer function at ω = 0 and is suitable when zero frequency is the dominant operating point; generalized SPA allows adjustable trade-offs between low- and high-frequency performance. (2) Frequency-weighted balanced truncation (FWBT), which uses frequency weighting functions to improve frequency-specific performance but requires iterative design and increases model order. (3) Frequency-limited Grammians balanced truncation (FGBT), which extends standard Grammians to finite frequency ranges, but may fail due to non-positive semi-definite solutions and lacks an error bound; modified versions with error bounds have been proposed. A common issue among these methods is that they still use entire-frequency performance indices to evaluate finite-frequency approximation performance.\n\nSummary chunk number 1\nThe task is to construct a reduced-order model of order 3 that well approximates the frequency-domain dynamic behavior of the original model near ω = 0. Among existing balancing-related methods, the generalized SPA is most suitable for this problem, as is the proposed SF-type FDBT method. Sigma plots of the error systems obtained using generalized SPA and SF-type FDBT are shown in Figures and , respectively; both methods yield small approximation errors near ω = 0. The local and global approximation performance can be balanced by adjusting the user-defined parameter (ρ for generalized SPA and ε for SF-type FDBT). In this example, generalized SPA and SF-type FDBT perform similarly, though significant differences may occur in other cases (e.g., Example 3, where only SF-type FDBT is effective). Among balancing-related methods, FGBT is the exact method developed for interval-type finite-frequency model reduction, while the proposed interval-type FDBT is also designed for such problems. The key difference between them is illustrated via sigma plots and error bounds in Figures – Figures , showing that FGBT provides error bounds over the entire frequency range, whereas the interval-type FDBT provides bounds only over a pre-specified frequency interval. Since operating frequencies are assumed to lie within the given interval, the interval-type error bounds are sufficient for performance estimation. Compared to standard FIBT, both FGBT and interval-type FDBT improve approximation performance over a specified frequency interval. Additionally, the interval-type FDBT achieves better approximation performance and smaller error bounds simultaneously. As noted in Remark 4, the interval-type FDBT yields small error bounds as long as the frequency interval size is sufficiently small. This is demonstrated through a randomization experiment involving 100 stable systems of order 4, where the off-diagonal elements of matrix A and elements of matrices B, C, D are generated with zero mean and unit variance normal distribution, and diagonal elements of A have mean −5.5 and variance 4.5. The average performance between FGBT and interval-type FDBT is compared using several indices defined in Table II.\n\nSummary chunk number 2\nThe Kalman-Yakubovich-Popov (KYP) Lemma is a fundamental result in system and control theory. Iwasaki and Hara generalized it from the entire-frequency to finite-frequency cases, leading to the Generalized KYP Lemma, which is used here. Lemma 2.1 states that for a continuous-time system, certain frequency-domain conditions are equivalent to the existence of symmetric positive definite matrices P and Q satisfying specific inequalities involving a user-specified scalar ǫ, with ǫ = −(j̟ − λ_i) ensuring invertibility of (ǫI + j̟I − A), where λ_i are eigenvalues of matrix A. Proposition 3.2 shows that an SF-type frequency-dependent extended system can be obtained via a Möbius transformation. Proposition 3.3 states that if the original system is Hurwitz stable and ǫ > 0, then the extended system is stable; if the original system is unstable, the extended system is stable for 0 < ǫ < min(ǫ⁺_i), where ǫ⁺_i = (̟ − Im(λ_i))² / Re(λ_i) + Re(λ_i). Definition 3.4 introduces SF-type frequency-dependent Lyapunov equations for controllability and observability Gramians. Definition 3.5 defines SF-type frequency-dependent balanced realization as one where the controllability and observability Gramians are equal and diagonal. Proposition 3.6 is incomplete in the provided text.\n\nSummary chunk number 3\nThe fragment presents a proposition and theorem on SF-type frequency-dependent balanced realizations of stable linear continuous-time systems. It states that if certain frequency-dependent Lyapunov equations hold, the system realization is referred to as an SF-type frequency-dependent balanced realization. Given a stable system, the standard controllability and observability Gramians satisfy frequency-independent Lyapunov equations. By post- and pre-multiplying frequency-dependent equations with specific matrices and subtracting them, the derivation of the frequency-dependent balanced realization is justified. The SF-type frequency-dependent balanced truncation method is introduced: for a given operating frequency ω = ̟, a reduced-order model is constructed using a frequency-dependent extension of the system. The approximation error at the frequency point ω = ̟ is bounded in SF-type, and the error over the entire frequency range is bounded in EF-type. The proof relies on constructing dilated error systems and applying the Generalized KYP Lemma to derive the error bounds. The derivation for r = n−1 is detailed, while lower-order cases follow by similar steps. The reduced model is obtained via standard FIBT applied to the extended system, and the error bounds are established using the triangle inequality.\n\nSummary chunk number 4\nThe SF-type frequency-dependent balanced truncation (FDBT) algorithm is derived by applying the standard FIBT algorithm to obtain the frequency-dependent error bound. Using the triangle inequality, the entire-frequency error bound is established. The algorithm (Algorithm 1) proceeds by solving SF-type frequency-dependent Lyapunov equations, performing a coordinate transformation via a matrix that simultaneously diagonalizes two matrices, and constructing a reduced-order model. The error bound can be made arbitrarily small by decreasing parameter ǫ, but ǫ must be chosen carefully to ensure satisfactory performance over a neighborhood interval [̟ − δ, ̟ + δ]. A suitable ǫ* can be selected by balancing SF-type and EF-type error bounds. If the uncertain frequency interval size δ is known, ǫ should be smaller than ǫ*. The method is developed in complex setting, but for real systems, it is only applicable at ̟ = 0, where all matrices remain real. It is not the only method for low-frequency model reduction; SPA is also effective, though the underlying mechanisms differ. The SF-type FDBT is presented as a new alternative. Conventional balanced truncation methods apply to stable systems; for unstable systems, decomposition techniques are required. According to Proposition 2, a stable SF-type frequency-dependent method can always be found.\n\nSummary chunk number 5\nIn an unstable system, techniques such as stable and unstable part decomposition should be combined. According to Proposition 2, a stable SF-type frequency-dependent extended system can always be obtained by selecting an appropriate ǫ, even if the original system is unstable. Therefore, the SF-type FDBT can be directly applied to model reduction of unstable systems. However, the resulting reduced model is not guaranteed to be stable, even if the original system is stable.\n\nSummary chunk number 6\nIV. FREQUENCY-DEPENDENT BALANCED TRUNCATION OVER KNOWN FREQUENCY-INTERVALS\n\nThe section presents results for linear continuous-time systems where the operating frequency lies within a pre-specified interval ω ∈ [̟₁, ̟₂]. It introduces definitions of interval-type frequency-dependent controllability and observability Lyapunov equations and corresponding Gramians. A system realization is defined as interval-type frequency-dependent balanced if the controllability and observability Gramians are equal and diagonal, satisfying the respective Lyapunov equations.  \n\nTheorem 4.4 establishes that for a system in interval-type frequency-dependent balanced realization, the reduced-order model obtained by balanced truncation preserves stability if the original system is stable. The approximation error over the frequency interval ω ∈ [̟₁, ̟₂] is bounded in an interval-type manner (bound 44), while the error over the entire frequency range is bounded in an EF-type manner (bound 51), with the reduced model's extended form defined as Gᵣ̟₁,̟₂(𝑗ω).  \n\nProof: Stability preservation follows from standard arguments used in classical FIBT. The interval-type error bound (44) is derived by constructing a structure-preserving dilated error system and applying the Generalized KYP Lemma with symmetric Lyapunov variables. The EF-type error bound (51) is proven similarly to existing SF-type FDBT results.  \n\nProposition 4.5 states that under the given conditions, a limit condition holds as n approaches infinity.\n\nSummary chunk number 7\nThe fragment presents a proof of the interval-type frequency-dependent balanced truncation (FDBT) method, showing that under convergence of matrices C ei, N ei, and B ei as the frequency interval size tends to zero, a scalar µ < ∞ exists such that a norm-bounded inequality holds. It demonstrates that the square of matrices on both sides of equation (66) implies the existence of matrices U and V, where U contains eigenvectors and V contains corresponding eigenvalues, leading to the conclusion that the frequency-dependent realization can be obtained via coordinate transformation. The interval-type FDBT algorithm is then outlined: solving interval-type Lyapunov equations, transforming the system via a matrix T(̟₁, ̟₂) that diagonalizes Wc and Wo, and computing the reduced-order model. The method is distinguished by providing an interval-type error bound (44), which tends to zero as the interval size decreases, offering a better in-band error bound while maintaining good approximation performance. The algorithm supports complex or real system matrices and both symmetric and asymmetric frequency intervals. The error bound is theoretically appealing, and the method is shown to outperform standard FIBT and FGBT in small intervals and provide better in-band approximation in medium intervals. Experimental results on an RLC ladder circuit show that standard FIBT and generalized SPA fail to approximate dynamics near ω = 0, while the interval-type FDBT successfully generates good approximations for orders above 50. The method is applicable to real systems with symmetric intervals and can be adapted for asymmetrical cases.\n\nSummary chunk number 8\nThe fragment lists publication details, including publishers, years, titles of conferences or proceedings, and page numbers, related to scientific publications from various institutions and conferences, such as IEEE, SIAM, Springer-Verlag, and the Max Planck Institute. It does not include additional information beyond the listed publication metadata.\n\nSummary article number 0\nThe article addresses the challenge of reducing the order of large-scale linear time-invariant continuous-time systems to enable efficient simulation and design, while preserving accurate dynamic behavior within a known finite frequency range. Standard balanced truncation (FIBT) is effective for stability and global error bounds but suboptimal for finite-frequency applications. To improve in-band performance, several balancing-related methods are analyzed: singular perturbation approximation (SPA), frequency-weighted balanced truncation (FWBT), and frequency-limited Grammians balanced truncation (FGBT). However, these methods either require iterative design, increase model order, or lack guaranteed error bounds over finite intervals.  \n\nThe paper introduces two novel frequency-dependent methods: SF-type frequency-dependent balanced truncation (FDBT) and interval-type FDBT. Both extend standard balanced truncation to finite frequency ranges by using frequency-dependent Lyapunov equations and extended systems derived via Möbius transformation. The SF-type FDBT provides error bounds over a specific frequency point and can be extended to an entire frequency range using the Generalized KYP Lemma. The interval-type FDBT is specifically designed for systems operating within a pre-specified frequency interval [ω₁, ω₂], offering an interval-type error bound that vanishes as the interval size decreases. It outperforms FIBT and FGBT in both small and medium frequency intervals, particularly in low-frequency applications such as near ω = 0.  \n\nThe methods are validated through numerical experiments on a stable RLC ladder circuit, where standard FIBT and generalized SPA fail to capture dynamics near zero frequency, while interval-type FDBT successfully achieves accurate approximation even for systems of order above 50. The reduced-order model is constructed via standard balanced truncation applied to a frequency-dependent extended system, with stability preserved for stable original systems. For unstable systems, a stable extended system can be constructed via appropriate parameter selection, though the resulting reduced model may not be stable.  \n\nIn summary, the common goal is to develop reduced-order models that accurately approximate system dynamics within a finite frequency interval. The key results show that both SF-type and interval-type FDBT methods provide tighter in-band error bounds and better performance than existing methods, with the interval-type FDBT offering a theoretically sound, computationally feasible, and practically effective solution for finite-frequency model reduction. The conclusion is that frequency-dependent balanced truncation methods, particularly the interval-type FDBT, are superior for practical applications where input signals are confined to a known frequency range.\nROUGE Metrics:\nrouge1: 0.1403\nrouge2: 0.0688\nrougeL: 0.0742\nrougeLsum: 0.1216\n\nBERTScore: F1 = 0.6826\nLongDocFACTScore for sum: -4.517411851882935\nLongDocFACTScore for abstract: -5.1785284042358395\n\n=== Статья 1 ===\nТокенов в статье: 5944\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f72428ac6f2d46d988c2cc5657bf7d19"}},"metadata":{}},{"name":"stdout","text":"\nSummary chunk number 0\nThe fragment introduces a novel loss framework for recurrent neural network language models (RNNLMs) aimed at addressing two limitations of the classical classification framework: (1) the absence of a natural metric on output classes, and (2) the separation of inputs and outputs with no semantic link. The proposed framework augments the standard cross-entropy loss with a term that minimizes the Kullback-Leibler divergence between the model's prediction and an estimated target distribution derived from word embeddings. This estimated distribution leverages word vector similarities to define a meaningful metric space among words. The framework further includes a synergistic improvement by reusing the input word embedding matrix as the output classification matrix, reducing model complexity and improving information utilization. Theoretical analysis supports this design, and empirical validation on the Penn Treebank and Wikitext-2 datasets shows that models trained with this framework outperform conventional models, with performance consistent across datasets. The improvement is largely achieved by reusing word embeddings, even under mild assumptions.\n\nSummary chunk number 1\nTheoretical motivation is provided for a modification in language model training involving the reuse of word embeddings. In a setting where input embedding dimension equals RNN hidden state dimension (dₓ = dₕ), bias b is set to zero (yₜ = W hₜ), only the augmented loss is used, training loss is assumed to be zero, and temperature τ is large, the augmented loss is shown to align the model’s logits with those of the more informative labels ỹ. Using first-order approximation of the exponential function and assuming average zero inner product between uₜ and lⱼ, the derivative of the loss with respect to logits is analyzed, leading to the conclusion that the loss constrains the output probability space to a subspace defined by the embedding matrix. Under full-rank assumptions and sufficient linear independence of hₜ examples, the column space of Lᵀ equals that of W, enabling the representation W = LᵀA. This implies that reusing the embedding matrix in the output projection layer (via transpose) and applying a linear mapping h → Ah achieves the same effect as the original setup. This mechanism is proposed to be made explicit by constraining W = Lᵀ during training with b = 0, which reduces network size and eliminates redundant computation in the augmented loss.  \n\nRelated work includes prior efforts in RNNLMs with dropout, novel units, and pointer networks, none of which address loss structure. Our approach differs from prior work using KL divergence by transferring knowledge within the same network without external training. A concurrent preprint empirically reuses the embedding matrix but lacks theoretical justification. Previous models with shared input and output embeddings implicitly used such structure without proposing it as a trainable augmentation.  \n\nExperiments use the Penn Treebank (PTB) and Wikitext-2 datasets. PTB includes 923k training, 73k validation, and 82k test words, with a 10k-word vocabulary. Wikitext-2 has 2.088M training, 217k validation, and 245k test tokens, with a 33,278-word vocabulary—larger in both size and vocabulary than PTB.  \n\nThe baseline model follows a 2-layer LSTM with equal hidden units (200, 650, 1500 units), trained with stochastic gradient descent and a dropout variant. Training details are deferred to the appendix\n\nSummary chunk number 2\nThe authors empirically validate their theory on loss augmentation by training a 2-layer LSTM on a randomly selected 20,000-word sequence from the PTB dataset, using a loss augmented objective with varying β and temperature τ. They constrain input embedding row norms to 1 and do not apply regularization. After training, they compute the subspace distance between the input embedding matrix L and the output projection matrix W, using a residual norm metric. When β increases from 0 to 1, the subspace distance drops from nearly 1 to ~0.06, indicating convergence of W's column space to L^T. Even at low τ (e.g., τ = 2), the augmented loss drives W toward L^T, supporting the claim that explicit reuse of input embeddings (W = L^T) is not merely regularization but an optimal design choice. The empirical results confirm the theoretical mechanism and suggest that the proposed loss augmentation enables effective subspace alignment without explicit reuse.\n\nSummary chunk number 3\nThe results on the PTB and Wikitext-2 datasets show that four variants of a 2-layer LSTM with variational dropout outperform the baseline: (1) VD-LSTM, (2) VD-LSTM + augmented loss (AL), (3) VD-LSTM + reused embeddings (RE), and (4) VD-LSTM + both AL and RE (REAL). All models with AL, RE, or both show significantly better performance. AL improves performance especially in small networks, where it outperforms RE on PTB but shows limited gain on Wikitext-2, consistent with larger datasets better representing the true data distribution. When small networks are trained on two equally sized partitions of Wikitext-2, AL still outperforms RE despite a larger embedding size, supporting the claim that AL improves information extraction from the dataset. RE significantly outperforms AL in larger networks, indicating that enforcing proximity between input embedding and output projection spaces enhances performance, especially by reducing model complexity while preserving representational power. The best model, VD-LSTM+REAL, surpasses previous work including large ensembles and outperforms VD-RHN+RE by 2.5 in perplexity. Qualitative results show reduced <unk> token generation and improved predictions of semantically close words due to metric similarity in embeddings. The framework improves language modeling by using metric-based data distributions and reusing embeddings, reducing trainable parameters, and is applicable to other NLP tasks.\n\nSummary chunk number 4\nAppendix A: We train the networks with an initial learning rate of 1, decaying it at epochs 5, 10, and 1 for small, medium, and large networks respectively, with decay rates of 0.9 and 0.97 for small/medium and large networks. Backpropagation unrolls the network for 35 steps. Gradient clipping is applied with thresholds of 5 and 6 for small/medium and large networks. Dropout is applied to hidden states with the same mask across unrolled steps, and dropout weights are tied across layers; no dropout is used in the input embedding layer. Dropout probabilities are 0.7, 0.5, and 0.35 for small, medium, and large networks on PTB, and 0.8 and 0.6 for small and medium on Wikitext-2. For augmented loss training, temperature τ is set to 20; α (augmented loss weight) is set to α = γτ, with γ between 0.5–0.8 for PTB and 1.0–1.5 for Wikitext-2. Performance does not deteriorate significantly under moderate variations in τ or α.\n\nAppendix B: The subspace distance between two matrices X and Y is computed in three steps: (1) obtain orthonormal matrices U and V with spans equal to those of X and Y via QR decomposition; (2) compute projection S = U U^T V, residual R = V − S; (3) define distance metric d as d² = (1/C) ∑_{i=1}^C sin²(θ_i), where θ_i are the principal angles between subspaces, derived from singular values of U^T V. The metric is non-negative, symmetric, and zero only when span(X) = span(Y). It equals the average of the squared sines of the principal angles.\n\nSummary article number 1\nThe article proposes a novel loss framework for recurrent neural network language models (RNNLMs) that addresses two key limitations of classical training: the lack of a natural metric on output classes and the semantic disconnect between inputs and outputs. The framework augments cross-entropy loss with a term that minimizes the Kullback-Leibler divergence between model predictions and a target distribution derived from word embeddings, thereby defining a meaningful metric space among words. A key innovation is the reuse of the input word embedding matrix as the output classification matrix (via transpose), which reduces model complexity and improves information utilization. Theoretical analysis shows that under certain assumptions—particularly when input embedding dimension equals hidden state dimension and bias is zero—the augmented loss aligns model logits with more informative labels and constrains the output probability space to a subspace defined by the embedding matrix. This leads to the conclusion that the output projection matrix W converges to the transpose of the input embedding matrix Lᵀ, enabling effective subspace alignment without explicit regularization. Empirical validation on the Penn Treebank and Wikitext-2 datasets demonstrates that models using this framework outperform baseline models, with performance consistent across datasets. The augmented loss improves performance—especially in small networks—by enhancing information extraction from the data, while embedding reuse significantly boosts performance in larger networks by reducing redundancy and improving representational efficiency. The best-performing model, combining both augmented loss and embedding reuse (REAL), surpasses prior state-of-the-art models in perplexity. The results confirm that the proposed framework enables effective metric-based learning, reduces trainable parameters, and is applicable to broader NLP tasks.\nROUGE Metrics:\nrouge1: 0.0989\nrouge2: 0.0462\nrougeL: 0.0627\nrougeLsum: 0.0788\n\nBERTScore: F1 = 0.6886\nLongDocFACTScore for sum: -4.416718377007379\nLongDocFACTScore for abstract: -4.481736040115356\n\n=== Статья 2 ===\nТокенов в статье: 6253\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/6 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5b0995f90d04a0ebb859b8235154ab6"}},"metadata":{}},{"name":"stdout","text":"\nSummary chunk number 0\nWord embeddings are widely used in natural language processing to represent words as continuous vectors. While most existing work focuses on semantic representations, this paper addresses the problem of learning embeddings that capture the acoustic characteristics of words. The authors propose a multi-view approach that jointly learns embeddings from both acoustic (audio) and orthographic (character) sequences of words. Using contrastive losses based on matched and mismatched pairs of acoustic and orthographic sequences, the method aims to ensure that embeddings are close for the same word and far for different ones, or that distances reflect orthographic edit distances. A key advantage is that the multi-view approach enables direct comparison between acoustic and orthographic embeddings, allowing the same learned representations to be used across various tasks. The method outperforms prior work on acoustic word discrimination and shows promising results in cross-view discrimination and word similarity tasks.\n\nSummary chunk number 1\nThe paper introduces an approach for learning acoustic word embeddings in a multiview setting, reviewing prior methods such as single-view classification and Siamese networks. It proposes using a multi-view contrastive loss that jointly embeds acoustic segments and character sequences into a common space, leveraging weak supervision based on pairwise distances. The method uses a bidirectional LSTM network architecture to extract embeddings, with a multi-view contrastive objective that considers both acoustic and character sequence views. The paper explores various triplet-based loss functions, including those with same-view negative pairs, and introduces a cost-sensitive margin that scales with character sequence edit distance to better reflect word similarity. The embeddings are learned through a contrastive objective where the distance between a positive pair (acoustic segment and its corresponding character sequence) is minimized relative to a negative pair.\n\nSummary chunk number 2\nThe text describes a multi-view learning framework for acoustic and character-based word embeddings using recurrent neural networks, specifically long-short term memory (LSTM) networks. Both views employ stacked bidirectional LSTMs, where inputs are frame-level acoustic features (e.g., MFCCs and derivatives) and character sequences (one-hot vectors). At each layer, forward and backward LSTM cells process the sequence, and intermediate outputs are concatenated to form the input to the next layer. At the top layer, the final outputs are concatenated into fixed-dimensional embeddings used to compute cosine distances in the objective function. The authors note that prior work on acoustic word embeddings has used DTW-based methods or RNN autoencoders, but none explicitly focus on character sequence embeddings for acoustic similarity. Evaluation is conducted through three surrogate tasks: acoustic word discrimination (determining if two acoustic segments represent the same word), cross-view word discrimination (matching written words with spoken segments), and word similarity (measuring correlation between embedding distances and character edit distances). The dataset consists of spoken word pairs from the Switchboard corpus, with 9971–11024 training/validation/test pairs across 1687–3390 unique words, and inputs include MFCC-based acoustic features and character one-hot vectors. The evaluation uses average precision (AP) as the primary metric, with AP and rank correlation used to assess performance across tasks.\n\nSummary chunk number 3\nWe experiment with different neural network architectures for each view, varying the number of stacked LSTM layers, hidden units, and bidirectional vs. single-directional LSTM cells. A coarse grid search shows that 2-layer bidirectional LSTMs with 512 hidden units per direction per layer perform well on the acoustic word discrimination task, and this structure is fixed for subsequent experiments. The outputs of the top-layer LSTMs serve as the learned embedding for each view, resulting in a 1024-dimensional embedding when bidirectional LSTMs are used. During training, dropout is applied to the acoustic view inputs and between stacked layers for both views. For each training example, a corresponding negative example is required; negative character label sequences are generated by uniformly sampling a word label different from the positive label, and negative acoustic feature sequences are sampled uniformly from differently labeled acoustic sequences. Network weights are initialized uniformly from [−0.05, 0.05]. The Adam optimizer is used with mini-batches of 20 acoustic segments, with initial learning rate tuned over {0.0001, 0.001}. Dropout rate is tuned over {0, 0.2, 0.4, 0.5}, with 0.4 performing best. The margin in the basic contrastive objectives (0–3) is tuned over {0.3, 0.4, 0.5, 0.6, 0.7}, with 0.4 and 0.5 yielding the best results. For objective 0 with cost-sensitive margin, the maximum margin *m_max* is tuned over {0.5, 0.6, 0.7} and threshold *t_max* over {9, 11, 13}. Models are trained up to 1000 epochs, and the model achieving the highest AP on the development set is used for test set evaluation.\n\nFour contrastive losses (3)–( ) and their combinations are evaluated. Table shows development set AP for acoustic and cross-view word discrimination using different objectives. For the acoustic discrimination task, obj 0 and obj 2 (involving only cross-view distances) slightly outperform the others. The combined objective obj 0 + obj 2 significantly outperforms all individual objectives and their combinations. The cost-sensitive objective is competitive but slightly behind. A similar objective to obj 0 +\n\nSummary chunk number 4\nThe text presents results from word similarity tasks using acoustic and text embeddings, measuring rank correlation (Spearman's ρ) between embedding distances and orthographic edit distances. The cost-sensitive loss improves this correlation compared to the fixed-margin objective, despite similar performance on word discrimination tasks. Acoustic and text embeddings show comparable performance, with correlations of 0.226 and 0.241 respectively with phonetic edit distances. The authors note that the data contains few similar word pairs, limiting the ability to learn fine-grained character sequence distinctions. A t-SNE visualization shows that acoustic and text embeddings cluster similarly, with unseen words clustering comparably to seen ones. A separate visualization of words ending in \"-ly\", \"-ing\", and \"-tion\" confirms that related words form well-defined clusters in text embeddings. The authors propose future work involving phonetic supervision and extending the approach to non-word segments. The study evaluates different LSTM architectures, finding bidirectional 2-layer LSTMs to be most effective, with no significant improvement beyond two layers.\n\nSummary chunk number 5\nA ADDITIONAL ANALYSIS\n\nThe study evaluates the impact of different network architectures on embedding models. Embeddings are learned using objective obj 0 and evaluated on acoustic and cross-view word discrimination tasks. All models are trained for 1000 epochs, except 1-layer unidirectional models, which converge after 500 epochs. Bidirectional LSTMs outperform unidirectional ones, and two-layer bidirectional LSTMs are significantly better than one-layer ones. No significant improvement is observed with more than two layers. For subsequent experiments, a 2-layer bidirectional LSTM architecture is fixed for each view. The table presents average precision (AP) on the development set for different architectures. Precision-recall curves and a scatter plot of cosine distances between acoustic embeddings and orthographic edit distances are also provided for the best models.\n\nSummary article number 2\nThis paper proposes a multi-view contrastive learning framework to jointly learn acoustic and orthographic word embeddings by leveraging both audio and character sequence inputs. The method uses stacked bidirectional LSTM networks to extract embeddings from frame-level acoustic features (e.g., MFCCs) and character sequences (one-hot vectors), jointly embedding them into a shared space through a multi-view contrastive loss. The loss function minimizes distances between positive pairs (same word with matched acoustic and character sequences) and maximizes distances for negative pairs, with a cost-sensitive margin scaled to character edit distance to better reflect word similarity. The model is evaluated on three surrogate tasks: acoustic word discrimination, cross-view word discrimination, and word similarity (measured via rank correlation with edit distances). Results show that the combined contrastive objective (obj 0 + obj 2) outperforms individual objectives and achieves strong performance across all tasks, with acoustic and orthographic embeddings showing comparable clustering behavior and moderate correlation (0.226–0.241) with edit distances. The best-performing architecture is a 2-layer bidirectional LSTM with 512 hidden units per direction, which is fixed for subsequent experiments. The study concludes that the proposed multi-view approach effectively captures acoustic characteristics of words and enables consistent, cross-view performance, with embeddings that cluster similarly across views and show promise for downstream tasks.\nROUGE Metrics:\nrouge1: 0.0801\nrouge2: 0.0424\nrougeL: 0.0517\nrougeLsum: 0.0665\n\nBERTScore: F1 = 0.7158\nLongDocFACTScore for sum: -4.509091241019113\nLongDocFACTScore for abstract: -4.3108125030994415\n\n=== Статья 3 ===\nТокенов в статье: 8151\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e2fd2ec8346a4458abeca8a9be875664"}},"metadata":{}},{"name":"stdout","text":"\nSummary chunk number 0\nGaussian processes (GPs) are Bayesian nonparametric models that model functions using a Gaussian prior and compute the posterior distribution given observations. They can learn function approximations well with sufficient data and avoid overfitting with limited data. The choice of kernel function, which defines the autocovariance structure, is crucial for GP performance and is typically selected based on expert knowledge. The spectral mixture (SM) kernel, which models the spectral density of a stationary signal as a sum of Gaussians, has been successfully applied in various domains. Existing variants of SM kernels—such as the SM product (SMP), nonstationary SM (NSM), and grid SM (GSM)—extend the model to handle image, spatial, and input-dependent data but fail to explicitly represent dependency structures between SM components. This paper proposes a new SM kernel with dependency structure (SMD), modeling inter-component dependencies via a generalized cross covariance derived from Bienaymé’s identity. A complex-valued Gaussian mixture model incorporating time and phase delays is used to characterize the spectral densities. The resulting SMD kernel generalizes the original SM kernel and explicitly captures dependency structures, which are interpretable and informative. To enable efficient and interpretable learning, a structure adaptation (SA) algorithm is introduced, including hyperparameter initialization, component compression, and sparsification of dependencies. The SMD kernel and SA algorithm are evaluated on synthetic and real datasets for interpolation, extrapolation, scalability, compression ratio, and sparsity ratio. The work introduces a new representation of dependency structure, a complex-valued SD model, a more expressive and interpretable SMD kernel, and an effective SA algorithm for model compression.\n\nSummary chunk number 1\nThe spectral mixture (SM) kernel is derived using Bochner's Theorem, representing a covariance function as a Fourier transform of a positive finite measure. When the measure has a density, it defines the spectral density (SD) of the kernel, and the kernel and its SD form a Fourier transform pair. The original SM kernel approximates stationary kernels by modeling the spectral density as a mixture of Gaussian components in the frequency domain. Each component is defined by signal magnitude, center frequency, and bandwidth parameters. Related work includes variants like SMP and NSM kernels, which introduce modifications such as non-stationary components or input-dependent parameters. However, existing SM variants typically assume independence among components and neglect dependency structures. Some approaches attempt to model dependency structures or reduce hyperparameters, but issues remain in dependency sparsification, hyperparameter optimization, and stable component selection. This paper proposes a novel SM kernel with dependency structure, modeling it via convolution of basis components in the time domain, ensuring compatibility with the original SM components when i = j, without introducing additional parameters. The dependency structure is captured through cross-correlation of basis components, and the spectral density is modeled using time- and phase-characterized Gaussians.\n\nSummary chunk number 2\nTime and phase modulated dependency structure (TP modulated SD) is introduced to enrich the representation of spectral density (SD) in signal processing. Based on Fourier transform properties, time delay θ and phase delay ϕ are embedded into the SD of a signal component, resulting in a complex-valued time-and-phase modulated SD function. The cross amplitude, cross Gaussian, cross time delay, and cross phase delay parameters characterize the dependency structure. The dependency structure is defined via the inverse Fourier transform, with the cross weight and cross amplitude forming a normalization term c_ij that reflects the strength of dependency. The SMD kernel is positive semidefinite, ensuring its spectral density is also positive semidefinite. The dependency structure is quantified by γ_ij ∈ [−1, 1], with γ_ij = ±1 when i = j depending on the sign of the component's amplitude. The covariance and SD are modified by TP delays, leading to extended covariance ranges, altered SD shapes and decay behaviors, and reduced predictive uncertainty. The posterior distribution and sampling paths show tighter confidence intervals for SMD compared to SM, indicating improved predictive precision due to the dependency structure.\n\nSummary chunk number 3\nComparisons between the SMD and related kernels: Fig. visualizes covariance differences between the SM and SMD kernels, where each black solid link represents a covariance structure, and circle q_i corresponds to f_SM,i. Cross connections denote Cov(f_SM,i, f_SM,j). The SM considers only the autocovariance Cov(f_SM,i, f_SM,i) and ignores component dependencies. Table summarizes hyperparameter differences between SMD and SM kernels in a P-dimensional input setting. In NSM, each SM hyperparameter is parameterized as a GP with a squared exponential kernel (e.g., weight w_i becomes w_i,x ∼ GP(0, k_SE(x, x'))), requiring three times more hyperparameters than SM. Without TP delays, SMD and SM have identical hyperparameter spaces. Incorporating TP delays introduces additional hyperparameters, increasing gradient computation complexity. For large initial Q in SM and SMD, the number of retained components after compression, Q_rest, is much smaller than Q.\n\nStructure adaptation for the spectral mixture with dependency structure: The SM kernel has a large number of hyperparameters (size 3Q), complicating inference, learning, and interpretability. Both SM and SMD suffer from hyperparameter initialization and component selection issues. SMD's dependency structures are dense and require sparsification. A structure adaptation (SA) algorithm (Algorithm 1) is proposed to address these issues and enable efficient inference and interpretable structure discovery. The algorithm includes: (1) M_init trainings to find better hyperparameter initialization with lower loss; (2) pruning of unimportant components based on weight comparison; (3) sparsification of dependency structures by quantifying intensity, removing weak ones, and fine-tuning the GP with sparse structures. Hyperparameter optimization is performed via maximum likelihood in pretrain (step 1) and fine-tuning (step 12). Two levels of sparsity are introduced: first from component compression (Q_rest), second from weak dependency removal. \n\nBootstrap-based hyperparameter initialization (BHI): Good initialization is crucial for optimization in high-dimensional hyperparameter space. Empirical spectral densities (SDs) can guide initialization via Bochner's Theorem, but contain noise and false peaks. Bootstrap sampling (B = 100) is used to generate spectral samples S* from empirical SDs to filter noise and false peaks. A Gaussian mixture model (GMM) fits the bootstrap samples to obtain Q Gaussians:\n\nSummary chunk number 4\nSparse dependency structure and its behavior: The study examines the sparsity and behavior of dependency structures in SMD. There are Q² − Q dependency structures; for components far apart, their dependency is weak due to near-zero intersection of their SDs. Closer values of µ_i, Σ_i, and w_i between components indicate stronger dependency. A binary mask β_ij, based on w_ij and a_ij, is introduced to remove low-intensity dependency structures. The resulting SMD has sparse dependency structures. The SR (α_SR) is defined as a percentage to evaluate sparsity: α_SR ∈ [0,1], with α_SR = 1 indicating no significant dependency and α_SR = 0 indicating all dependencies are large.\n\nExperiments: Performance of SMD is evaluated against state-of-the-art kernels (e.g., LIN, SE, Poly, PER, RQ, MA, Gabor, FBM, ULL, NN, SM) using synthetic and real-world datasets. All kernels use the same number of components Q. Training, testing data, SM prediction, SMD prediction, and confidence intervals (CI) are shown in black, green, red, blue, and gray, respectively.\n\nModel assessment: Performance is assessed using generalization ability, 95% CI for prediction uncertainty, posterior correlation ρ_ij for latent dependency between components, γ_ij for dependency intensity, and SMD-specific metrics CR (α_CR) and SR (α_SR). \n\nExperiment: A synthetic signal is generated from a GP with hybrid kernel k_SMD(θ={0.1,0.3}, ϕ={0.1,0.3}) + k_SM, containing dependency structure with Q=2 components. A time series of length 300 over [-10,10] is created and noise is added. The middle 40% is removed as missing testing data (green), with the rest as training data (black). Both SMD and SM use Q=5 components and identical initial hyperparameters w_i, µ_i, σ_i². SMD hyperparameters θ_i and ϕ_i are initialized to zero. In Fig., SMD outperforms SM in prediction accuracy and smoothness of CIs, especially in interpolating missing data. SM (dashed red) struggles to capture dependency and interpolate missing blocks. SMD variants without TP delays, only time delay, or only phase delay fail to interpolate missing blocks\n\nSummary chunk number 5\nLong-range interpolation of monthly river flow monitoring: The study validates the long-range interpolation capability of Gaussian process (GP) models using the SMD kernel. The monthly river flow dataset from the Madison River near West Yellowstone (1923–1960) exhibits short-term variations, seasonal patterns, irregular long-term trends due to lunar and solar influences, and noise. With 456 records, 30% of the data (middle part) is removed for testing, while the rest is used for training. Results show both SMD and SM models interpolate missing data well, but SMD outperforms SM in accuracy and confidence. SMD better captures complex patterns and achieves a higher component reduction rate (CR: 38.9%) and lower structural redundancy (SR: 89.3%). Posterior cross-covariance analysis reveals dependency structures between components; SMD shows clearer, non-zero posterior correlation coefficients indicating underlying dependencies, while SM exhibits high complexity without clear alignment. In addition, SMD performs well in both interpolation and extrapolation tasks on the yearly sunspot dataset (1700–2014), with extrapolation performance superior to SM. For extrapolation, SMD shows significant time-phase delay dependencies (e.g., between components 2 and 4), with improved performance in capturing temporal dynamics and reducing error by 40.7% in time and 50.3% in dependency structure. Scalable SMD is evaluated on a large multidimensional abalone dataset using automatic relevance determination (ARD). Exact GP inference is avoided via stochastic variational inference due to computational cost. SMD with SA algorithm achieves better CR than SM and maintains high SR, indicating sparse dependency structures. NSM performs poorly due to high hyperparameter count and overfitting. Conclusion: SMD effectively interpolates and extrapolates incomplete signals and scales well on large multidimensional data with sparse, interpretable dependency structures.\n\nSummary chunk number 6\nThe authors propose a novel sparse matrix decomposition (SMD) kernel that extends the SM kernel by incorporating TP delayed dependency structures. An interpretable structure adaptation (SA) algorithm is introduced to initialize hyperparameters, compress components, and automatically obtain sparse dependency structures. Extensive experiments on synthetic and real datasets show that the SMD with SA outperforms baselines and variants in learning TP delayed dependencies, achieving more accurate interpolation and extrapolation. Two open issues remain: (1) the initialization of TP parameters (currently set to zero, requiring more tailored methods), and (2) sparse or efficient inference, which needs improvement for large-scale data. The authors thank Elena Marchiori, Twan van Laarhoven, and Perry Groot for their feedback. Experimental results show that SMD with θ ≠ 0, ϕ ≠ 0 outperforms other methods, and the BHI-based initialization in SA provides a strong starting point for hyperparameter optimization in high-dimensional spaces.\n\nSummary article number 3\nThis paper proposes a novel spectral mixture kernel with dependency structure (SMD), which extends the classical spectral mixture (SM) kernel by explicitly modeling inter-component dependencies through a time- and phase-delayed dependency structure (TP modulated SD). The SMD kernel captures spectral densities using complex-valued Gaussian components with time and phase delays, enabling the representation of cross-correlations between components via cross-amplitude and cross-phase parameters. These dependencies are derived from a generalized cross covariance based on Bienaymé’s identity and ensure positive semidefiniteness of the resulting kernel. Unlike existing SM variants that assume independence among components, the SMD kernel explicitly models dependency structures via convolution in the time domain, with dependency strength quantified by γ_ij ∈ [−1, 1], and interpreted through cross-covariance and posterior correlation. To enable efficient and interpretable learning, a structure adaptation (SA) algorithm is introduced, comprising hyperparameter initialization via bootstrap sampling (BHI), component pruning based on weight comparison, and sparsification of dependency structures using intensity thresholds. The SA algorithm reduces model complexity by compressing components (Q_rest) and removing weak dependencies, with sparsity measured by component reduction ratio (CR) and structural redundancy (SR).  \n\nEvaluated on synthetic and real-world datasets—including a time series with missing data, monthly river flow, and long-term sunspot records—the SMD kernel outperforms baseline kernels (e.g., SM, NSM, LIN, SE) in prediction accuracy, smoothness of confidence intervals, and extrapolation performance. It achieves superior interpolation and extrapolation, particularly in capturing long-range temporal dynamics and complex dependencies. On real data, SMD demonstrates better generalization, reduced predictive uncertainty, and clearer posterior dependency structures compared to SM. In scalable settings (e.g., abalone dataset), SMD with SA achieves higher component compression and maintains sparse, interpretable dependency structures, outperforming NSM in scalability and avoiding overfitting. The results show that SMD improves predictive precision and interpretability by explicitly modeling inter-component dependencies, while the SA algorithm enables efficient learning and structure discovery. The work introduces a new, interpretable framework for dependency modeling in Gaussian processes, with two open challenges: initialization of TP parameters and scalable inference for large-scale data.\nROUGE Metrics:\nrouge1: 0.0998\nrouge2: 0.0471\nrougeL: 0.0558\nrougeLsum: 0.0858\n\nBERTScore: F1 = 0.7142\nLongDocFACTScore for sum: -4.886511663595836\nLongDocFACTScore for abstract: -4.574126243591309\n\n=== Статья 4 ===\nТокенов в статье: 13917\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/19 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d3512a50b15e460b9f6ea7552bb12137"}},"metadata":{}},{"name":"stdout","text":"\nSummary chunk number 0\nThe Introduction section provides background information on the scientific problem being addressed, outlines the objectives of the study, and reviews relevant prior research to establish the context and significance of the work presented.\n\nSummary chunk number 1\nSolving systems of multivariate polynomial equations over F₂ is NP-complete. This problem can be reduced to solving an exponential number of linear equations using the Macaulay matrix, which encodes polynomial equations and their monomial multiples as linear equations. The classical approach involves computing the Gröbner basis via triangularization of the Macaulure matrix. In quantum computing, the HHL algorithm solves large linear systems and has been applied to the Macaulay matrix, with Chen and Gao showing that the access requirements for the system (state preparation, data extraction, sparsity, conditioning) can be met. However, the condition number κ of the resulting matrix was not analyzed. This paper proves an exponential lower bound on κ for Boolean polynomial systems, showing that the quantum algorithm requires exponential time in the worst case, and presents a Grover-based approach.\n\nSummary chunk number 2\nThe number κ of the matrix A associated with a Boolean polynomial system shows that the original quantum algorithm runs in exponential time in the worst case. A Grover-based brute-force search algorithm outperforms it when there is a unique solution or all solutions have the same Hamming weight. For the unique solution case, the authors prove that the condition number κ is Ω((3n)^{h/2}), where h is the Hamming weight. A simple Grover-based brute-force algorithm takes time O(n^h). The authors also define a truncated QLS condition number κ_b(A) := ∥A∥_{A + b} / ∥b∥_2, which is a lower bound on κ, and show that their lower bounds apply to κ_b, ruling out improvements via truncated QLS algorithms. These results suggest that the original quantum algorithm does not pose a fatal cryptanalytic threat and provide tools for analyzing cryptosystems against such attacks. The authors refine Chen and Gao's algorithm by constructing a \"Boolean Macaulay matrix over C\" via Gaussian elimination, exploiting the quadratic nature of the input polynomials over C restricted to 0/1 solutions. This matrix is a submatrix of the original Macaulay matrix, preserves the solution set, and is significantly smaller, leading to a lower bound of Ω(2^{h/2}) on the condition number. For h = Θ(log n), this bound becomes polynomial, while the brute-force algorithm remains quasipolynomial. The refined algorithm does not rule out superpolynomial quantum speedup in such cases. The authors leave open the possibility of finding input instances where the condition number is small enough to allow a quantum speedup over classical or Grover-based methods, potentially enabling a new type of quantum speedup using the HHL algorithm. The correctness of the refined algorithm follows from the equivalence between the Boolean Macaulay linear system and the original Macaulay linear system.\n\nSummary chunk number 3\nThe correctness of the refined algorithm is established through the equivalence between the Boolean Macaulay linear system and the Macaulay linear system, whose correctness has been previously proven. A self-contained, elementary proof is provided in Appendix A, which does not rely on Gröbner bases but instead combines a special case of the reduction from [CGY18] with the Valiant-Vazirani affine hashing method, reducing any Boolean polynomial system with more than one solution to one with a unique solution. For a Boolean Macaulay linear system with a unique solution, an alternative method is proposed to extract the Boolean solution from the normalized monomial solution vector, reformulated as a generalization of the quantum coupon collector problem, requiring only O(log n) iterations—improving upon Chen and Gao’s O(n) iterations. However, the affine hashing introduces O(n) extra rounds, resulting in a total iteration complexity of O(n log n). The algorithm relies on the sparsity of vectors b and matrix A, which are s-sparse for some polynomially large s, and leverages the quasi-Toeplitz structure of the Boolean Macaulay matrix A to efficiently compute nonzero entries. Efficient quantum circuits are required to access the locations and values of nonzero elements, enabling the implementation of transformations involving matrix elements. The QLSP can be solved efficiently even for non-sparse b or A if efficient quantum circuits C_b and C_A can be constructed, such as using quantum random access memory (qRAM).\n\nSummary chunk number 4\nThe text describes a quantum algorithm for solving systems of polynomial equations over finite fields, building on prior work by Chen, Gao, and Yuan [CGY18]. It establishes a bijection between solutions of systems of n-variate quadratic polynomials over the binary field F₂ and over the complex field C, by introducing auxiliary variables z_i representing the values of polynomials modulo 2, and encoding each z_i in binary using Boolean variables y_ib. The resulting system over C includes field equations that enforce variables to take values in {0,1}, and the binary representation of z_i ensures that each polynomial evaluates to an even integer. By substituting the binary expansions into the equations, a polynomial system F over C is obtained, which is equivalent to the original system over F₂. This equivalence enables the use of quantum methods for solving the F₂ system via complex field techniques, with implications for quantum algorithms in polynomial system solving.\n\nSummary chunk number 5\nDue to (7), it follows that \\( f_i(s_1, \\ldots, s_n) \\equiv 0 \\mod 2 \\), so \\( (s_j) \\) is a solution of \\( F \\). Given a set of polynomials \\( F \\subseteq \\mathbb{F}_q[X] \\), Chen, Gao and Yuan [CGY18] propose an analogous approach to reduce the polynomial system \\( F \\) to a system where \\( Y \\) is the set of new variables introduced during the reduction. If \\( F_R \\) isolates a unique solution of \\( F \\), then the polynomial system \\( F_{RC} \\cup F \\) has a unique solution. Thus, without loss of generality, we can assume that Problem 3.2 has a unique solution. Red2: Any polynomial system \\( f_1, \\ldots, f_m \\) has no constant terms. If no polynomial in \\( F \\) has a constant term, then the all-zero vector is a trivial solution. Otherwise, let \\( c_i \\) denote the constant term of \\( f_i \\), and assume without loss of generality that \\( c_1 \\neq 0 \\). Then set \\( f'_1 := -f_1 / c_1 \\), and so on for \\( i = 2, \\ldots, m \\). The above two reductions increase the parameters considered in the paper only moderately: Red1 introduces at most \\( O(n \\log n) \\) new equations and variables, while Red2 only increases the number of nonzero terms in the polynomial system. Moreover, Red2 increases the size of \\( F \\) and the total number of nonzero terms \\( \\#f_i \\) by at most a factor of 2 (we choose \\( f_1 \\) to be the polynomial with a nonzero constant term and the fewest nonzero coefficients).\n\nSummary chunk number 6\nWe define the Macaulay linear system of a set of polynomials F ⊆ C[x₁, ..., xₙ] and show that when F has a unique solution, the condition number of the matrix is Ω(nh), where h is the Hamming weight of the solution. We prove that this lower bound holds even when using max degree instead of total degree. This implies that the quantum algorithm based on QLS for solving polynomial equations via Macaulay linear systems requires time Ω((3n)^(h/2)). If there are t different solutions with the same Hamming weight h, the lower bound on the condition number can decrease by at most a factor of √t. We also provide a formula for a lower bound on the condition number for any number of solutions, and present computational evidence that this bound is exponentially large in the smallest Hamming weight among the solutions.\n\nSummary chunk number 7\nThe Macaulay matrix is a linear matrix used to solve polynomial systems by linearizing them via auxiliary variables. Each row corresponds to a pair of polynomials (m, f), with entries representing coefficients of monomials in the product mf, and columns indexed by monomials of degree at most d. The matrix can be defined with respect to total or maximum degree, with max degree being specified in this text. The size of the matrix grows exponentially with the chosen degree. For m quadratic polynomials in n variables, the degree is at least √m and at most cαn for m = αn. Row operations correspond to polynomial ideal operations and preserve common roots. Classically, Gaussian elimination on the matrix yields a Gröbner basis, enabling solution recovery. In the quantum setting, Gaussian elimination is not feasible; instead, the QLS algorithm samples from nonzero solutions. The Macaulay matrix is the augmented matrix of the system M⃗y = ⃗b, with ⃗b set to [1 0]ᵀ under the assumption of a unique constant term. Chen and Gao set the max degree to 3n and showed that if F has a unique solution, the linear system also has a unique solution. The QLS output state |ŷ⟩ corresponds to this solution. Classically, the solving degree is at most n+2, allowing Gröbner basis computation. However, for systems with multiple solutions, the solution space is an affine subspace of dimension equal to the number of solutions minus one, and the QLS algorithm outputs the state corresponding to the smallest ℓ²-norm vector in this space. The matrix M is O(m • #F)-sparse and row computable, and ⃗b can be efficiently prepared. Assuming |F| = O(poly(n)), the QLS algorithm runs in time O(poly(n)κ(M)). Due to complexity-theoretic evidence (e.g., HHL09), the running time is at least Ω(κ(M)), so a lower bound on the condition number κ⃗b(M) provides a lower bound on the algorithm’s runtime.\n\nSummary chunk number 8\nWe provide a lower bound on the tQLScn κ⃗b(M). Since κ⃗b(M) ≤ κ(M), this also gives a lower bound on the time complexity of Chen and Gao's algorithm. To bound κ⃗b(M), it suffices to lower bound the ℓ₂-norm of the solution vector ⃗y = M + ⃗b. For a degree-d Macaulay linear system, a monomial exponent e ∈ ℕⁿ is a \"valid\" coordinate of a monomial solution vector ⃗y(a) if e ∈ {0,1,…,d}ⁿ and eᵢ ≤ d (total degree), and ⃗y(a)ₑ = ∏ᵢ aₑᵢᵢ, which equals 1 if and only if aᵢ = 1 for all variables in the monomial ∏ᵢ xₑᵢᵢ indexed by e. If the Hamming weight of a is h and M is constructed with max degree, the number of non-zero coordinates is h; with total degree, it is h. The affine subspace of solutions is spanned by monomial solution vectors ⃗y₁, ⃗y₂, ..., ⃗yₜ under assignments a₁, ..., aₜ. When the max degree is 3n, this affine subspace is spanned by these vectors [CG21, Theorem 3.21 and Lemma 4.1], though this may hold for lower degrees. Assuming degree d such that the system has this property, the solution ⃗y = M + ⃗b has minimum ℓ₂-norm in the affine hull spanned by ⃗y₁, ..., ⃗yₜ. If all a₁, ..., aₜ ∈ {0,1}ⁿ have the same Hamming weight, then the length ∥⃗y∥ can be bounded using Lemma 4.3: if vectors ⃗y₁, ..., ⃗yₜ have equal Hamming weights, then every vector in their complex affine hull A has ℓ₂-norm at least ∥⃗y₁∥ / √t. This follows because each ⃗yᵢ has 0–1 entries, so ∥⃗yᵢ∥₁ = ⟨⃗\n\nSummary chunk number 9\nThe fragment presents two lemmas. Lemma 4.4 states that if ⃗ y₁ is a 0/1 vector with minimum Hamming weight among a set of 0/1 vectors ⃗ y₁, ..., ⃗ yₜ, then any vector in their convex hull has ℓ²-norm at least ∥⃗ y₁∥ / √t. The proof uses the non-negativity of inner products and the fact that each ⃗ yᵢ has at least as large ℓ²-norm as ⃗ y₁ due to minimal Hamming weight, followed by Cauchy-Schwarz. Lemma 4.6 addresses the shortest vector in the affine hull of column vectors of a matrix V ∈ ℂⁿ×k, showing that the origin is in the affine hull iff the all-ones vector is in the column space of the Gram matrix G = V†V. If the origin is not included, the squared length of the shortest vector is given by 1 / (1₁₁, G + 1₁₁), and the proof establishes that this bound is tight via strong duality in the associated semidefinite program (SDP).\n\nSummary chunk number 10\nWe conclude that γ* ≠ 0 if and only if 1 1 1 is in the column space of G. If γ* > 0, a dual optimization problem is formulated by multiplying matrices with √G from both sides, yielding an equivalent maximization problem involving the orthogonal projector onto the column space of G. Since 1 1 1 is in the image of G, this is equivalent to equation (12). If γ* = 0, then 1 1 1 is not in the column space of G, and from w = βG + 1 1 1 + v with v ∈ ker(G) and ker(G) = ker(V), we may assume v = 0. It follows that V G + 1 1 1 / 1 1 1 ∈ A, and since A is an affine subspace not containing the origin, it contains at most one vector of the form βV G + 1 1 1, which holds only for γ ≤ 0 = γ*. Thus, (12) holds even when γ* = 0. For a Boolean solution set S = {a₁, ..., aₜ}, let A_S be the affine subspace spanned by monomial solution vectors corresponding to the Boolean solutions. To lower bound the length of the shortest vector in A_S, it suffices to consider an enlarged affine subspace A_S′ ⊇ A_S. Let S′ be the symmetrized solution set obtained by permuting variables and taking the union. Let A_S′ be the corresponding affine subspace, and let v be its shortest vector. For each Hamming weight h in S′, there is a symmetrized monomial solution vector v_h, which is the average over all monomial vectors associated with Boolean solutions of weight h. The minimum ℓ²-norm vector v in A_S′ is an affine combination of the v_h. Under induced permutations of coordinates, the ℓ²-norm is preserved, and due to minimality and symmetry, v is invariant under all such permutations. Hence, v is an affine combination of the v_h. To lower bound M + F b, we compute the minimum ℓ²-norm in the affine subspace spanned by the v_h for Hamming weights h in S, using the Gram matrix as in Lemma 4.6.\n\nSummary chunk number 11\nThe symmetrized vectors \\( v_h \\) corresponding to Hamming weights \\( h \\) in the system \\( S \\) are considered using the Gram matrix as described in Lemma 4.6. To compute this Gram matrix, the orthonormal vector system \\( (b_s) \\) associated with monomials of degree at most \\( d \\) containing exactly \\( s \\) non-zero exponents is introduced. The Gram matrix \\( G \\) has entries defined by the coefficients \\( c_d^s \\), which depend on the degree model (max degree or total degree). Theorem 4.7 states that the tQLScn of the degree-\\( d \\) Macaulay linear system is lower bounded by an expression involving the bottom-right \\( (n - h + 1) \\times (n - h + 1) \\) minor \\( G^{(h)} \\) of \\( G \\). This bound is exponentially large in \\( h \\) for large \\( d \\), and for max degree \\( d = 3n \\), it is lower bounded by \\( h / 2 \\) for every \\( h \\in [n] \\) up to \\( n = 300 \\).\n\nSummary chunk number 12\nThe fragment discusses the comparison of a quantum algorithm based on the HHL method to classical brute-force search and Grover's algorithm when solving a Boolean polynomial system. It shows that for a unique Boolean solution of Hamming weight h, the quantum algorithm has an exponential lower bound in h, and provides evidence that this bound holds even when multiple solutions exist. Classical search over all Hamming-weight-h assignments requires O(n^h) time, while Grover search achieves O(√n^h) evaluations. When the Hamming weight is unknown, classical iteration over increasing weights up to h ≤ n/2 results in O(√h n^h) complexity. Grover search with unknown weight size achieves similar complexity. For d + h ≥ n, Grover outperforms or matches the HHL-based algorithm. When multiple solutions have the same Hamming weight, Grover achieves the same speedup as the HHL-based method. In the general case with solutions of different weights, the quantum algorithm still has an exponential lower bound in the smallest Hamming weight (up to n = 300), suggesting it does not offer a significant advantage over Grover search. The Boolean Macaulay matrix is defined by replacing monomials with multilinear equivalents under the constraint that solutions are Boolean, reducing the matrix size and enabling extraction of Boolean solutions from quantum state measurements. The matrix is restricted to degree 1 due to redundancy of higher degrees, with total degree set to n.\n\nSummary chunk number 13\nThe matrix has rows labeled by pairs (m, f) where m is a multilinear monomial and f ∈ F₁, such that ψ(mf) has degree at most d. Columns are labeled by multilinear monomials in x₁, ..., xₙ of degree at most d, ordered by a specified monomial ordering. The entry in row (m, f) and column m′ is the coefficient of m′ in ψ(mf). Compared to the Macaulay matrix, the Boolean Macaulay matrix excludes polynomials of maximum degree at least 2. It can be obtained as a submatrix of the Macaulubay matrix M of maximum degree d corresponding to F₁ ∪ F₂ after Gaussian row reduction. When F₁ = ∅, row reduction on the Macaulay matrix of F₂ shows that field equations take a special form. The rows of M₂ are indexed by pairs (m, x₂ⱼ − xⱼ) with maxdeg(m(x₂ⱼ − xⱼ)) ≤ d. Row operations transform each row starting with coefficients of Πxₐᵢᵢ − x⁻¹ⱼΠxₐᵢᵢ into coefficients of Πxₐᵢᵢ − Πxₘᵢₙ{aᵢ,1}ᵢ. This results in at most one 1 per row in the left part (nonmultilinear monomials). The process proceeds by descending total degree, reducing the total degree of the second term while preserving variables. Rows with identical variable sets and equal values of mx₂ᵢ = m′x₂ⱼ are equal and can be eliminated (zeroed out).\n\nSummary chunk number 14\nThe fragment describes a row-reduction process on the F₂ submatrix of a Macaulay matrix M, where row operations eliminate rows corresponding to nonmultilinear monomials because ψ(mxᵢ) = ψ(m′xⱼ) implies equal rows. After elimination, each column has a unique nonzero entry corresponding to a leading multilinear monomial, allowing the matrix to be written (up to row permutation) as M′ = 0 B. The Boolean Macaulay matrix B is constructed by placing ψ(tf) in rows indexed by multilinear monomials t and f ∈ F₁, with row support size at most O(#F₁). Column sparsity is bounded: for a multilinear monomial t of degree at most 2, the number of divisors is at most 4, and for a quadratic f ∈ F₁, the sparsity is at most 4•#F₁ due to linear combinations of at most #F₁ quadratic monomials. Thus, the entire Boolean Macaulay matrix is s-sparse with s = 4•#F₁.\n\nSummary chunk number 15\nThe column sparsity of the Boolean Macaulay matrix of {f} is at most 4 • #F₁. The entire Boolean Macaulay matrix of F₁ is obtained by stacking the Boolean Macaulay matrices of {f} for f ∈ F₁, resulting in a total column sparsity of at most 4m • #F₁. The linear system M⃗y = ⃗b is equivalent to the linear system 0, so a solution ŷ₂ of the Boolean Macaulay linear system M₂⃗y₂ = ⃗b₂ corresponds to a solution ŷ₁ of the Macaulay linear system. Since M is an O(m • #F)-sparse row/column computable matrix and the sparse vector ⃗b can be efficiently prepared as the quantum state |b⟩, a QLS algorithm can be applied to solve the Boolean Macaulay linear system M⃗y = ⃗b, which takes time. The key parameter in the running time is the condition number of the matrix M. A lower bound on the tQLScn of M is provided, thus also giving a lower bound on known QLS algorithms.\n\nSummary chunk number 16\nThe text discusses a quantum linear system (QLS) algorithm for solving Boolean Macaulay linear systems with a unique solution. Given a system \\( M \\vec{y} = \\vec{b} \\) of total degree \\( d \\), with solution vector \\( \\vec{y} \\) corresponding to multilinear monomials of degree at most \\( d \\), the algorithm applies a quantum linear system algorithm to obtain the solution \\( \\vec{y} \\) as a quantum state \\( |\\vec{y}\\rangle \\). Measurement in the computational basis samples subsets of variables, and when \\( 0 \\leq d \\leq \\lfloor |S|/3 \\rfloor \\), the probability of not observing a variable after \\( r \\) trials is bounded. The algorithm proceeds by measuring the quantum state, setting variables in the outcome to 1, repeating the process \\( O(\\log n) \\) times, and setting remaining variables to 0. It is shown that when \\( h = \\Omega(\\sqrt{n}) \\), classical brute-force search is faster than the quantum algorithm; when \\( h = O(\\sqrt{n}) \\), the relative speed is unknown. In the Boolean case, with lower bound \\( \\kappa_{\\vec{b}}(M) \\geq \\frac{1}{2}(2^h - 1) \\), for \\( h = pn \\), \\( p \\in (0, \\frac{1}{2}] \\), classical algorithms may outperform quantum ones. Compared to Chen and Gao’s algorithm, Algorithm 1 uses a smaller Macaulay matrix (size \\( m2^n \\times 2^n \\)), allowing a potentially superpolynomial speedup, and requires only \\( O(\\log n) \\) iterations due to the unique solution, whereas Chen and Gao’s method requires \\( O(n \\log n) \\) iterations due to repeated system updates via Valiant-Vazirani reduction. The quantum algorithm achieves at most quadratic speed-up over classical brute-force search.\n\nSummary chunk number 17\nThe Boolean Macaulay linear system approach provides a framework for studying quantum computation, encompassing problems like Factoring, Graph Isomorphism, and Learning with binary errors. The QLS algorithm for this framework is BQP-complete, and since Factoring is in BQP via Shor's algorithm, overcoming the condition number curse could extend quantum advantages to other problems. An analytical lower bound on the condition number decreases when polynomial systems have multiple solutions, but cryptographic systems typically have one or few solutions, suggesting the QLS algorithm cannot be used to attack cryptosystems via the Macaulay matrix. Adding field equations increases the number of solutions exponentially but does not reduce the length of the shortest vector, as it remains an affine combination with new variables set to zero. Two main approaches to handle ill-conditioned QLSPs—truncated and preconditioned QLS—have been proposed; the lower bound rules out speedup via the truncated algorithm, but further study is needed on preconditioned variants such as sparse approximate inverse, circulant, or fast inversion preconditioners. The Boolean Macaulay system cannot be dequantized by classical methods due to its full column rank. A variant of the quantum coupon collector problem enables efficient solution extraction when solution vector values exhibit correlation, and generalizing this to broader correlated patterns could be useful. When the solution’s Hamming weight is logarithmic in the number of variables, the lower bound does not exclude superpolynomial speedup over exhaustive search, though such cases are unlikely in finite field applications due to significant variable expansion. Row operations on the Boolean Macaulay matrix preserve rank, leading to the conclusion that the system has a unique solution ŷ = M⁺⃗b. Standard bounds on binomial coefficients are derived for completeness, with a focus on h ≤ n/2.\n\nSummary chunk number 18\nA simple proof is given for the uniqueness of the solution in Algorithm 1 for Problem 3.2 when the system has a unique solution. Let \\( a = (a_1, \\dots, a_n) \\in \\{0,1\\}^n \\) be the unique solution of a set of polynomials \\( F \\). Let \\( \\vec{y} = [a_1, \\dots, a_i, a_j, \\dots, a_n]^\\top \\) be the 0/1 solution vector corresponding to the multilinear monomials under assignment \\( a \\). It is shown that the Boolean Macaulay linear system \\( M\\vec{y} = \\vec{b} \\) has the unique solution \\( \\vec{y} = M^+ \\vec{b} \\) when \\( F \\) has a unique solution \\( a \\), because the matrix \\( M \\) has linearly independent columns. If \\( F \\) has more than one solution, the columns of \\( M \\) are not linearly independent, and the solution space of \\( M\\vec{y} = \\vec{b} \\) becomes a multidimensional affine subspace. The proof proceeds by induction on degree \\( d \\), showing that for all nontrivial multilinear monomials \\( X_\\beta \\), there exist polynomials \\( p_{i\\beta}, q_{j\\beta} \\) such that \\( X_\\beta - \\sum_{i} a_{\\beta_i} x_i \\in \\langle F \\rangle \\), using the fact that \\( x_k - a_k \\in \\langle F \\rangle \\) for each \\( k \\) and the inductive hypothesis.\n\nSummary article number 4\nThe article addresses the problem of solving systems of multivariate polynomial equations over the binary field \\( \\mathbb{F}_2 \\), which is known to be NP-complete. It analyzes the applicability of quantum computing—specifically the HHL algorithm—to this problem via the Macaulay linear system approach, which reduces polynomial systems to linear systems over the complex field. The study proves an exponential lower bound on the condition number \\( \\kappa \\) of the Macaulay matrix for Boolean polynomial systems with a unique solution of Hamming weight \\( h \\), showing that the quantum algorithm requires time \\( \\Omega((3n)^{h/2}) \\) in the worst case. This lower bound applies even to truncated QLS algorithms, ruling out speedups via such methods. A refined Grover-based brute-force search algorithm is shown to outperform the quantum method when there is a unique solution or all solutions have the same Hamming weight, with time complexity \\( O(n^h) \\), which is better than the quantum algorithm for \\( h \\gg \\sqrt{n} \\). For solutions with Hamming weight \\( h = \\Theta(\\log n) \\), the condition number bound becomes polynomial, and the refined algorithm achieves a superpolynomial speedup over classical exhaustive search. The authors introduce a \"Boolean Macaulay matrix over \\( \\mathbb{C} \\)\" via Gaussian elimination, which is smaller and preserves the solution set, leading to a lower bound of \\( \\Omega(2^{h/2}) \\) on the condition number. The correctness of the refined algorithm is established through a self-contained proof based on affine hashing and equivalence to the original Macaulay system. A variant of the quantum coupon collector problem enables efficient solution extraction in \\( O(\\log n) \\) iterations, improving upon prior methods. The results show that the original quantum algorithm does not pose a fatal cryptanalytic threat, and suggest that quantum speedup in this setting is limited to cases with very small Hamming weights. The paper concludes that the condition number curse limits quantum advantage in solving Boolean polynomial systems, and that classical or Grover-based methods remain superior in most practical cryptographic settings.\nROUGE Metrics:\nrouge1: 0.0607\nrouge2: 0.0379\nrougeL: 0.0387\nrougeLsum: 0.0527\n\nBERTScore: F1 = 0.6887\nLongDocFACTScore for sum: -3.9413005005229604\nLongDocFACTScore for abstract: -4.2445260882377625\n","output_type":"stream"}],"execution_count":53},{"cell_type":"code","source":"print(\"\\n--- ROUGE Metrics (средние значения) ---\")\nfor key in rouge_list[0].keys():\n    avg = sum(r[key] for r in rouge_list) / len(rouge_list)\n    print(f\"{key}: {avg:.4f}\")\n\navg_bert = sum(bert_list) / len(bert_list)\nprint(f\"\\n--- BERTScore F1 (среднее): {avg_bert:.4f} ---\")\n\navg_sum = sum(l['for_summary'] for l in ldfacts_list) / len(ldfacts_list)\navg_abs = sum(l['for_abstract'] for l in ldfacts_list) / len(ldfacts_list)\n\nprint(f\"\\n--- LongDocFACTScore (средние) ---\")\nprint(f\"Для суммаризаций: {avg_sum:.4f}\")\nprint(f\"Для абстрактов: {avg_abs:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T20:02:46.924993Z","iopub.execute_input":"2026-01-08T20:02:46.925314Z","iopub.status.idle":"2026-01-08T20:02:46.932023Z","shell.execute_reply.started":"2026-01-08T20:02:46.925286Z","shell.execute_reply":"2026-01-08T20:02:46.931315Z"}},"outputs":[{"name":"stdout","text":"\n--- ROUGE Metrics (средние значения) ---\nrouge1: 0.0960\nrouge2: 0.0485\nrougeL: 0.0566\nrougeLsum: 0.0811\n\n--- BERTScore F1 (среднее): 0.6980 ---\n\n--- LongDocFACTScore (средние) ---\nДля суммаризаций: -4.4542\nДля абстрактов: -4.5579\n","output_type":"stream"}],"execution_count":54},{"cell_type":"markdown","source":"![image.png](attachment:196635df-7b8f-4f75-9452-007daee38c93.png)","metadata":{},"attachments":{"196635df-7b8f-4f75-9452-007daee38c93.png":{"image/png":"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"}}},{"cell_type":"markdown","source":"различия несущественные, но эти метрики все равно неинформативны","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}