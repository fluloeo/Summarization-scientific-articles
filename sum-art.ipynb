{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nimport ast\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-08T18:05:48.860750Z","iopub.execute_input":"2026-01-08T18:05:48.861006Z","iopub.status.idle":"2026-01-08T18:05:50.651274Z","shell.execute_reply.started":"2026-01-08T18:05:48.860977Z","shell.execute_reply":"2026-01-08T18:05:50.650333Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/articles/df_dict_test.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"**Роман мне скинул чанки, сделанные парсером, я очень доволен!!**","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/articles/df_dict_test.csv')\ndf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T18:05:54.581566Z","iopub.execute_input":"2026-01-08T18:05:54.581883Z","iopub.status.idle":"2026-01-08T18:05:54.718268Z","shell.execute_reply.started":"2026-01-08T18:05:54.581829Z","shell.execute_reply":"2026-01-08T18:05:54.717531Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"            id                                              title  \\\n0   1602.04402  balanced truncation of linear time-invariant s...   \n1   1611.01462  tying word vectors and word classifiers: a los...   \n2   1611.04496  multi-view recurrent neural acoustic word embe...   \n3   1808.00560  compressible spectral mixture kernels with spa...   \n4   2111.00405  limitations of the macaulay matrix approach fo...   \n..         ...                                                ...   \n95  2307.14341  virtual mirrors: non-line-of-sight imaging bey...   \n96  2307.14354  learned gridification for efficient point clou...   \n97  2307.14362  learnable wavelet neural networks for cosmolog...   \n98  2307.14392  human-centric scene understanding for 3d large...   \n99  2309.03177  3d object positioning using differentiable mul...   \n\n                                             abstract  \\\n0   this paper discusses model order reduction of ...   \n1   recurrent neural networks have been very succe...   \n2   recent work has begun exploring neural acousti...   \n3   spectral mixture (sm) kernels comprise a power...   \n4   recently chen and gao~\\cite{chengao2017} propo...   \n..                                                ...   \n95  non-line-of-sight (nlos) imaging methods are c...   \n96  neural operations that rely on neighborhood in...   \n97  convolutional neural networks (cnns) have been...   \n98  human-centric scene understanding is significa...   \n99  this article describes a multi-modal method us...   \n\n                                            dict_test  \n0   {'I. INTRODUCTION AND PROBLEM FORMULATIONS': '...  \n1   {'INTRODUCTION': \"Neural network models have r...  \n2   {'INTRODUCTION': 'Word embeddings-continuous-v...  \n3   {'Introduction': \"Gaussian processes (GPs) con...  \n4   {'Introduction': 'Solving systems of multivari...  \n..                                                ...  \n95  {'Computed image of': 'T-shaped object from a ...  \n96  {'Introduction': 'Point clouds provide sparse ...  \n97  {'Introduction': 'The process of extracting in...  \n98  {'Introduction': 'Human-centric scene understa...  \n99  {'I. INTRODUCTION': \"Differentiable rendering ...  \n\n[100 rows x 4 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>title</th>\n      <th>abstract</th>\n      <th>dict_test</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1602.04402</td>\n      <td>balanced truncation of linear time-invariant s...</td>\n      <td>this paper discusses model order reduction of ...</td>\n      <td>{'I. INTRODUCTION AND PROBLEM FORMULATIONS': '...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1611.01462</td>\n      <td>tying word vectors and word classifiers: a los...</td>\n      <td>recurrent neural networks have been very succe...</td>\n      <td>{'INTRODUCTION': \"Neural network models have r...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1611.04496</td>\n      <td>multi-view recurrent neural acoustic word embe...</td>\n      <td>recent work has begun exploring neural acousti...</td>\n      <td>{'INTRODUCTION': 'Word embeddings-continuous-v...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1808.00560</td>\n      <td>compressible spectral mixture kernels with spa...</td>\n      <td>spectral mixture (sm) kernels comprise a power...</td>\n      <td>{'Introduction': \"Gaussian processes (GPs) con...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2111.00405</td>\n      <td>limitations of the macaulay matrix approach fo...</td>\n      <td>recently chen and gao~\\cite{chengao2017} propo...</td>\n      <td>{'Introduction': 'Solving systems of multivari...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>2307.14341</td>\n      <td>virtual mirrors: non-line-of-sight imaging bey...</td>\n      <td>non-line-of-sight (nlos) imaging methods are c...</td>\n      <td>{'Computed image of': 'T-shaped object from a ...</td>\n    </tr>\n    <tr>\n      <th>96</th>\n      <td>2307.14354</td>\n      <td>learned gridification for efficient point clou...</td>\n      <td>neural operations that rely on neighborhood in...</td>\n      <td>{'Introduction': 'Point clouds provide sparse ...</td>\n    </tr>\n    <tr>\n      <th>97</th>\n      <td>2307.14362</td>\n      <td>learnable wavelet neural networks for cosmolog...</td>\n      <td>convolutional neural networks (cnns) have been...</td>\n      <td>{'Introduction': 'The process of extracting in...</td>\n    </tr>\n    <tr>\n      <th>98</th>\n      <td>2307.14392</td>\n      <td>human-centric scene understanding for 3d large...</td>\n      <td>human-centric scene understanding is significa...</td>\n      <td>{'Introduction': 'Human-centric scene understa...</td>\n    </tr>\n    <tr>\n      <th>99</th>\n      <td>2309.03177</td>\n      <td>3d object positioning using differentiable mul...</td>\n      <td>this article describes a multi-modal method us...</td>\n      <td>{'I. INTRODUCTION': \"Differentiable rendering ...</td>\n    </tr>\n  </tbody>\n</table>\n<p>100 rows × 4 columns</p>\n</div>"},"metadata":{}}],"execution_count":2},{"cell_type":"markdown","source":"*Пример для одной статьи:*","metadata":{}},{"cell_type":"code","source":"data_dict = ast.literal_eval(df['dict_test'].iloc[0])\nprint(*data_dict.keys(),sep='\\n')\nprint(f'Количество чанков: {len(data_dict.keys())}',end='\\n\\n')\n\nfor key, value in data_dict.items():\n    print(key,end='\\n\\n')\n    print(value,end='\\n\\n')\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T18:29:22.638784Z","iopub.execute_input":"2026-01-08T18:29:22.639589Z","iopub.status.idle":"2026-01-08T18:29:22.645401Z","shell.execute_reply.started":"2026-01-08T18:29:22.639558Z","shell.execute_reply":"2026-01-08T18:29:22.644687Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"I. INTRODUCTION AND PROBLEM FORMULATIONS\nDRAFT\nII. FUNDAMENTAL TOOL\nIV. FREQUENCY-DEPENDENT BALANCED TRUNCATION OVER KNOWN FREQUENCY-INTERVALS\nTheorem 4.4 (Interval-type Frequency-dependent Balanced Truncation):\n, n\nV. EXAMPLES\nIndexes computation formula\nBerlin/Heidelberg, Germany\nКоличество чанков: 9\n\nI. INTRODUCTION AND PROBLEM FORMULATIONS\n\nWe study model order reduction for linear time-invariant continuous-time systems where A ∈ C n×n , B ∈ C n×m , C ∈ C p×n , D ∈ C p×m , x(t) ∈ C n is the state vector, u(t) ∈ C m is the input signal, y(t) ∈ C p is the output signal. Modeling of complex physical processes often leads to large order n. The corresponding high storage requirements and expensive computations make it very difficult to simulate, optimize or even design such large scale systems - . In this case model order reduction (MOR) plays an important role. It consists in approximating the system (1) by a reduced-order system: G r (ω) : 1 Max Planck Institute for Dynamics of Complex Technical Systems, Sandtorstraße 1, 39106 Magdeburg, Germany. * Corresponding author: benner@mpi-magdeburg.mpg.de 2 School of Mechatronic Engineering and Automation, Shanghai University, Shanghai, 200072, P. R. China. where A r ∈ C r×r , B r ∈ C n×m , C r ∈ C p×n , D r ∈ C p×m with r < n. Balanced truncation is a well grounded and the most commonly used model order reduction scheme . The standard form is the so-called Lyapunov balanced truncation, which was first introduced in the systems and control literature by Moore . The prominent advantages of balanced truncation is that it preserves stability and provides an a priori known error bound over the entire-frequency range. In detail, it gives a upper bound of the following entire-frequency (EF) type approximation performance index function In many practical applications, the operating frequency of input signal belongs to a fully or partially known finite-frequency range such as a limited interval (i.e. ω ∈ [̟ 1 , ̟ 2 ]). For those cases, the reduced-order model is only needed to capture the input-output behavior of the original system for input signals with admissible frequency. Correspondingly, good in-band approximation performance is more expected, while the out-band approximation performance might be neglected - . In other words, the objective of finite-frequency (FF) model order reduction is only to minimize the following finite-frequency type performance index function: Since the standard balanced truncation is intrinsically frequency-independent, hereby we will call it as frequency-independent balanced truncation (FIBT) in the sequel, it cannot be used to further improve the in-band approximation performance with pre-known frequency information. To enhance the approximation performance over pre-specified frequency range, several balancingrelated approaches have been developed. Some famous and popular ones include: (1) Singular perturbation approximation (SPA). SPA is a companion balancing-related method of the standard FIBT and is first introduced by Liu and Anderson . Although FIBT and SPA gives same entire-frequency type error bound, the characteristics of them are contrary to each other. The reduced systems generated by FIBT generally have a smaller error at high frequencies, and tend to be larger at low frequencies. In contrast, SPA generally leads to good approximation performance at frequencies around ω = 0 by forcing the transfer function of full order model and reduced order model to be matched exactly at ω = 0 (i.e G(0) = G r (0)). Therefore, SPA is particularly suited for solving model reduction problems in the cases that ω = 0 is pre-known as the dominating operating frequency point ( [10]). To further make the a flexible tradeoff between the local approximation performance over low-frequency ranges and the global approximation performance over entire frequency range, generalized SPA algorithm has been developed by introducing a userdefined adjustable scalar (see Obinata and Anderson ). DRAFT Frequency-weighted balanced truncation (FWBT). In the fields of system analysis and control theory, frequency weighting functions is a conventional tool which has been widely applied for solving various analysis and synthesis problems with pre-known frequency information. For finite-frequency model order reduction problems, utilizing the frequency weighting technique and combing it with the standard balanced truncation method also is very prevailing. During the last three decades, many frequency weighted balanced reduction approaches have been developed (see Enns ; Zhou ; Sreeram ; Ghafoor and Sreeram ; Houlis and Sreeram ; Wang et al ; Sreeram et al and the references therein). The common procedure of FWBT is build frequency-weighted model first by introducing input/out frequency weighted transfer functions and then apply the standard FIBT on the weighted model. Indeed, good frequency-specific approximation performance may be obtained if the selected weighting function is an appropriate one. However, the design iterations to search for an appropriate weighting transfer function can be tedious and time consuming. Besides, FWBT also suffers from the drawback of the increased order of the weighted plant model. (3) Frequency-limited Grammians balanced truncation (FGBT). It was first introduced by Gawronski and Juang in . This methodology stems from the consideration of extending the definition of standard Gramians to the frequency-limited case and then applying the standard balanced truncation procedures to the frequency-limited Gramians ( [21] ). As has been pointed out in , FGBT may be invalid in some cases as the solutions of the \"frequency-limited Lyapunov equations\" cannot be guaranteed to be positive semi-definite, and it provides no error bound. To overcome those drawbacks, several modified FGBT schemes providing error bound have been proposed (Gugercin and Antoulas ; Gahfoor and Sreeram ) A common feature of the those existing finite-frequency balancing-related approaches is that they continue to use entire-frequency type index (3) to evaluate the actually concerned finite-frequency approximation performance (See Table ).\n\nDRAFT\n\nThe task is to build reduced model of order 3 approximating the frequency domain dynamic behaviors of the original model well in the neighborhood of ̟ = 0. Among the existing balancingrelated methods, the (generalized) SPA is the most suitable one for coping with this kind of model reduction problems. At the same time, our proposed SF-type FDBT method can also be applied for this kind of problems. The sigma plots of error systems generated by generalized SPA and SFtype FDBT are depicted in Fig. and Fig. , respectively. As Fig. and Fig. shown, both of them could gives rise to small approximation error around ̟ = 0. Moreover, one can make a tradeoff between the local approximation performance and global approximation performance by adjusting the the user-defined parameter (ρ for generalized SPA and ǫ for SF-type FDBT). In this example the generalized SPA and the SF-type FDBT performs very similar with each other, however, huge variety on their performance may occurred in some cases (see example 3 in the below, in which only the SF-type FDBT is effective). Among the existing balancing-related methods, FGBT is the exact one developed for solving such interval-type finite-frequency model reduction problems. Our proposed interval-type FDBT is also aimed to solve this kind of problems. We will show the differences between them by this example. The sigma plot of error models and the corresponding error bound are given in the Fig. -Fig. , by which the most striking difference on the type of error bounds can be illustrated. The FGBT provides error bound over entire-frequency range, in contrast, the intervaltype only provides error bound over the pre-specified frequency interval. Since it is assumed that the operating frequencies belong to the given intervals, the interval-type error bounds are adequate for approximation performance estimation. Compared with the standard FIBT, both the FGBT and the interval-type FDBT are effective in improving the approximation performance over specified DRAFT frequency interval. At the same time, the interval-type FDBT has the advantage that it gives rise to better approximation performance and smaller error bound simultaneously. As referred to in Remark 4, the interval-type FDBT always provides small error bound as long as the size of frequency interval is small enough. To show this, a randomization experiment was carried out. We randomly generate 100 stable systems with order 4. (The off-diagonal elements of matrix A and each element of the matrices B, C, D are obtained with a zero mean and unitary DRAFT variance normal distribution, the diagonal element of matrix A are obtained with mean -5.5 and variance 4.5). To compare the average performance between FGBT and interval-type FDBT, several indices are defined in Table .\n\nII. FUNDAMENTAL TOOL\n\nThe Kalman-Yakubovich-Popov (KYP) Lemma is a cornerstone in system and control theory. In fact, the EF-type error bound provided by the standard FIBT can be proofed and interpreted with the aid of KYP Lemma . In , Iwasaki and Hara successfully generalized the KYP Lemma from entire-frequency case to finite-frequency cases. The Generalized KYP Lemma plays a fundamental role in our developed and it is included here. Lemma 2.1 , Generalized KYP lemma): Consider a continuous-time system (1), the following statements are equivalent: (1) The frequency domain inequality (2) There exist symmetric matrices P and Q of appropriate dimensions, satisfying Q > 0 and G ǫ̟ (ω) : DRAFT where ǫ > 0 is a user-specified scalar. It should be pointed out that ǫ should be a scalar satisfying the condition: ǫ = −(̟ − λ i ) to ensure the invertibility of (ǫI + ̟I − A), where λ i , i = 1, ..., n denote the eigenvalues of the matrix A. Proposition 3.2: For a given system (1), the corresponding SF-type frequency-dependent extended system (7) can be obtained by applying a particular Moebius transformation as follows: Proposition 3.3: The following statements are true: a). If the original system (1) is Hurwitz stable and ǫ > 0, then the corresponding SF-type frequencydependent extended system is stable. b). Given the original system (1) is unstable and denote the unstable eigenvalues of A as λ + i , i = 1, ..., n u , then the corresponding SF-type frequency-dependent extended system is stable if the value of ǫ satisfying 0 < ǫ < min(ǫ + i ), i = 1, ..., n u , where ǫ + i = (̟ − Im(λ i )) 2 /Re(λ i ) + Re(λ i ). Proof: a). Let us denote λ i , i = 1, 2..., n, and λ ǫi (̟), i = 1, 2..., n as the eigenvalues of the matrices A and A ǫ (̟), respectively. According to the mapping between A and A ǫ (̟) given in , we know that Noticing that Re(λ i ) < 0 if the system G(ω) is stable, then the following inequalities hold if ǫ > 0. Thus the proof is completed. b). Denote λ + ǫi (̟), i = 1, ..., n u as the eigenvalues of A ǫ (̟) mapped from λ + i , i.e. then it can be concluded that Re(λ + ǫi (̟)) < 0, i = 1, ..., n u for all ǫ satisfying 0 < ǫ < min(ǫ + i ), i = 1, ..., n u , according to the computational formula . Thus the proof is completed. Definition 3.4 (SF-type Frequency-dependent Lyapunov Equations): Given a linear continuoustime system (1) and one of its corresponding Hurwitz stable SF-type frequency-dependent extended systems , then the following two Lyapunov equation DRAFT are defined as SF-type frequency-dependent controllability and observability Lyapunov equations of the continuous-time system . Furthermore, the solutions W cǫ (̟) and W oǫ (̟) will be referred to as SF-type frequency-dependent controllability and observability Gramians of the continuous-time system (1). Definition 3.5 (SF-type Frequency-dependent Balanced Realization): Given a linear continuoustime system (1) and one of its Hurwitz stable SF-type frequency-dependent extended systems , the corresponding SF-type frequency-dependent controllability and observability Gramians are equal and diagonal, i.e. the following Lyapunov equations simultaneously hold, then this particular realization will be referred to as a SF-type frequencydependent balanced realization Proposition 3.6: Suppose the given system ( ) is stable and let W c , W o , Σ denote its standard controllability and observability and balanced Gramian matrices, then the following statements are true: Proof: a). It is well known that the standard controllability and observability Gramian matrices W c , W o of system (1) satisfy the following standard frequency-independent Lyapunov equations: Post-and-pre multiply the SF-type frequency-dependent Lyapunov equations ( ) by ǫ −1 (ǫI + ̟I − A), then we have Furthermore, the following equations can be derived by subtracting the equations ( ) from ( ) It is easily to conclude that DRAFT Thus the proof is completed. b). The SF-type frequency-dependent Lyapunov equations ( ) can be rewritten as: thus one can conclude that: Thus the proof is completed. 3). It can be easily observed that the ̟-dependent matrices A ̟ , B ̟ , C ̟ will recover A, B, C as ǫ → ∞, i.e. Then it is trivial to conclude that Theorem 3.7 (SF-type Frequency-dependent Balanced Truncation): Given a linear continuoustime system (1) and the pre-known dominating operating frequency point ω = ̟, then for any one of its Hurwitz stable SF-type frequency-dependent extended systems given in SF-type frequency-dependent balanced realization with respect to the SF-type frequency-dependent Gramian and given by: where Z r = [I r×r 0 r×(n−r) ]. Furthermore, the truncated model G r (ω) possesses the following DRAFT properties: 1). The approximation error between the original system model ( ) and the truncated r th reduced model at the given frequency point ω = ̟ satisfies the following SF-type error bound: 2). The approximation error between the original system model ( ) and the truncated r th reduced model ( ) over entire frequency range satisfies the following EF-type error bound: where G rǫ̟ (ω) : Proof: 1). The detailed proof for r = n − 1 case will be provided in the sequel, and the r = n − 2, ...1 cases can be easily completed step by step. The error system model between the original high-order system model G(ω) and the truncated From the error system E n (ω), we can construct a dilated system E n (ω) as follow: DRAFT where B dn , C dn , D 12 dn , D 21 dn , D 22 dn are auxiliary 'dilated' matrices, and those matrices are constructed as follows: Defining the Lyapunov variable Q en = Q * en ≥ 0 and P en = P en as follows: Substituting the above constructed Lyapunov variable Q en , P en into the following SF-type matrix inequality suggested by the Generalized KYP Lemma, Combing the balanced SF-type frequency-dependent Lyapunov equations , one can derive the following equations: DRAFT where According to the Generalized KYP Lemma, the dilate error systems E n (ω) satisfying DRAFT therefore the error system E n (ω) satisfying This completes the SF-type error bound for the r = n − 1 case. The remainder of the proof for the r = n − 2, ...1 cases can be easily completed in a reciprocal way. 2). From ( ) and ( ), it can be concluded that the SF-type frequency-dependent extended system G rǫ̟ (ω) of reduced system G r (ω) can be obtained by applying the standard FIBT algorithm for G ǫ̟ (ω). Therefore, we have Noting that Using triangle inequality we get This completes the proof of entire-frequency error bound . Based on above preliminaries and results, we now at the stage to present the SF-type frequencydependent balanced truncation algorithm (see Algorithm 1). Remark 3.8: According to Proposition 3, the SF-type error bound can be regulated to an arbitrary small value by decreasing the parameter ǫ, in other word, arbitrary approximation accuracy at the given frequency point ω = ̟ can be achieved. To make the approximation performance over the neighboring intervals (ω ∈ [̟ − δ, ̟ + δ]) be satisfactory, the value of parameter ǫ should be selected carefully. One possible way to pick an appropriate value of ǫ is to plot the curves of SFtype error bound and EF-type error bound with respect to the parameter ǫ, then one can choose a proper value ǫ * which make the SF-type and EF-type error bound be traded off against each other. Furthermore, it is suggested to adopt the value of ǫ be smaller than ǫ * if there exists an estimation ( δ) on the size of the uncertain frequency interval (δ). The smaller δ is, the smaller value of ǫ could be. DRAFT Algorithm 1 SF-type FDBT Input: Full-order model (A, B, C, D), frequency (̟), user-defined parameter ǫ and the order of reduced model (r), Step 1. Solve the SF-type frequency-dependent Lyapunov equations (9) Step 2. Get the SF-type frequency-dependent balanced realization of the given system by coordinate transformation: where T ǫ (̟) is a matrix that simultaneously diagonalize the matrices W cǫ (̟) and W oǫ (̟), i.e., Compute the reduced-order model as: Output: Reduced-order model (A r , B r , C r , D r ) Remark 3.9: For the sake of theoretical completeness, the SF-type FDBT approach is developed in a complex setting. The original system matrices and the reduced system matrices are allowed to be complex. In many applications, only real systems are of practical interest. With real model restriction, the proposed SF-type FDBT can only be applied in the case that ̟ = 0. It is easy to find that the involved matrices W cǫ (̟), W oǫ (̟), T ǫ (̟) and the generated reduced model A r , B r , C r , D r are all real if the original system is real and the frequency point is ̟ = 0. In the framework of balancing related methods, the proposed SF-type FDBT is not the only way for solving model order reduction problems assuming the dominating frequency is ̟ = 0. As referred to in Section I, SPA is also regarded as an effective way for improving the approximation performance over lowfrequency ranges. However, it should be noticed that the underlying mechanisms and the algorithms of SPA and SF-type FDBT are totally different. Which one will performs better on low-frequency approximation accuracy improvement depends on the given original system model. From the results of Example 3 in Section 5, to say the least, the proposed SF-type FDBT can be viewed as a new non-trivial alternative option besides SPA. Remark 3.10: It is well-known that the conventional balanced truncation methods (such as the DRAFT above mentioned FIBT, SPA, FWBT and FGBT) are developed for stable systems. To make those methods applicable for unstable system, some techniques like stable part and unstable part decomposition should be combined [27] . According to Proposition 2, one can always find a stable SF-type frequency-dependent extended system by choosing a proper ǫ, even if the given original system is unstable. Thus, the SF-type FDBT can be used for coping with model reduction of unstable systems directly. The corresponding cost is that it cann't guarantee the generated reduced model is stable even if the original system is stable.\n\nIV. FREQUENCY-DEPENDENT BALANCED TRUNCATION OVER KNOWN FREQUENCY-INTERVALS\n\nIn this section, we present our results for the cases that the operating frequency belongs to a pre-known limited interval, i.e. ω ∈ [̟ 1 , ̟ 2 ]. We will present some related definitions first and then show the related results and the interval-type frequency-dependent balanced truncation algorithm. where are defined as interval-type frequency-dependent controllability and observability Lyapunov equa-DRAFT tions of the continuous-time system . Furthermore, the solutions will be referred to as interval-type frequency-dependent controllability and observability Gramians of the continuous-time system (1) Definition 4.3 (Interval-type Frequency-dependent Balanced Realization): Given a linear continuoustime system (1) and a pre-specified frequency interval (ω ∈ [̟ 1 , ̟ 2 ]), the corresponding intervaltype frequency-dependent controllability and observability Gramians are equal and diagonal, i.e. the following Lyapunov equations simultaneously hold, then this particular realization will be referred to as interval-type frequencydependent balanced realization.\n\nTheorem 4.4 (Interval-type Frequency-dependent Balanced Truncation):\n\nGiven a linear continuoustime system (1) with a pre-specified frequency interval (ω ∈ [̟ 1 , ̟ 2 ]), and assume the system is given in interval-type frequency-dependent balanced realization with respect to the interval-type frequency-dependent Gramian: and where Z r = [I r×r 0 r×(n−r) ]. Furthermore, the truncated model G r (ω) possesses the following properties: 1). If the original system is stable then the reduced system is stable. 2). The approximation error between the original system model (1) and the truncated r th reduced model (43) over the given frequency interval (ω ∈ [̟ 1 , ̟ 2 ]) satisfies the following interval-type DRAFT error bound: where and 3). The approximation error between the original system model ( ) and the truncated r th reduced model (43) over entire frequency range satisfies the following EF-type error bound: where G r̟ 1 ,̟ 2 (ω) represents the corresponding interval-type frequency-dependent extended system of reduced system G r (ω), i.e. G r̟1,̟2 (ω) : where Proof: 1) It can be easily completed by the similar procedure adopted in the proof of stability preservation for classic FIBT . 2). Similar with the proof of SF-type error bound provided in Theorem 1, only the sketch of the proof for r = n − 1 case will be given below. We abuse notation a little bit for simplification. The error system E n (ω) between the original DRAFT system model G(ω) and the (n − 1) th order reduced model G n−1 (ω) can be represented by: Based on the error system E n (ω), one can construct a structure-preserving dilated system E n (ω) as follows: where B en , B dn , C en , C dn , N en are defined as ( )-( ). Now, if one choose two symmetrical Lyapunov variables Q en = Q * en ≥ 0 and P en = P * en as follows: According to the Generalized KYP Lemma, the dilated error system E n (ω) satisfies Therefore the error system satisfying the following inequality This completes the proof of interval-type error bound (44) for the r = n−1 case, the r = n−2, ..., 1 cases can be fulfilled step by step. 3). Similar with proof of EF-type error bound provided by SF-type FDBT, the proof of EF-type error bound (51) provided by interval-type FDBT can be completed in the same way. Proposition 4.5: the the following statements are true: DRAFT a). lim\n\n, n\n\nProof: a). It can be easily observed that From the interval-type frequency-dependent Lyapunov equation ( ), we know that which means lim b). Similar with the above proof, we have and Then lim furthermore, one can conclude that there exists a scalar µ < ∞ such that the following inequality holds since the convergence of matrices C ei ,N ei and B ei in cases that ̟ d → 0 are norm bounded. Thus the proof is completed. DRAFT The following equation holds for arbitrarily given invertible matrix T ∈ C n×n Proof: Lets consider the square of matrices of the left side and right side in (66), we have The above equation means that there exist matrices U, V such that where U is the matrix whose columns are eigenvectors of and V is the diagonal matrix whose diagonal elements are the corresponding eigenvalues. Furthermore, one get This completes the proof. With the above preparations, the corresponding interval-type FDBT algorithm (Algorithm 2) can be presented as follows. Remark 4.7: Compared with other balancing-related approaches, the most distinctive feature of the proposed interval-type FDBT method is that it gives an interval-type error bound (44). To the best of our knowledge, it is the first time to provide such an interval-type error bound using the interval-type index (4) in the model order reduction research areas. In particular, as revealed by Proposition 4, the interval-type error bound (44) always tends to be zero while the interval size tends to zero. This property means that the interval-type FDBT generally will gives rise to good in-band approximation performance while provides better in-band error bound simultaneously as DRAFT Algorithm 2 Interval-type FDBT Input: Full-order model (A, B, C, D), Frequency interval (̟ 1 , ̟ 2 ), order of reduced model (r). Step 1. Solve the interval-type frequency-dependent Lyapunov equations (41) Step 2. Get the frequency-dependent realization of the given system by coordinate transformation: where T (̟ 1 , ̟ 2 ) is a matrix that simultaneously diagonalize the matrices W c (̟ 1 , ̟ 2 ) and W o (̟ 1 , ̟ 2 ), i.e., Step 3. Compute the reduced-order model as: Output: Reduced-order model long as the size of frequency interval is small enough. Although the interval-type error bound may be increasing quickly with respect to the size of frequency interval. The interval-type error bound and its property are still appealing from a theoretical viewpoint. Remark 4.8: Again, the interval-type FDBT is also presented in a general form, i.e. the system matrices are allowed to be complex or real and the frequency interval might be symmetrical or asymmetrical. It can be easily verified that the interval-type FDBT will generate real reduced models for real full models if the given frequency interval is symmetrical (i.e ̟ 1 = −̟ 2 ). For applications with real system parameter restriction in asymmetrical frequency interval cases (ω ∈ [̟ 1 , ̟ 2 ]), the interval-type FDBT can also be applied in a conservative way by modifying the frequency as\n\nV. EXAMPLES\n\nExample 5.1: Lets consider a LTI system (1) with the following parameter matrices: Here we assume that the frequency of input signal belongs to an uncertain interval around ̟ = 0.\n\nIndexes computation formula\n\nErr(̟ l , r, FDBT) In Table , ̟ l represents the upper bound of the symmetrical frequency interval, r is the order of reduced model, G l Dr (ω), G l Sr (ω), G l Gr (ω), G l Ir (ω) represent the reduced models of order r generated by interval-type FDBT, SPA, FGBT and the classic FIBT for the l th random model, respectively. Fig. and Fig. display the experiment results on the these indices. Fig. validated that the interval-type error bound provided by interval-type FDBT generally is smaller than the EF-type error bound generated by FIBT and FDBT for the cases that the intervalsize is small enough (about ̟ < 1 in this experiment). Although the advantage on the error bound is restricted for small interval-size cases, it is suggested to take the interval-type FDBT as a feasible option even for medium interval-size cases. According to our experiment, the interval-type FDBT DRAFT generally also gives rise to better in-band approximation performance than FIBT and FGBT for medium interval-size cases (see Fig. for details). As has been pointed out in , approximating the ladder circuit is quite difficult in the framework of balancing related model order reduction approaches since neither the Hankel nor the singular values decay to any extent. In particular, its dynamic behavior over low frequency ranges is too complex to be well approximated due to the special distribution of its poles and zeros. Here we are interested to approximate this circuit in the following cases: Case I: the frequency of input signal belongs to a unknown neighborhood of dominating operating frequency point (̟ = 0). Case II: the frequency of input signal is known to be within the interval (ω ∈ [−0.5, +0.5]). At first, lets consider the case I and apply FIBT and generalized SPA to build reduced models. The frequency response of full model and reduced model of order 181 are shown in Fig. . As indicated by the visual inspections of the frequency response of the reduced vs. the full system from Fig. , the standard FIBT is failed to approximate the dynamic behaviors around ω = 0 even the order of reduced model is 181. Besides, it is surprising and remarkable that the generalized SPA method also failed here. Although the generalized SPA approach generally leads to good approximation performance around ω = 0, it is incapable to cope with this example. Now, lets resort to the proposed SF-type FDBT for dealing with the model reduction problem in case I. Our experiment results show that good approximants can be generated via SF-type FDBT as long as the order of reduced system is larger than 50. The frequency response of the full system and th order RLC Ladder Circuit system and the 181 th order approximants obtained via FIBT and SPA frequency ω 201 th order original RLC Ladder Circuit system 181 th order reduced system via FIBT 181 th order reduced system via generalized SPA (ρ=0) 181 th order reduced system via generalized SPA (ρ=1) 181 th order reduced system via generalized SPA (ρ=10) 181 th order reduced system via generalized SPA (ρ=100)\n\nBerlin/Heidelberg, Germany\n\nSpringer-Verlag 2005 45 2005 SIAM, Philadelphia 2008 Springer-Verlag Berlin, Heidelberg eeding of Joint 48th IEEE Conference on Decision and Control and 28th Chinese Control Conference 2009 77 8 2004 83 1 2010 26 1 1981 50 4 1989 6 2013 eeding of IEEE International Symposium on Computer-Aided Control System Design 2000 eeding of 23rd Conference on Decision and Control Las Vegas, NV, USA 1984 40 10 1995 12 1 1989 44 9 1999 86 5 2013 130 6 2008 54 5 2009 21 2 1990 127 3 2005 99 2014 55 9 2008 1996 Prentice-Hall Upper Saddle River, NJ 28 1 1996 50 1 2005 2013 Max Planck Institute Magdeburg Preprint MPIMD/13-02 9 3 1999 2005. 2005 54 4 2005\n\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"!pip install -U bitsandbytes accelerate transformers -q\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nimport torch\nos.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\"\n)\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-4B-Instruct-2507\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen3-4B-Instruct-2507\",\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    trust_remote_code=True\n)\n\ntorch.cuda.empty_cache()\n!pip install langchain==0.0.208  -q\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n!pip install rouge-score evaluate -q\nfrom rouge_score import rouge_scorer\nimport evaluate\nrouge = evaluate.load('rouge')\n\n!pip install bert-score -q\nfrom bert_score import BERTScorer\n\nscorer = BERTScorer(lang=\"en\", model_type=\"bert-base-multilingual-cased\")\n\n!pip install longdocfactscore -q\nimport nltk\nnltk.download('punkt_tab')\nnltk.download('punkt')\nfrom longdocfactscore.ldfacts import LongDocFACTScore\nldfacts_scorer = LongDocFACTScore(device=device)\nfrom tqdm.notebook import tqdm\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T18:07:36.868062Z","iopub.execute_input":"2026-01-08T18:07:36.868612Z","iopub.status.idle":"2026-01-08T18:10:38.267784Z","shell.execute_reply.started":"2026-01-08T18:07:36.868584Z","shell.execute_reply":"2026-01-08T18:10:38.266788Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m380.9/380.9 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hcuda\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"269a35bdc8784900a378415bd06b9cd2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a28bbb244c641749756129bcde80c84"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d153348f07f496f97bd5e5ab5752756"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4e62cbf900e4e33bc84b7dc3fe49f3e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/727 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"edddf4895fc84a5e9feb76e2341b0262"}},"metadata":{}},{"name":"stderr","text":"2026-01-08 18:08:20.891349: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1767895701.241712      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1767895701.331492      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1767895702.415966      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767895702.415992      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767895702.415995      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767895702.415998      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e897c9ba5d948b68635ce8102c7ae79"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0123f74db6a14b8c8df9eb65c6bb0031"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00003.safetensors:   0%|          | 0.00/3.96G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"efa2b2713f6940f39a169e51147e5b3a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00003.safetensors:   0%|          | 0.00/3.99G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"939101277c3f4edf8b139fd9b14d4fd2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00003.safetensors:   0%|          | 0.00/99.6M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a17d4faa14a246cd9da3133326a0949e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d1f0d31def25403dafe2a58d4571c31b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/238 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e17517ec4d744c878c5feaf62be5b52b"}},"metadata":{}},{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.1/155.1 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m100.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m72.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m94.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.26.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\nsigstore 4.1.0 requires pydantic<3,>=2, but you have pydantic 1.10.26 which is incompatible.\nsigstore-models 0.0.5 requires pydantic>=2.11.7, but you have pydantic 1.10.26 which is incompatible.\nsigstore-rekor-types 0.0.18 requires pydantic[email]<3,>=2, but you have pydantic 1.10.26 which is incompatible.\nydata-profiling 4.18.0 requires pydantic<3,>=2, but you have pydantic 1.10.26 which is incompatible.\nxmanager 0.7.1 requires sqlalchemy==1.2.19, but you have sqlalchemy 2.0.45 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngoogle-colab 1.0.0 requires jupyter-server==2.14.0, but you have jupyter-server 2.12.5 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\njaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\nthinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\nthinc 8.3.6 requires pydantic<3.0.0,>=2.0.0, but you have pydantic 1.10.26 which is incompatible.\nopencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\nopencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\njax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\nlangchain-core 0.3.79 requires pydantic<3.0.0,>=2.7.4, but you have pydantic 1.10.26 which is incompatible.\ncudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ngradio 5.49.1 requires pydantic<2.12,>=2.0, but you have pydantic 1.10.26 which is incompatible.\nalbumentations 2.0.8 requires pydantic>=2.9.2, but you have pydantic 1.10.26 which is incompatible.\nbigframes 2.26.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nopencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\npytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\ngoogle-genai 1.45.0 requires pydantic<3.0.0,>=2.0.0, but you have pydantic 1.10.26 which is incompatible.\npydantic-settings 2.11.0 requires pydantic>=2.7.0, but you have pydantic 1.10.26 which is incompatible.\nmcp 1.18.0 requires pydantic<3.0.0,>=2.11.0, but you have pydantic 1.10.26 which is incompatible.\npylibcudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c4d6d4d592e46948d6fd504a5fcf4ec"}},"metadata":{}},{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"44ff0757623c43aea8566f60efa621b8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"933dd01ddd3c44ebb784f5c22e071f0c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1bea91803086483f855c015fca457801"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de59fca16ef046b5b9f191985b9fb492"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/714M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"888a51efd4144e3ebb2a5a62263c495c"}},"metadata":{}},{"name":"stderr","text":"[nltk_data] Downloading package punkt_tab to /usr/share/nltk_data...\n[nltk_data]   Package punkt_tab is already up-to-date!\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7bd1dcbb6fa242fdbfe9e34bf236d5cb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b4acfef409745699dadde5e41d959f6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eea250a91bcb44c5b33cd6ea90cee047"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e10ff56b2681423e9d22e6a515081189"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8666627a5c734bd7841e767c407ccc40"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d424ac765ee3422385aea3a0f306db68"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/399 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"31380722c881444fa074d52b09b7ef95"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a51c73e7248248cfb654c0b2dc0050a4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"485466a955c741e38cedb82c359030e7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"104056d7cbfe44c086755cbdde93b185"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ade638136ab5468ea89bb3978802d354"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab1ee75fe5984ae39e0edbfc96f37ee6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f85db40bfdf24d2abc571e0d0165f3ba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9acde1e5947b4a4f880260ea01f4096a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"36fd72f0bc814981b9a41f748bdf67ab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4065bb341f64a03badd2a441175b61e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5285afcffd414c70849d73d6875f832e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/1.02G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"12ef640f6be54d2c82c973491e2c64a1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.02G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ecc7caec1da4d15ae00d2a11c1796e0"}},"metadata":{}}],"execution_count":9},{"cell_type":"markdown","source":"Функция для того чтобы отправить инструкцию модели","metadata":{}},{"cell_type":"code","source":"def qwen(article, prompt, max_tokens):\n    torch.cuda.empty_cache()\n    final_prompt = f\"{prompt}:\\n\\n{article}\"\n    messages = [{\"role\": \"user\", \"content\": final_prompt}]\n    model.eval()\n    with torch.no_grad():\n        inputs = tokenizer.apply_chat_template(\n            messages,\n            add_generation_prompt=True,\n            tokenize=True,\n            return_dict=True,\n            return_tensors=\"pt\",\n            truncation=False\n        ).to(model.device)\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=max_tokens,\n            do_sample=False\n        )\n    final_summary = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:], skip_special_tokens=True)\n    return final_summary","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T18:22:36.409112Z","iopub.execute_input":"2026-01-08T18:22:36.409431Z","iopub.status.idle":"2026-01-08T18:22:36.414926Z","shell.execute_reply.started":"2026-01-08T18:22:36.409402Z","shell.execute_reply":"2026-01-08T18:22:36.414206Z"}},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":"Функция для обработки чанков статьи, встроенный text_splitter можно отключить","metadata":{}},{"cell_type":"code","source":"def summarize(article, prompt_0, prompt_1,chunk_size=1500, max_tokens_0=700, max_tokens_1=700, text_splitter = True):\n    torch.cuda.empty_cache()\n    if text_splitter:\n        text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n            tokenizer=tokenizer,\n            chunk_size=chunk_size,\n            chunk_overlap=chunk_size//20,\n            separators=[\"\\n\\n\", \"\\n\", \". \"],\n            keep_separator=True\n        )\n        article_chunks = text_splitter.split_text(article)\n    else:\n        article_chunks = article\n    summaries = []\n    for i, chunk in tqdm(enumerate(article_chunks), total=len(article_chunks)):\n        try:\n            chunk_summary = qwen(article=chunk, prompt=prompt_0, max_tokens=max_tokens_0)\n            print(f\"\\nSummary chunk number {i}\")\n            print(chunk_summary)\n        except torch.cuda.OutOfMemoryError as e:\n            print(f\"⚠️ Ошибка нехватки памяти CUDA, слишком длинный чанк: {str(e)[:100]}...\")\n            break\n\n        summaries.append(chunk_summary)\n        torch.cuda.empty_cache()\n    combined_summary = \"\\n\".join(summaries)\n    \n    try:\n        final_summary = qwen(article=combined_summary, prompt=prompt_1, max_tokens=max_tokens_1)\n    except torch.cuda.OutOfMemoryError as e:\n        print(f\"⚠️ Ошибка нехватки памяти CUDA при итоговой суммаризации: {str(e)[:100]}...\")\n    return final_summary","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T18:43:50.395031Z","iopub.execute_input":"2026-01-08T18:43:50.395550Z","iopub.status.idle":"2026-01-08T18:43:50.402107Z","shell.execute_reply.started":"2026-01-08T18:43:50.395522Z","shell.execute_reply":"2026-01-08T18:43:50.401371Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"rouge_list = []\nbert_list = []\nldfacts_list = []\nsummaries_list = []\n\ndef to_full_text(data_dict):\n    text = ''\n    for key, value in data_dict.items(): \n        text += f\"{key}\\n\\n{value}\\n\\n\"\n    return text\n\n\nfor i in range(5):\n    print(f\"\\n=== Статья {i} ===\")\n    #извлекаем чанки, абстракт, названия разделов\n    article_dict = ast.literal_eval(df['dict_test'].iloc[i]) #словарь для i-той статьи \n    chunked_article = list(article_dict.values()) #список чанков\n    chapters_list = list(article_dict.keys()) #список глав\n    print('Список разделов')\n    display(pd.DataFrame(chapters_list))\n    abstract = df['abstract'].iloc[i] \n    article = to_full_text(article_dict) #полный текст статьи с ключами и значениями словаря\n    \n    print(f\"Токенов в статье: {len(tokenizer.encode('\\n'.join(chunked_article)))}\")\n    prompt_0 = \"You're a science editor. Briefly summarize this fragment of the scientific text in original language. Do not add information that is not in the source texts\"\n    prompt_1 = \"You're a science editor. Based on the following summaries of the parts of the article, create a single, coherent and concise summary of the entire scientific article in original language, highlighting the common goal, methods, key results and conclusion. Do not add information that is not in the source texts\"\n    \n    summary_full = summarize(chunked_article, prompt_0 = prompt_0, prompt_1 = prompt_1, max_tokens_0 = 500, max_tokens_1 = 700, text_splitter = False)\n    print(f'\\nSummary article number {i}')\n    print(summary_full)\n    summaries_list.append({\n        'article_id': i,\n        'original_text': article,\n        'abstract': abstract,\n        'summary': summary_full\n    })\n\n    results = rouge.compute(\n    predictions=[summary_full],\n    references=[article],\n    use_stemmer=True\n    )\n    print(\"ROUGE Metrics:\")\n\n    rouge_dict = {}\n    for key, value in results.items():\n        print(f\"{key}: {value:.4f}\")\n        rouge_dict[key] = value\n    rouge_list.append(rouge_dict)\n\n    _, _, F1 = scorer.score([summary_full], [abstract])\n    bert_f1 = F1.item()\n    print(f\"\\nBERTScore: F1 = {bert_f1:.4f}\")\n    bert_list.append(bert_f1)\n\n    ldfacts_sum = ldfacts_scorer.score_src_hyp_long([article], [summary_full])\n    ldfacts_abs = ldfacts_scorer.score_src_hyp_long([article], [abstract])\n    print(f\"LongDocFACTScore for sum: {ldfacts_sum[0]}\")\n    print(f\"LongDocFACTScore for abstract: {ldfacts_abs[0]}\")\n    ldfacts_list.append({\n        'for_summary': ldfacts_sum[0],\n        'for_abstract': ldfacts_abs[0]\n    })","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T18:44:03.831242Z","iopub.execute_input":"2026-01-08T18:44:03.831831Z"}},"outputs":[{"name":"stdout","text":"\n=== Статья 0 ===\nСписок разделов\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"                                                   0\n0           I. INTRODUCTION AND PROBLEM FORMULATIONS\n1                                              DRAFT\n2                               II. FUNDAMENTAL TOOL\n3  IV. FREQUENCY-DEPENDENT BALANCED TRUNCATION OV...\n4  Theorem 4.4 (Interval-type Frequency-dependent...\n5                                                , n\n6                                        V. EXAMPLES\n7                        Indexes computation formula\n8                         Berlin/Heidelberg, Germany","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>I. INTRODUCTION AND PROBLEM FORMULATIONS</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>DRAFT</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>II. FUNDAMENTAL TOOL</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>IV. FREQUENCY-DEPENDENT BALANCED TRUNCATION OV...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Theorem 4.4 (Interval-type Frequency-dependent...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>, n</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>V. EXAMPLES</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Indexes computation formula</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Berlin/Heidelberg, Germany</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":"Токенов в статье: 6903\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/9 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"54df005f5e0e436cbbc42f150df6341e"}},"metadata":{}},{"name":"stdout","text":"\nSummary chunk number 0\nWe study model order reduction for linear time-invariant continuous-time systems with state vector \\( x(t) \\in \\mathbb{C}^n \\), input \\( u(t) \\in \\mathbb{C}^m \\), and output \\( y(t) \\in \\mathbb{C}^p \\), described by matrices \\( A \\in \\mathbb{C}^{n\\times n} \\), \\( B \\in \\mathbb{C}^{n\\times m} \\), \\( C \\in \\mathbb{C}^{p\\times n} \\), \\( D \\in \\mathbb{C}^{p\\times m} \\). Large system orders \\( n \\) lead to high computational and storage costs, making simulation, optimization, and design challenging. Model order reduction (MOR) is thus essential to approximate the system by a reduced-order model \\( G_r(j\\omega) \\) with \\( r < n \\). Balanced truncation, particularly Lyapunov balanced truncation introduced by Moore, is widely used due to its stability preservation and a priori error bounds over the entire frequency range. However, in practical applications where input signals operate within a known finite frequency range \\( \\omega \\in [\\omega_1, \\omega_2] \\), the objective shifts to minimizing a finite-frequency performance index. Standard balanced truncation (called frequency-independent balanced truncation, FIBT) is not suited for this, as it provides entire-frequency error bounds and does not optimize in-band performance. To improve in-band approximation, balancing-related methods have been developed: (1) Singular perturbation approximation (SPA), which matches the transfer functions at \\( \\omega = 0 \\), offering good low-frequency performance; (2) Frequency-weighted balanced truncation (FWBT), which uses frequency weighting functions to improve frequency-specific performance but requires iterative design and increases model order; (3) Frequency-limited Grammians balanced truncation (FGBT), which extends standard Gramians to finite frequencies but may fail due to non-positive semi-definite solutions and lacks error bounds; modified FGBT schemes provide error bounds. A common feature of these finite-frequency approaches is that they still use entire-frequency performance indices to evaluate actual finite-frequency approximation performance.\n\nSummary chunk number 1\nThe task is to construct a reduced-order model of order 3 that well approximates the frequency-domain dynamic behavior of the original model near ω = 0. Among balancing-related methods, generalized SPA is the most suitable for this problem, and the proposed SF-type FDBT method is also applicable. Sigma plots of the error systems from generalized SPA and SF-type FDBT are shown in Fig. and Fig., respectively, both of which exhibit small approximation errors near ω = 0. The local and global approximation performance can be balanced by adjusting the user-defined parameter (ρ for generalized SPA and ε for SF-type FDBT). In this example, generalized SPA and SF-type FDBT perform similarly, though significant differences may occur in other cases (e.g., Example 3, where only SF-type FDBT is effective). Among balancing-related methods, FGBT is the exact method developed for interval-type finite-frequency model reduction, while the proposed interval-type FDBT is also designed for such problems. The key difference lies in the type of error bounds: FGBT provides bounds over the entire frequency range, whereas interval-type FDBT provides bounds only over a pre-specified frequency interval. Since operating frequencies are assumed to lie within the given interval, the interval-type error bounds are sufficient for performance estimation. Compared to standard FIBT, both FGBT and interval-type FDBT improve approximation performance over a specified frequency interval, with interval-type FDBT offering better approximation and smaller error bounds simultaneously. As noted in Remark 4, interval-type FDBT provides small error bounds as long as the frequency interval size is sufficiently small. This is demonstrated via a randomization experiment involving 100 stable systems of order 4, with system matrices generated using normal distributions. Several performance indices are defined in Table to compare the average performance between FGBT and interval-type FDBT.\n\nSummary chunk number 2\nThe Kalman-Yakubovich-Popov (KYP) Lemma is a fundamental tool in system and control theory, enabling the proof and interpretation of error bounds in frequency-domain methods. Iwasaki and Hara generalized the KYP Lemma to finite-frequency cases, forming the Generalized KYP Lemma, which is used here to analyze stability and error bounds in frequency-dependent system extensions. For a continuous-time system, the lemma establishes equivalence between a frequency-domain inequality and the existence of symmetric positive definite matrices satisfying a matrix inequality involving a user-defined scalar ǫ, which must satisfy ǫ = −(j̟ − λ_i) to ensure invertibility of (ǫI + j̟I − A), where λ_i are eigenvalues of matrix A. A Moebius transformation generates an SF-type frequency-dependent extended system. If the original system is Hurwitz stable and ǫ > 0, or if it is unstable and 0 < ǫ < min(ǫ⁺_i), the extended system is stable. SF-type frequency-dependent Lyapunov equations are defined for controllability and observability Gramians, and a balanced realization is defined when these Gramians are equal and diagonal. Propositions show that as ǫ → ∞, the extended system recovers the original system. The SF-type frequency-dependent balanced truncation method reduces system order at a given frequency ̟, with approximation error bounds derived via a dilated error system and the Generalized KYP Lemma. The error bounds are frequency-dependent at ̟ and extend to the entire frequency range via standard FIBT. The algorithm involves solving Lyapunov equations, performing coordinate transformation, and truncating the system. The method works for unstable systems by selecting appropriate ǫ, though the reduced model may not be stable. The approach is valid in complex settings, and under real system and ̟ = 0 assumptions, all matrices remain real. It is distinct from SPA and other balancing methods, offering a new alternative for model order reduction, especially at low frequencies.\n\nSummary chunk number 3\nWe present results for systems with operating frequency in a known interval ω ∈ [̟₁, ̟₂]. First, we define interval-type frequency-dependent controllability and observability Lyapunov equations for continuous-time systems. The solutions are called interval-type frequency-dependent controllability and observability Gramians. Definition 4.3: Given a linear continuous-time system (1) and a pre-specified frequency interval ω ∈ [̟₁, ̟₂], if the interval-type frequency-dependent controllability and observability Gramians are equal and diagonal, and the corresponding Lyapunov equations hold simultaneously, then the realization is referred to as interval-type frequency-dependent balanced realization.\n\nSummary chunk number 4\nGiven a linear continuous-time system (1) with a pre-specified frequency interval (ω ∈ [̟₁, ̟₂]), and assuming the system is given in interval-type frequency-dependent balanced realization with respect to the interval-type frequency-dependent Gramian, where Zᵣ = [Iᵣ×ᵣ 0ᵣ×(n−r)]. The truncated r-th reduced model Gᵣ(jω) has the following properties:  \n1) If the original system is stable, the reduced system is also stable.  \n2) The approximation error over the given frequency interval satisfies an interval-type DRAFT error bound, where and  \n3) The approximation error over the entire frequency range satisfies an EF-type error bound, where Gᵣ̟₁,̟₂(jω) is the interval-type frequency-dependent extended system of the reduced model Gᵣ(jω), defined as:  \nProof:  \n1) Stability preservation follows by a procedure similar to that in the proof of stability preservation for classic FIBT.  \n2) Similar to the proof of SF-type error bound in Theorem 1, only the sketch is given for the r = n−1 case. Using a simplified notation, the error system Eₙ(jω) between the original DRAFT model G(jω) and the (n−1)th-order reduced model Gₙ₋₁(jω) is represented as: Based on this error system, a structure-preserving dilated system Eₙ(jω) is constructed with Bₑₙ, B𝒹ₙ, Cₑₙ, C𝒹ₙ, Nₑₙ defined as in ( )–( ). By choosing two symmetrical Lyapunov variables Qₑₙ = Q*ₑₙ ≥ 0 and Pₑₙ = P*ₑₙ, and applying the Generalized KYP Lemma, the dilated error system satisfies certain conditions, leading to the error system satisfying the inequality, thus proving the interval-type error bound (44) for r = n−1; the cases r = n−2, ..., 1 follow similarly.  \n3) The proof of EF-type error bound (51) follows the same approach as in the proof of EF-type error bound for SF-type FDBT.  \n\nProposition 4.5: The following statements hold: DRAFT  \na). lim\n\nSummary chunk number 5\nProof: It is observed that from the interval-type frequency-dependent Lyapunov equation, the limit implies lim. Similarly, under the same conditions, lim, and thus there exists a scalar µ < ∞ such that the inequality holds due to the norm boundedness of matrices C ei, N ei, and B ei as ̟ d → 0. This completes the proof. For an invertible matrix T ∈ C^n×n, consider the square of matrices on both sides of (66). This implies the existence of matrices U and V such that U consists of eigenvectors of and V is a diagonal matrix with corresponding eigenvalues. From this, it follows that the result is established. The interval-type FDBT algorithm (Algorithm 2) is then presented. Remark 4.7: The proposed interval-type FDBT method features an interval-type error bound (44), which is the first such bound in model order reduction using the interval-type index (4). As shown in Proposition 4, the error bound tends to zero as the interval size tends to zero, indicating good in-band approximation and a reliable error bound. Algorithm 2: Input: Full-order model (A, B, C, D), frequency interval (̟₁, ̟₂), reduced model order r. Step 1: Solve the interval-type frequency-dependent Lyapunov equations (41). Step 2: Obtain the frequency-dependent realization via coordinate transformation: where T(̟₁, ̟₂) simultaneously diagonalizes W_c(̟₁, ̟₂) and W_o(̟₁, ̟₂). Step 3: Compute the reduced-order model as: Output: Reduced-order model. The method provides good in-band approximation when the frequency interval size is small enough, though the error bound may increase with interval size. The error bound and its properties are theoretically significant. Remark 4.8: The interval-type FDBT is presented in general form, allowing complex or real system matrices and symmetrical or asymmetrical frequency intervals. It generates real reduced models for real full models when the frequency interval is symmetrical (̟₁ = −̟₂). For real system parameter constraints in asymmetrical intervals (ω ∈ [̟₁, ̟₂]), the method can be applied conservatively by modifying the frequency.\n\nSummary chunk number 6\nWe consider an LTI system (1) with the given parameter matrices, assuming the input signal frequency lies within an uncertain interval centered at ̟ = 0.\n\nSummary chunk number 7\nErr(̟ l , r, FDBT) — in the table, ̟ l denotes the upper bound of the symmetrical frequency interval, r is the order of the reduced model, and G l Dr (ω), G l Sr (ω), G l Gr (ω), G l Ir (ω) represent reduced models of order r generated by interval-type FDBT, SPA, FGBT, and classic FIBT, respectively, for the l-th random model. Figures show experimental results on these indices. The interval-type error bound provided by interval-type FDBT is generally smaller than the EF-type error bound from FIBT and FDBT when the interval size is small (about ̟ < 1). Although this advantage is limited to small interval sizes, interval-type FDBT is suggested as a feasible option even for medium interval sizes. Experimental results show that interval-type FDBT generally provides better in-band approximation performance than FIBT and FGBT for medium interval sizes. Approximating a ladder circuit is difficult in model order reduction methods due to insufficient decay of Hankel or singular values, especially in low-frequency ranges due to complex pole and zero distribution. The study focuses on two cases: Case I — input signal frequency in an unknown neighborhood of the dominant operating frequency (̟ = 0); Case II — input signal frequency known to lie in [−0.5, +0.5]. For Case I, FIBT and generalized SPA are applied to build reduced models of order 181. Visual inspection of frequency responses shows that standard FIBT fails to approximate dynamics near ω = 0, and generalized SPA also fails despite generally good performance near ω = 0. The proposed SF-type FDBT is applied; experimental results show that good approximations can be generated as long as the reduced model order exceeds 50.\n\nSummary chunk number 8\nThe fragment lists publication details, including publishers, years, titles, and conference names, such as proceedings from IEEE Conference on Decision and Control, Chinese Control Conference, and IEEE International Symposium on Computer-Aided Control System Design, as well as books and preprints from various institutions and years. No additional information is provided beyond the listed data.\nSummary article number 0\nThe article addresses model order reduction (MOR) for linear time-invariant continuous-time systems with large orders, where computational and storage costs hinder simulation, optimization, and design. The primary goal is to construct a reduced-order model of order 3 that accurately approximates the system's dynamic behavior near ω = 0, particularly within a known finite frequency interval. Standard balanced truncation (FIBT) is inadequate for this finite-frequency objective, as it provides entire-frequency error bounds and does not optimize in-band performance. To improve finite-frequency approximation, balancing-related methods such as singular perturbation approximation (SPA), frequency-weighted balanced truncation (FWBT), and frequency-limited Grammians balanced truncation (FGBT) are considered. However, these methods either require iterative design, increase model order, or suffer from non-positive semi-definite Gramians and lack of error bounds.  \n\nThe study introduces an interval-type frequency-dependent balanced truncation (FDBT) method, which extends standard balanced truncation to finite frequency intervals. This method defines interval-type frequency-dependent controllability and observability Gramians, constructs a balanced realization via coordinate transformation, and derives both interval-type and entire-frequency error bounds. The reduced model preserves stability and provides a frequency-dependent error bound over the specified interval [̟₁, ̟₂], with the interval-type error bound tending to zero as the interval size decreases. The method is valid for both stable and unstable systems, works in complex settings, and yields real reduced models when applied to real systems with symmetric frequency intervals.  \n\nKey results show that interval-type FDBT outperforms standard FIBT and FGBT in in-band approximation, especially for medium-sized frequency intervals, with smaller error bounds and better performance near ω = 0. Experimental validation on random stable systems and a ladder circuit demonstrates that interval-type FDBT provides superior low-frequency approximation, particularly when the operating frequency is near zero. The method is distinct from SPA and other balancing approaches, offering a new, theoretically grounded alternative with explicit error bounds tailored to finite-frequency applications.  \n\nIn conclusion, the proposed interval-type FDBT method provides a robust, stable, and performance-optimized approach for reducing high-order systems with known finite-frequency operating ranges, enabling accurate and reliable approximation of dynamic behavior in practical applications.\nROUGE Metrics:\nrouge1: 0.1276\nrouge2: 0.0642\nrougeL: 0.0740\nrougeLsum: 0.1088\n\nBERTScore: F1 = 0.6902\nLongDocFACTScore for sum: -4.1922071530268745\nLongDocFACTScore for abstract: -5.1785284042358395\n\n=== Статья 1 ===\nСписок разделов\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"                                                    0\n0                                        INTRODUCTION\n1   BACKGROUND: RECURRENT NEURAL NETWORK LANGUAGE ...\n2                   AUGMENTING THE CROSS-ENTROPY LOSS\n3       THEORETICALLY DRIVEN REUSE OF WORD EMBEDDINGS\n4                                        RELATED WORK\n5                                         EXPERIMENTS\n6                       MODEL AND TRAINING HIGHLIGHTS\n7   EMPIRICAL VALIDATION FOR THE THEORY OF REUSING...\n8              RESULTS ON PTB AND WIKITEXT-2 DATASETS\n9                                 QUALITATIVE RESULTS\n10                                         CONCLUSION\n11              APPENDIX A MODEL AND TRAINING DETAILS\n12        B METRIC FOR CALCULATING SUBSPACE DISTANCES\n13                                               2001","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>INTRODUCTION</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>BACKGROUND: RECURRENT NEURAL NETWORK LANGUAGE ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>AUGMENTING THE CROSS-ENTROPY LOSS</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>THEORETICALLY DRIVEN REUSE OF WORD EMBEDDINGS</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>RELATED WORK</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>EXPERIMENTS</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>MODEL AND TRAINING HIGHLIGHTS</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>EMPIRICAL VALIDATION FOR THE THEORY OF REUSING...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>RESULTS ON PTB AND WIKITEXT-2 DATASETS</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>QUALITATIVE RESULTS</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>CONCLUSION</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>APPENDIX A MODEL AND TRAINING DETAILS</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>B METRIC FOR CALCULATING SUBSPACE DISTANCES</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>2001</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":"Токенов в статье: 5944\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/14 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"93e11690146f4bd89db326aafa1a029a"}},"metadata":{}},{"name":"stdout","text":"\nSummary chunk number 0\nNeural network models have made significant advances in NLP tasks like speech recognition, sentiment analysis, text summarization, and machine translation. Although recurrent neural networks excel at modeling long-range word dependencies, current RNNLMs rely on a classical classification framework with two key limitations: (1) no natural metric is assumed on output classes, despite the existence of a well-defined metric space in language modeling via word embeddings; and (2) inputs and outputs are treated as isolated, whereas in language modeling they reside in the same space, leading to inefficiency in model complexity and information utilization. To address these issues, we propose a novel loss framework comprising two interlinked improvements: (1) augmenting cross-entropy loss with a term that minimizes KL-divergence between model predictions and a target distribution derived from word embedding similarities; and (2) theoretically analyzing this loss leads to tying input and output matrices by reusing the input word embedding matrix as the output classification matrix. The framework is empirically validated on the Penn Treebank corpus, showing superior performance over conventional models, and confirmed on the Wikitext-2 dataset, demonstrating consistent gains across different datasets.\n\nSummary chunk number 1\nIn any variant of a recurrent neural network language model (RNNLM), the goal is to predict the next word at position t in a sequence of one-hot word tokens (y*₁, ..., y*ₙ) using a function f, without making assumptions about the specific structure of the recurrent units. The loss is primarily the cross-entropy between the model prediction distribution yₜ and the observed one-hot target y*ₜ. Since cross-entropy and Kullback-Leibler (KL) divergence are equivalent when the target is one-hot, the loss can be rewritten as KL divergence. The optimization process is interpreted as minimizing the distance between the model prediction distribution (yₜ) and the empirical target distribution (y*ₜ), which, with sufficient training data, approximates the actual target distribution. In the proposed framework, KL divergence is used instead of cross-entropy due to its intuitive interpretation as a distance between distributions, even though the two are not equivalent in this context.\n\nSummary chunk number 2\nWe propose augmenting the conventional cross-entropy loss with an additional term. The augmented loss includes a hyperparameter α and a modified prediction distribution ŷt, obtained by dividing the logits of the standard prediction distribution y_t by a temperature parameter τ. We define ỹt as a probability distribution estimating the true data distribution (conditioned on word history) such that Eỹt = Ey*t. The goal is to minimize the distribution distance between the prediction and this estimated true distribution. In an ideal case where the true distribution is known (ỹt = Ey*t), stochastic gradient descent updates the logits based on all class labels with non-zero conditional probability, with step sizes proportional to ỹt,i. This update is less noisy than standard training since the target distribution is exact and deterministic, leading to improved supervision unless all examples belong to a single class. A recent work applying this framework estimates ỹ using large models' predictions. We propose estimating ỹ by finding the target word vector u_t and computing the inner product of u_t with all other word vectors to obtain an unnormalized distribution, normalized via softmax with the same temperature τ. This yields a distribution measuring word vector similarity and assigning higher probabilities to similar words. The estimation of ỹ is iterative and initially uninformative, but as training progresses, it is expected to better capture word statistics and provide a more accurate estimate of the true data distribution.\n\nSummary chunk number 3\nWe theoretically analyze the augmented loss under a specific setting: input embedding dimension equals RNN hidden state dimension (dₓ = dₕ), bias b = 0 (yₜ = W hₜ), only augmented loss is used, training loss is zero, and temperature τ is large. At high τ, the augmented loss matches the model’s logits to those of the informative labels ỹ. Using first-order approximation of the exponential function and assuming average zero inner product between uₜ and lⱼ, we show that the loss drives the model logits toward ỹ’s logits. Given zero training loss, gradients vanish, and under full-rank assumptions, the column spaces of W and Lᵀ are equal. Introducing matrix A such that W = LᵀA, we show that the output projection can be realized via the embedding matrix transpose and a linear mapping h → Ah. This reveals that the augmented loss implicitly constrains the output probability space to a subspace defined by the embedding matrix. We propose making this mechanism explicit by setting W = Lᵀ and b = 0 during training, which reduces network size and eliminates redundant work by the augmented loss.\n\nSummary chunk number 4\nSince their introduction, various improvements have been proposed for RNNLMs, such as different dropout methods, novel recurrent units, and pointer networks, but none addressed the loss structure. To the best of our knowledge, our work is the first to introduce a new loss framework. Our approach is related to prior work that estimates a more informed data distribution and augments the conventional loss with KL divergence between model prediction and estimated data distributions; however, that work estimates the data distribution by training large networks and applying it to smaller networks, whereas we improve learning through knowledge transfer within the same network in a self-contained manner. The present work is based on a report published in . We have also encountered a concurrent preprint where authors reuse the word embedding matrix in output projection to improve language modeling, but their approach is purely empirical without theoretical justification. The idea of using shared input and output representations has been explored before, with some language models interpreted as neural networks with shared embeddings; however, such sharing was implicitly built into these models rather than proposed as an improvement over a baseline. Consequently, the potential for improvement via shared representations was not explicitly pursued.\n\nSummary chunk number 5\nIn our experiments, we use the Penn Treebank (PTB) corpus and the Wikitext-2 dataset. PTB is a standard benchmark dataset consisting of 923k training, 73k validation, and 82k test words. The version used selects the most frequent 10k words for the vocabulary, with the rest replaced by an <unk> token. Wikitext-2 is a recent alternative to PTB, containing 2,088k training, 217k validation, and 245k test tokens, with a vocabulary of 33,278 words. Compared to PTB, Wikitext-2 is roughly twice as large in dataset size and three times larger in vocabulary.\n\nSummary chunk number 6\nWe base our baseline model on an LSTM language model. It consists of a 2-layer LSTM with equal hidden units per layer, using three network sizes: small (200 units), medium (650 units), and large (1500 units). Training is performed using stochastic gradient descent with a variant of the dropout method. Further training details are provided in section A of the appendix. The baseline is referred to as variational dropout LSTM (VD-LSTM).\n\nSummary chunk number 7\nIn Section 4, it was shown that the chosen loss augmentation scheme constrains the output projection matrix to be close to the input embedding matrix without explicitly setting W = L^T. To validate this, a 20,000-word sequence from the PTB training set was used to train a 2-layer LSTM with 300 units, using loss augmentation with a parameter β that controls the proportion of augmented loss. The input embedding matrix rows were constrained to have unit norm to ensure stable training. After training, the distance between the subspaces spanned by the input embedding matrix L and the output projection matrix W was measured using a residual norm metric, where 0 indicates identical subspaces and 1 indicates orthogonality. Experiments show that as β increases from 0 to 1, the subspace distance drops rapidly from nearly 1 to around 0.06. When β = 1, the distance remains low even at low temperatures (τ = 2), with further reduction at higher temperatures. These results confirm that the loss augmentation drives W to align with L^T, suggesting that explicitly setting W = L^T is not merely regularization but an optimal choice in the framework. The performance gains from combining the two proposed improvements are empirically unresolved and are addressed in the next section.\n\nSummary chunk number 8\nTo evaluate the effectiveness of proposed improvements, four models are trained for each network size: (1) VD-LSTM, (2) VD-LSTM + augmented loss (AL), (3) VD-LSTM + reused embeddings (RE), and (4) V D-LSTM + AL and RE (REAL). Validation perplexities on the PTB corpus for small (panel a) and large (panel b) networks are shown. All models with AL, RE, or both outperform the baseline. Table compares final validation and test perplexities on PTB and Wikitext-2. For both datasets, AL and RE individually improve performance, with REAL achieving the best results. AL provides greater gains for small networks, as small models are inflexible and benefit from more informative data distribution via AL. For larger datasets, RE outperforms AL, indicating that enforcing proximity between input embedding and output projection spaces is more effective in large models. RE reduces model size while preserving representational capacity. The best model (VD-LSTM+REAL) surpasses previous work using conventional frameworks, including large ensembles. VD-RHN+RE achieves the best performance, improving VD-RHN by 2.5 in perplexity. Small models retrained with initialization from first session show improved performance for non-baseline models. Results on equally sized partitions of Wikitext-2 (1044K and 929K) show AL outperforms RE despite threefold larger embedding size, supporting that AL improves information extraction from data. Table compares performance across models on PTB and Wikitext-2 partitions, consistent with PTB results in terms of training set size.\n\nSummary chunk number 9\nA key feature of the framework is the explicit mechanism that assigns word probabilities based not only on observed output statistics but also on metric similarity between words. In the Penn Treebank, this leads to lower <unk> token probabilities compared to the baseline network, because <unk> is an aggregated token not closely aligned with specific words in embedding space. The same effect is observed for frequent words like \"a\", \"an\", and \"the\", which are not closely related to specific words. Additionally, the model assigns higher probabilities to words nearby the target in embedding space, sometimes predicting semantically related words even when the target is not accurately captured. Examples from the PTB test set are provided in Table 4, comparing 1500-unit VD-LSTM and VD-LSTM +REAL. The prediction performance of VD-LSTM +REAL is similar to that of VD-LSTM +RE for the large network.\n\nSummary chunk number 10\nWe introduce a novel loss framework for language modeling that uses the metric structure of word embeddings to generate a more informed data distribution than one-hot targets, improving learning. Theoretically, the framework allows reusing the input embedding matrix in the output projection layer, reducing trainable parameters. Empirically, we validate this framework on Penn Treebank and Wikitext-2, showing it outperforms conventional methods, with the simple reuse of embeddings sufficient for large networks. The improvements are not limited to vanilla language modeling and apply to tasks like neural machine translation, speech recognition, and text summarization, offering performance gains and reduced parameter count—especially beneficial for large vocabularies.\n\nSummary chunk number 11\nTraining begins with a learning rate of 1, decaying at constant rates of 5, 10, and 1 for small, medium, and large networks respectively, with decay rates of 0.9 and 0.97 for small/medium and large networks. Backpropagation unrolls the network for 35 steps on both PTB and Wikitext-2 datasets. Gradient clipping rescales gradients using global norm: 5 for small and medium, 6 for large networks. Dropout is applied as in Gal, using the same mask across unrolled steps; hidden state dropout masks are tied and shared across layer propagation and input to next layer. Dropout is not applied in the input embedding layer; the same dropout probability is used for inputs and hidden states. For PTB, dropout probabilities are 0.7, 0.5, and 0.35 for small, medium, and large networks; for Wikitext-2, they are 0.8 and 0.6 for small and medium networks. When using augmented loss (AL), temperature τ is set to 20. Empirically, setting α (augmented loss weight) as α = γτ works well; γ is set between 0.5–0.8 for PTB and 1.0–1.5 for Wikitext-2. No sudden performance deterioration is observed with moderate variations in τ or α.\n\nSummary chunk number 12\nThe section describes a metric for computing the distance between two matrices X and Y based on the principle angles between their column subspaces. The method involves three steps: (1) obtaining orthonormal matrices U and V such that span(U) = span(X) and span(V) = span(Y) via QR decomposition; (2) computing the projection S = U U^T V of V onto U, and the residual R = V − S; (3) defining the distance metric as d, where d² = (1/C) trace(R^T R), with C being the number of columns. This distance is expressed as d² = (1/C) Σ sin²(θ_i), where θ_i are the principal angles between the subspaces, corresponding to the singular values of U^T V. The metric is nonnegative, zero only when span(X) = span(Y), symmetric in U and V, and ranges between 0 and 1.\n\nSummary chunk number 13\nThe fragment refers to \"Recurrent highway networks,\" a topic associated with research publications and preprints from various years (1973, 1993, 1997, 2007, 2010, 2013, 2014, 2015, 2016), including arXiv preprints and conference papers such as at the 24th International Conference on Machine Learning (2013), Interspeech (2016), and others. It lists specific dates, conference names, and authors (e.g., Jan Cernocký), but no detailed content is provided.\nSummary article number 1\nThe article proposes a novel loss framework for recurrent neural network language models (RNNLMs) that addresses two key limitations of conventional models: the absence of a natural metric on output classes and the treatment of inputs and outputs as isolated. The framework introduces two interlinked improvements: (1) an augmented cross-entropy loss that minimizes the Kullback-Leibler divergence between model predictions and a target distribution derived from word embedding similarities, thereby leveraging the intrinsic metric structure of language embeddings; and (2) a theoretical and empirical mechanism that ties the output projection matrix to the input embedding matrix by reusing the input embedding matrix as the output classification matrix, reducing model complexity and improving information efficiency. The method is validated on the Penn Treebank and Wikitext-2 datasets, where models incorporating both improvements outperform baseline RNNLMs, with the combined approach (REAL) achieving superior performance, especially in large models where embedding reuse yields significant gains in parameter efficiency and accuracy. The results demonstrate that the proposed framework enhances prediction quality by assigning higher probabilities to semantically similar words and reducing reliance on the <unk> token, while also enabling a reduction in trainable parameters through shared representations. The findings confirm that the loss augmentation improves learning by providing more informative supervision, and that explicit reuse of input embeddings is both theoretically justified and empirically effective, offering a scalable and efficient improvement for language modeling and related NLP tasks.\nROUGE Metrics:\nrouge1: 0.0863\nrouge2: 0.0356\nrougeL: 0.0539\nrougeLsum: 0.0717\n\nBERTScore: F1 = 0.7024\nLongDocFACTScore for sum: -5.553670024871826\nLongDocFACTScore for abstract: -4.481736040115356\n\n=== Статья 2 ===\nСписок разделов\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"                                                  0\n0                                      INTRODUCTION\n1                                      OUR APPROACH\n2   MULTI-VIEW LEARNING OF ACOUSTIC WORD EMBEDDINGS\n3             RECURRENT NEURAL NETWORK ARCHITECTURE\n4                                      RELATED WORK\n5                           EXPERIMENTS AND RESULTS\n6                                              DATA\n7           MODEL DETAILS AND HYPERPARAMETER TUNING\n8                   EFFECTS OF DIFFERENT OBJECTIVES\n9                                         Objective\n10                                           Method\n11                            WORD SIMILARITY TASKS\n12              VISUALIZATION OF LEARNED EMBEDDINGS\n13                                       CONCLUSION\n14                                  ACKNOWLEDGMENTS\n15     Published as a conference paper at ICLR 2017\n16                            A ADDITIONAL ANALYSIS\n17                                     Architecture\n18                                       2014. 2014","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>INTRODUCTION</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>OUR APPROACH</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>MULTI-VIEW LEARNING OF ACOUSTIC WORD EMBEDDINGS</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>RECURRENT NEURAL NETWORK ARCHITECTURE</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>RELATED WORK</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>EXPERIMENTS AND RESULTS</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>DATA</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>MODEL DETAILS AND HYPERPARAMETER TUNING</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>EFFECTS OF DIFFERENT OBJECTIVES</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Objective</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>Method</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>WORD SIMILARITY TASKS</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>VISUALIZATION OF LEARNED EMBEDDINGS</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>CONCLUSION</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>ACKNOWLEDGMENTS</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>Published as a conference paper at ICLR 2017</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>A ADDITIONAL ANALYSIS</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>Architecture</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>2014. 2014</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":"Токенов в статье: 6253\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/19 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"107221d9b1274c67945afb34a136ba4c"}},"metadata":{}},{"name":"stdout","text":"\nSummary chunk number 0\nWord embeddings—continuous vector representations of words—are widely used in natural language processing. They can be learned via spectral methods or neural networks and are often composed to represent phrases, sentences, or documents. While most embeddings capture semantic meaning, few represent the acoustic characteristics of words. This work introduces a multi-view approach to learn embeddings that capture how words sound, using both acoustic and orthographic sequences. By jointly learning embeddings from matched and mismatched acoustic-orthographic pairs using contrastive losses, the method enables direct comparison between acoustic and orthographic embeddings. The approach improves performance in acoustic word discrimination and shows promising results in cross-view tasks and word similarity, offering a more effective alternative to sequence-based methods like dynamic time warping.\n\nSummary chunk number 1\nIn this section, we first introduce our approach for learning acoustic word embeddings in a multiview setting, following a brief review of related methods to provide context. We then discuss the specific neural network architecture used, based on bidirectional long short-term memory (LSTM) networks.\n\nSummary chunk number 2\nPrevious approaches to acoustic word embeddings have focused on single-view classification, where acoustic segments are labeled with words and a classifier is trained to predict labels. This method uses paired data {(x_i, y_i)} and optimizes a classification loss (e.g., cross-entropy) over a shared embedding network f and classifier h. However, this approach struggles with large label sets and unseen words due to insufficient training data. An alternative uses Siamese networks with triplet supervision (\"x₁ is similar to x₂, not to x₃\"), enforcing distance constraints between embeddings to learn discriminative features without explicit labels. This approach handles rare labels better and uses more training examples via triplet generation. The text notes that existing methods treat word labels as discrete classes, ignoring phonetic similarities. To address this, the authors propose a multi-view approach, treating acoustic segments and character sequences as complementary views of pronunciation. They use a multi-view contrastive loss, embedding both views into a common space and enforcing that the distance between a segment and its correct character sequence is smaller than that with a mismatched sequence. The objective includes a margin parameter and considers various triplet constructions. A cost-sensitive variant is introduced, where the margin scales with the edit distance between character sequences (measured by Levenshtein distance), with a threshold t_max and maximum margin m_max. This ensures embeddings respect word similarity. The method is implemented using cosine distance and explores combinations of different objectives.\n\nSummary chunk number 3\nBoth views use sequential inputs, so we implement functions f and g with recurrent neural networks, specifically long-short term memory (LSTM) networks. Recurrent neural networks are state-of-the-art for speech tasks like speech recognition, and LSTM-based acoustic word embeddings achieved the best results in one of our experiments. As shown in Figure , f and g are implemented using multi-layer stacked bidirectional LSTMs. The inputs can be any frame-level acoustic feature representation and character vector representation from the orthographic input. At each layer, two LSTM cells process the input sequence from left to right and from right to left. At intermediate layers, the outputs of the two LSTMs at each time step are concatenated to form the input sequence for the next layer. At the top layer, the last time step outputs of the two LSTMs are concatenated to form a fixed-dimensional embedding of the view, which is then used to compute cosine distances in the objective function.\n\nSummary chunk number 4\nThere is no prior work on multi-view learning of acoustic and character-based word embeddings. Acoustic word embeddings have recently been explored through approaches such as representing speech segments as fixed-dimensional vectors using dynamic time warping (DTW) distances to template words, which outperformed raw DTW distances in word discrimination and were later applied to query-by-example tasks. However, DTW is computationally expensive and relies on non-learned parameters. Other studies used recurrent neural network (RNN) autoencoders to learn acoustic word embeddings, showing better performance than DTW in word discrimination. Autoencoders for both acoustic and written words, along with a model for comparing them, were applied to keyword search tasks. Evaluations of acoustic embeddings in downstream tasks like speech recognition and search are costly and often rely on word discrimination—determining if two speech segments correspond to the same word—as a proxy for query-by-example search. While word discrimination provides fixed word boundaries, prior work has extended these embeddings to non-word segments to improve query-by-example systems without known boundaries. The only prior study explicitly targeting acoustic similarity of character sequences used nearest-neighbor retrieval, phonetic/orthographic similarity, and homophone disambiguation. This text uses similar tasks and acoustic word discrimination for comparison with prior acoustic embedding work.\n\nSummary chunk number 5\nThe ultimate goal is to improve speech systems requiring word-level discrimination, such as speech recognition and query-by-example search. To efficiently evaluate model performance and focus on embedding content, three surrogate tasks are considered based on cosine distance between embeddings. First, acoustic word discrimination involves determining if two acoustic sequences correspond to the same word, using average precision (AP) as the performance metric. In a multi-view setup, embeddings of both acoustic words and character sequences are computed, enabling evaluation of written-spoken word comparisons, such as in spoken term detection. A second task, cross-view word discrimination, assesses whether an acoustic signal matches a written word by comparing embeddings and measuring performance with AP. A third task, word similarity, evaluates whether embedding distances reflect character edit distances, using rank correlation as the metric, applied to both acoustic and character sequence pairs. These tasks are used to evaluate embeddings, with initial exploration focused on acoustic word discrimination, followed by evaluation using cross-view discrimination and word similarity measures.\n\nSummary chunk number 6\nThe same experimental setup and data as in previous studies are used. The task and setup were developed by . Data comes from the Switchboard English conversational speech corpus. Spoken word segments last between 50 and 200 frames (0.5–2 seconds). The train/dev/test splits contain 9971/10966/11024 pairs of acoustic segments and character sequences, corresponding to 1687/3918/3390 unique words. For dev or test sets, all pairs are used to compute AP, resulting in approximately 60 million word pairs. In the acoustic view, the embedding model receives a sequence of 39-dimensional vectors (one per frame) of standard mel frequency cepstral coefficients (MFCCs) and their first and second derivatives. In the character sequence embedding model, the input is a sequence of 26-dimensional one-hot vectors representing each character of the word’s orthography.\n\nSummary chunk number 7\nWe experiment with different neural network architectures by varying the number of stacked LSTM layers, hidden units per layer, and the use of single- or bidirectional LSTM cells. A coarse grid search shows that 2-layer bidirectional LSTMs with 512 hidden units per direction per layer perform well on the acoustic word discrimination task, and this structure is fixed for subsequent experiments. The outputs of the top-layer LSTMs serve as the learned embedding for each view, which is 1024-dimensional when bidirectional LSTMs are used. During training, dropout is applied to the inputs of the acoustic view and between stacked layers for both views. For each training example, a negative example is required; a negative character label sequence is generated by uniformly sampling a word label different from the positive label, and negative acoustic feature sequences are uniformly sampled from differently labeled sequences in the training set. Negative label sampling is performed anew at the beginning of each epoch. Network weights are initialized with values uniformly sampled from [−0.05, 0.05]. The Adam optimizer is used with mini-batches of 20 acoustic segments, and the initial learning rate is tuned over {0.0001, 0.001}. Dropout rates are tuned over {0, 0.2, 0.4, 0.5}, with 0.4 typically performing best. The margin in the basic contrastive loss is tuned over {0.3, 0.4, 0.5, 0.6, 0.7}, with 0.4 and 0.5 yielding the best results. For object 0 with cost-sensitive margin, the maximum margin m_max is tuned over {0.5, 0.6, 0.7} and the threshold t_max over {9, 11, 13}. Each model is trained for up to 1000 epochs, and the model achieving the best AP on the development set is selected for evaluation on the test set.\n\nSummary chunk number 8\nWe presented four contrastive losses and potential combinations in Section 2.1. We explore the effects of these objectives on word discrimination tasks. Table shows the development set AP for acoustic and cross-view word discrimination achieved using the various objectives. We tuned the objectives for the acoustic discrimination task and used the corresponding converged models for the cross-view task. Among simple contrastive objectives, obj 0 and obj 2 (involving only cross-view distances) slightly outperform the others on acoustic word discrimination. The best-performing objective is the \"symmetrized\" objective obj 0 + obj 2, which significantly outperforms all individual objectives and the combination of the four. The cost-sensitive objective is very competitive but falls slightly short of the best performance. We note that a similar objective to our obj 0 + obj 2 was used by for the task of caption-image retrieval, where the authors essentially use all non-paired.\n\nSummary chunk number 9\nDev AP (acoustic) (cross-view)\n\nSummary chunk number 10\nThe table presents final test set average precision (AP) for different approaches to word discrimination. The baseline uses MFCC features with DTW, achieving AP = 0.214. Subsequent methods apply DTW to learned correspondence autoencoder or phone posterior features (AP = 0.469 and 0.497, respectively). Methods based on cosine similarity between acoustic word embeddings are not detailed here. The best-performing method is the multi-view LSTM using objectives obj 0 + obj 2, achieving AP = 0.806 (acoustic) and 0.892 (cross-view). The development set AP does not saturate after 1000 epochs, suggesting room for improvement. Previous work used acoustic embeddings only and did not address joint learning with the text view, thus not applicable to the cross-view task.\n\nSummary chunk number 11\nTable presents results on word similarity tasks, reporting Spearman's ρ (rank correlation) between embedding distances and orthographic edit distances (Levenshtein distance). Results are given for both acoustic and text embeddings. The cost-sensitive objective shows greater improvement on word similarity compared to fixed-margin, despite minimal gains on word discrimination tasks. Acoustic embeddings show a rank correlation of 0.226 with phonetic edit distances, and text embeddings show 0.241, which are close to their orthographic counterparts. The performance of text embeddings is not significantly better than that of acoustic embeddings, even though text embeddings have access to the full text input. This is attributed to the limited variety of similar word pairs in the training data, with fewer than 2% of unique word pairs having an edit distance below 5 characters. The embeddings do not learn to distinguish fine differences in character sequences due to insufficient data. Future work could involve training on datasets with more similar word pairs.\n\nSummary chunk number 12\nFigure presents a 2-dimensional t-SNE visualization of selected acoustic and character sequences from the development set, including both previously seen and previously unseen words. Previously seen words were selected uniformly at random from those appearing at least 15 times in the development set (unseen words are the only six meeting this criterion). The visualization shows that acoustic embeddings are tightly clustered and closely aligned with text embeddings, and that unseen words also cluster nearly as well as previously seen ones. Although the figure illustrates the relationships among multiple acoustic and text embeddings, the words are highly diverse, so no conclusions about word relationships can be drawn. Another visualization explores the text embeddings of development set words ending in \"-ly\", \"-ing\", and \"-tion\", confirming that related words are embedded close together, with suffix-sharing words forming well-defined clusters.\n\nSummary chunk number 13\nWe present an approach for jointly learning acoustic and orthographic word embeddings. This multi-view method improves acoustic embedding performance and enables the same embeddings to be used for both spoken and written queries. Various contrastive objectives are explored, including fixed-margin losses for separating word pairs and a cost-sensitive loss targeting orthographic edit distances. While losses perform similarly in word discrimination, the cost-sensitive loss enhances alignment between embedding and orthographic distances. Future directions include incorporating phonetic pronunciation knowledge in training and evaluation, and extending the approach to train on both word and non-word segments. Visualization via t-SNE shows character sequence embeddings for words with suffixes \"-ly\", \"-ing\", and \"-tion\".\n\nSummary chunk number 14\nThis research was supported by a Google Faculty Award and an NSF grant IIS-1321015. The authors' opinions do not necessarily reflect those of the funding agencies. The study used GPUs donated by NVIDIA Corporation. The authors thank Herman Kamper and Shane Settle for their assistance with the data and experimental setup.\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"print(\"\\n--- ROUGE Metrics (средние значения) ---\")\nfor key in rouge_list[0].keys():\n    avg = sum(r[key] for r in rouge_list) / len(rouge_list)\n    print(f\"{key}: {avg:.4f}\")\n\navg_bert = sum(bert_list) / len(bert_list)\nprint(f\"\\n--- BERTScore F1 (среднее): {avg_bert:.4f} ---\")\n\navg_sum = sum(l['for_summary'] for l in ldfacts_list) / len(ldfacts_list)\navg_abs = sum(l['for_abstract'] for l in ldfacts_list) / len(ldfacts_list)\n\nprint(f\"\\n--- LongDocFACTScore (средние) ---\")\nprint(f\"Для суммаризаций: {avg_sum:.4f}\")\nprint(f\"Для абстрактов: {avg_abs:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}