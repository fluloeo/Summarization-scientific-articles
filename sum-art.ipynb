{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":14437418,"sourceType":"datasetVersion","datasetId":9221908}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/fluloeo/sum-art?scriptVersionId=291062016\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nimport ast\nimport matplotlib.pyplot as plt","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-10T04:32:01.172157Z","iopub.execute_input":"2026-01-10T04:32:01.172932Z","iopub.status.idle":"2026-01-10T04:32:01.180136Z","shell.execute_reply.started":"2026-01-10T04:32:01.172883Z","shell.execute_reply":"2026-01-10T04:32:01.17944Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/articles/df_dict_test.csv\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"!pip install -U bitsandbytes accelerate transformers -q\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nimport torch\nos.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\"\n)\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-4B-Instruct-2507\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen3-4B-Instruct-2507\",\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    trust_remote_code=True\n)\n\ntorch.cuda.empty_cache()\n!pip install langchain==0.0.208  -q\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n!pip install rouge-score evaluate -q\nfrom rouge_score import rouge_scorer\nimport evaluate\nrouge = evaluate.load('rouge')\n\n!pip install bert-score -q\nfrom bert_score import BERTScorer\n\nscorer = BERTScorer(lang=\"en\", model_type=\"bert-base-multilingual-cased\")\n\n!pip install longdocfactscore -q\nimport nltk\nnltk.download('punkt_tab')\nnltk.download('punkt')\nfrom longdocfactscore.ldfacts import LongDocFACTScore\nldfacts_scorer = LongDocFACTScore(device=device)\nfrom tqdm.notebook import tqdm\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T04:32:01.557547Z","iopub.execute_input":"2026-01-10T04:32:01.557816Z","iopub.status.idle":"2026-01-10T04:34:22.881918Z","shell.execute_reply.started":"2026-01-10T04:32:01.557793Z","shell.execute_reply":"2026-01-10T04:34:22.88092Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m380.9/380.9 kB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m123.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hcuda\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"003c66f199cb4e42ad35f57d622a46b9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b791bb084f74ff7a246ac42327163a8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc24777b9ea248afa2302926a74add5c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f7cc0d2c226450ab8e1860b53398997"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/727 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a4a2374c76143bd8b48f25e7e5b6dad"}},"metadata":{}},{"name":"stderr","text":"2026-01-10 04:32:32.946348: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1768019553.132947      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1768019553.183372      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1768019553.613293      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1768019553.613320      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1768019553.613323      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1768019553.613325      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c11ee93c8a034ab898b1e83ae090b064"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e1d9a983ce9440eb0c0f6860c3267bf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00003.safetensors:   0%|          | 0.00/3.96G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a674639490cb4f248c5fb2e24a9046b2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00003.safetensors:   0%|          | 0.00/3.99G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2815f4b7ffac4e43b81e5c6f2cbf1c00"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00003.safetensors:   0%|          | 0.00/99.6M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb3be2f7ef9440f08779e07d1be0066a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"571c56bff27d465f8dab008c450fa8de"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/238 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec20a77284964f1bb1a72b8f629bf776"}},"metadata":{}},{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.1/155.1 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m112.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m86.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m96.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.26.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\nsigstore 4.1.0 requires pydantic<3,>=2, but you have pydantic 1.10.26 which is incompatible.\nsigstore-models 0.0.5 requires pydantic>=2.11.7, but you have pydantic 1.10.26 which is incompatible.\nsigstore-rekor-types 0.0.18 requires pydantic[email]<3,>=2, but you have pydantic 1.10.26 which is incompatible.\nydata-profiling 4.18.0 requires pydantic<3,>=2, but you have pydantic 1.10.26 which is incompatible.\nxmanager 0.7.1 requires sqlalchemy==1.2.19, but you have sqlalchemy 2.0.45 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngoogle-colab 1.0.0 requires jupyter-server==2.14.0, but you have jupyter-server 2.12.5 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\njaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\nthinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\nthinc 8.3.6 requires pydantic<3.0.0,>=2.0.0, but you have pydantic 1.10.26 which is incompatible.\nopencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\nopencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\njax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\nlangchain-core 0.3.79 requires pydantic<3.0.0,>=2.7.4, but you have pydantic 1.10.26 which is incompatible.\ncudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ngradio 5.49.1 requires pydantic<2.12,>=2.0, but you have pydantic 1.10.26 which is incompatible.\nalbumentations 2.0.8 requires pydantic>=2.9.2, but you have pydantic 1.10.26 which is incompatible.\nbigframes 2.26.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nopencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\npytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\ngoogle-genai 1.45.0 requires pydantic<3.0.0,>=2.0.0, but you have pydantic 1.10.26 which is incompatible.\npydantic-settings 2.11.0 requires pydantic>=2.7.0, but you have pydantic 1.10.26 which is incompatible.\nmcp 1.18.0 requires pydantic<3.0.0,>=2.11.0, but you have pydantic 1.10.26 which is incompatible.\npylibcudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b31dbafeab6949d08ac40674d7a8cf1c"}},"metadata":{}},{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"111fbe37cca84318a993284d95544dfe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dff5e8272b6843b58d6c432cb9d09227"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90314274ee4144c3ab4fb49e8f508117"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ffcb28ada554a80ba609bbf5c8c07b6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/714M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"922b150cf5c0445ab70f356986eb9cef"}},"metadata":{}},{"name":"stderr","text":"[nltk_data] Downloading package punkt_tab to /usr/share/nltk_data...\n[nltk_data]   Package punkt_tab is already up-to-date!\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ad50be98cd741b1b83ea0d92716311f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b18d578436aa40188833c2a29dbe4df2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18ba60fd59304b268855d64a7c3c1bf2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"431a6c0b84a44041b518f490010eb4ca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c269fc507cec4a92847172581e12ac23"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9517ba8fe38c4f53885f9a7a34375768"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/399 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce78a8d46e2b4ef18bf18b71f8e2801c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6219eba2ce8847eb824c6e03b09e5cd7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d03fce221ac9438ab15f606dae862ef9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a06e786531c47809ec2b9a0d553d0e6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e9a9f307f1c4e9a8425f55faa965350"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb80a18665e0432f9af4f0a1b9d96563"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"acab7b4e1b8e42fd821a9c7c04f66ac6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"018416da2f2a4e1da7ea8a29be767dec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9abb839210a94f6c9ebefd4a2cddf761"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fbb22343ee2640e5af6c3b10769dc37e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"894151bdff4845b2beca2428a568e24c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/1.02G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f723c5970094f04a7b8d5fdb1d925bb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.02G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"175c027f5d2a4c80a74b2e21d7d9b269"}},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"def append_summary_to_file(final_summary, filename='all_summaries.txt', directory='/kaggle/working/'):\n    \"\"\"\n    Добавляет текст final_summary в конец указанного файла.\n\n    Параметры:\n        final_summary (str): Текст сводки для добавления в файл.\n        filename (str): Имя файла (по умолчанию 'all_summaries.txt').\n        directory (str): Путь к папке для сохранения (по умолчанию рабочая директория Kaggle).\n    \"\"\"\n    # Формируем полный путь к файлу\n    filepath = directory + filename\n\n    # Открываем файл в режиме добавления ('a') с кодировкой UTF-8\n    # Режим 'a' создаст файл, если его нет, и допишет текст в конец, если он существует\n    with open(filepath, 'a', encoding='utf-8') as file:\n        # Записываем текст и добавляем перенос строки для разделения записей\n        file.write(final_summary + '\\n')\n\n    print(f'Текст добавлен в файл: {filepath}')\n\n\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T04:34:23.606281Z","iopub.execute_input":"2026-01-10T04:34:23.606913Z","iopub.status.idle":"2026-01-10T04:34:23.624891Z","shell.execute_reply.started":"2026-01-10T04:34:23.606877Z","shell.execute_reply":"2026-01-10T04:34:23.62432Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/articles/df_dict_test.csv')\ndf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T04:34:23.62572Z","iopub.execute_input":"2026-01-10T04:34:23.625976Z","iopub.status.idle":"2026-01-10T04:34:23.704165Z","shell.execute_reply.started":"2026-01-10T04:34:23.625956Z","shell.execute_reply":"2026-01-10T04:34:23.703567Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"            id                                              title  \\\n0   1602.04402  balanced truncation of linear time-invariant s...   \n1   1611.01462  tying word vectors and word classifiers: a los...   \n2   1611.04496  multi-view recurrent neural acoustic word embe...   \n3   1808.00560  compressible spectral mixture kernels with spa...   \n4   2111.00405  limitations of the macaulay matrix approach fo...   \n..         ...                                                ...   \n95  2307.14341  virtual mirrors: non-line-of-sight imaging bey...   \n96  2307.14354  learned gridification for efficient point clou...   \n97  2307.14362  learnable wavelet neural networks for cosmolog...   \n98  2307.14392  human-centric scene understanding for 3d large...   \n99  2309.03177  3d object positioning using differentiable mul...   \n\n                                             abstract  \\\n0   this paper discusses model order reduction of ...   \n1   recurrent neural networks have been very succe...   \n2   recent work has begun exploring neural acousti...   \n3   spectral mixture (sm) kernels comprise a power...   \n4   recently chen and gao~\\cite{chengao2017} propo...   \n..                                                ...   \n95  non-line-of-sight (nlos) imaging methods are c...   \n96  neural operations that rely on neighborhood in...   \n97  convolutional neural networks (cnns) have been...   \n98  human-centric scene understanding is significa...   \n99  this article describes a multi-modal method us...   \n\n                                            dict_test  \n0   {'I. INTRODUCTION AND PROBLEM FORMULATIONS': '...  \n1   {'INTRODUCTION': \"Neural network models have r...  \n2   {'INTRODUCTION': 'Word embeddings-continuous-v...  \n3   {'Introduction': \"Gaussian processes (GPs) con...  \n4   {'Introduction': 'Solving systems of multivari...  \n..                                                ...  \n95  {'Computed image of': 'T-shaped object from a ...  \n96  {'Introduction': 'Point clouds provide sparse ...  \n97  {'Introduction': 'The process of extracting in...  \n98  {'Introduction': 'Human-centric scene understa...  \n99  {'I. INTRODUCTION': \"Differentiable rendering ...  \n\n[100 rows x 4 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>title</th>\n      <th>abstract</th>\n      <th>dict_test</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1602.04402</td>\n      <td>balanced truncation of linear time-invariant s...</td>\n      <td>this paper discusses model order reduction of ...</td>\n      <td>{'I. INTRODUCTION AND PROBLEM FORMULATIONS': '...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1611.01462</td>\n      <td>tying word vectors and word classifiers: a los...</td>\n      <td>recurrent neural networks have been very succe...</td>\n      <td>{'INTRODUCTION': \"Neural network models have r...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1611.04496</td>\n      <td>multi-view recurrent neural acoustic word embe...</td>\n      <td>recent work has begun exploring neural acousti...</td>\n      <td>{'INTRODUCTION': 'Word embeddings-continuous-v...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1808.00560</td>\n      <td>compressible spectral mixture kernels with spa...</td>\n      <td>spectral mixture (sm) kernels comprise a power...</td>\n      <td>{'Introduction': \"Gaussian processes (GPs) con...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2111.00405</td>\n      <td>limitations of the macaulay matrix approach fo...</td>\n      <td>recently chen and gao~\\cite{chengao2017} propo...</td>\n      <td>{'Introduction': 'Solving systems of multivari...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>2307.14341</td>\n      <td>virtual mirrors: non-line-of-sight imaging bey...</td>\n      <td>non-line-of-sight (nlos) imaging methods are c...</td>\n      <td>{'Computed image of': 'T-shaped object from a ...</td>\n    </tr>\n    <tr>\n      <th>96</th>\n      <td>2307.14354</td>\n      <td>learned gridification for efficient point clou...</td>\n      <td>neural operations that rely on neighborhood in...</td>\n      <td>{'Introduction': 'Point clouds provide sparse ...</td>\n    </tr>\n    <tr>\n      <th>97</th>\n      <td>2307.14362</td>\n      <td>learnable wavelet neural networks for cosmolog...</td>\n      <td>convolutional neural networks (cnns) have been...</td>\n      <td>{'Introduction': 'The process of extracting in...</td>\n    </tr>\n    <tr>\n      <th>98</th>\n      <td>2307.14392</td>\n      <td>human-centric scene understanding for 3d large...</td>\n      <td>human-centric scene understanding is significa...</td>\n      <td>{'Introduction': 'Human-centric scene understa...</td>\n    </tr>\n    <tr>\n      <th>99</th>\n      <td>2309.03177</td>\n      <td>3d object positioning using differentiable mul...</td>\n      <td>this article describes a multi-modal method us...</td>\n      <td>{'I. INTRODUCTION': \"Differentiable rendering ...</td>\n    </tr>\n  </tbody>\n</table>\n<p>100 rows × 4 columns</p>\n</div>"},"metadata":{}}],"execution_count":13},{"cell_type":"markdown","source":"*Пример для одной статьи:*","metadata":{}},{"cell_type":"markdown","source":"статьи 12 8 7 имеют проблемы с большим количеством чанков\n","metadata":{}},{"cell_type":"code","source":"from typing import List\n\ndef get_token_length(text: str) -> int:\n    return len(tokenizer.encode(text))\n\ndef merge_small_chunks_by_tokens(chunks: List[str], min_tokens: int) -> List[str]:\n    \"\"\"\n    Объединяет чанки, если их длина в ТОКЕНАХ меньше min_tokens.\n    Слияние происходит с наименьшим соседом (по количеству токенов).\n    \"\"\"\n    processed_chunks = chunks[:]\n    separator = \" \" # Разделитель при склейке\n    \n    i = 0\n    while i < len(processed_chunks):\n        current_chunk = processed_chunks[i]\n        current_len = get_token_length(current_chunk)\n        if current_len >= min_tokens:\n            i += 1\n            continue\n        if len(processed_chunks) == 1:\n            break\n        if i > 0:\n            left_len = get_token_length(processed_chunks[i-1])\n        else:\n            left_len = float('inf')\n\n        if i < len(processed_chunks) - 1:\n            right_len = get_token_length(processed_chunks[i+1])\n        else:\n            right_len = float('inf')\n        if left_len < right_len:\n            # print(f\"DEBUG: Слияние '{processed_chunks[i][:20]}...' ВЛЕВО (Tokens: {current_len} + {left_len})\")\n            \n            processed_chunks[i-1] = processed_chunks[i-1] + separator + processed_chunks[i]\n            processed_chunks.pop(i)\n            i -= 1 \n        else:\n            # print(f\"DEBUG: Слияние '{processed_chunks[i][:20]}...' ВПРАВО (Tokens: {current_len} + {right_len})\")\n            processed_chunks[i] = processed_chunks[i] + separator + processed_chunks[i+1]\n            processed_chunks.pop(i+1)\n    return processed_chunks\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T04:34:23.705045Z","iopub.execute_input":"2026-01-10T04:34:23.705333Z","iopub.status.idle":"2026-01-10T04:34:23.712918Z","shell.execute_reply.started":"2026-01-10T04:34:23.70531Z","shell.execute_reply":"2026-01-10T04:34:23.712158Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"def merge_chunks_in_dataset(df=df, min_tokens=100, n=100, n_threshold = 10):\n    data = pd.DataFrame(columns=['chunks'])\n    max_len_chunks = []\n    max_size_chunks = []\n    min_size_chunks = []\n    for i in range(n):\n        data_dict = ast.literal_eval(df['dict_test'].iloc[i])\n        tokens_chunks_list_0 = list(map(lambda x: get_token_length(x), list(data_dict.values())))\n        if len(tokens_chunks_list_0)>n_threshold or min(tokens_chunks_list_0)<min_tokens:\n            final_chunks = merge_small_chunks_by_tokens(list(data_dict.values()), min_tokens=min_tokens)\n            # print(f\"\\nКонечное количество чанков: {len(final_chunks)}\\n\")\n            tokens_chunks_list = list(map(lambda x: get_token_length(x), final_chunks))\n            # print('Максимальный размер чанка',max(tokens_chunks_list),'\\n')\n            max_len_chunks.append(len(final_chunks))\n            max_size_chunks.append(max(tokens_chunks_list))\n            min_size_chunks.append(min(tokens_chunks_list))\n            data.loc[len(data)] = [final_chunks]\n        else:\n            print(\"Ничего не меняем, кол-во чанков, мин токенов\", len(tokens_chunks_list_0),min(tokens_chunks_list_0))\n            data.loc[len(data)] = [data_dict]\n    print('Максимальное Кол-во чанков из всех статей\\n',(max(max_len_chunks)))\n    print('Минимальное Кол-во чанков из всех статей\\n',(min(max_len_chunks)))\n    print('Максимальный размер чанка из всех статей\\n',max(max_size_chunks))\n    print('Минимальный размер чанка из всех статей\\n',min(min_size_chunks))\n    return data\ndata = merge_chunks_in_dataset(df=df, min_tokens=700, n=20,n_threshold = 10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T04:34:23.713816Z","iopub.execute_input":"2026-01-10T04:34:23.714246Z","iopub.status.idle":"2026-01-10T04:34:27.647609Z","shell.execute_reply.started":"2026-01-10T04:34:23.714207Z","shell.execute_reply":"2026-01-10T04:34:27.646853Z"}},"outputs":[{"name":"stdout","text":"Максимальное Кол-во чанков из всех статей\n 14\nМинимальное Кол-во чанков из всех статей\n 5\nМаксимальный размер чанка из всех статей\n 3331\nМинимальный размер чанка из всех статей\n 706\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"Функция для того чтобы отправить инструкцию модели","metadata":{}},{"cell_type":"code","source":"def qwen(article, prompt, max_tokens):\n    torch.cuda.empty_cache()\n    final_prompt = f\"{prompt}:\\n\\n{article}\"\n    messages = [{\"role\": \"user\", \"content\": final_prompt}]\n    model.eval()\n    with torch.no_grad():\n        inputs = tokenizer.apply_chat_template(\n            messages,\n            add_generation_prompt=True,\n            tokenize=True,\n            return_dict=True,\n            return_tensors=\"pt\",\n            truncation=False\n        ).to(model.device)\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=max_tokens,\n            do_sample=False\n        )\n    final_summary = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:], skip_special_tokens=True)\n    return final_summary","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T04:34:38.210404Z","iopub.execute_input":"2026-01-10T04:34:38.210712Z","iopub.status.idle":"2026-01-10T04:34:38.216583Z","shell.execute_reply.started":"2026-01-10T04:34:38.210686Z","shell.execute_reply":"2026-01-10T04:34:38.21588Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"Функция для обработки чанков статьи, встроенный text_splitter можно отключить","metadata":{}},{"cell_type":"code","source":"def summarize(article, prompt_0, prompt_1,chunk_size=1500, max_tokens_0=700, max_tokens_1=700, text_splitter = True):\n    torch.cuda.empty_cache()\n    if text_splitter:\n        text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n            tokenizer=tokenizer,\n            chunk_size=chunk_size,\n            chunk_overlap=chunk_size//20,\n            separators=[\"\\n\\n\", \"\\n\", \". \"],\n            keep_separator=True\n        )\n        article_chunks = text_splitter.split_text(article)\n    else:\n        article_chunks = article\n    summaries = []\n    for i, chunk in tqdm(enumerate(article_chunks), total=len(article_chunks)):\n        try:\n            chunk_summary = qwen(article=chunk, prompt=prompt_0, max_tokens=max_tokens_0)\n            # print(f\"\\nSummary chunk number {i}\")\n            # print(chunk_summary)\n        except torch.cuda.OutOfMemoryError as e:\n            print(f\"⚠️ Ошибка нехватки памяти CUDA, слишком длинный чанк: {str(e)[:100]}...\")\n            return 'ERROR'\n\n        summaries.append(chunk_summary)\n        torch.cuda.empty_cache()\n    combined_summary = \"\\n\".join(summaries)\n    \n    try:\n        final_summary = qwen(article=combined_summary, prompt=prompt_1, max_tokens=max_tokens_1)\n    except torch.cuda.OutOfMemoryError as e:\n        print(f\"⚠️ Ошибка нехватки памяти CUDA при итоговой суммаризации: {str(e)[:100]}...\")\n        return 'ERROR'\n    return final_summary","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T04:34:41.114818Z","iopub.execute_input":"2026-01-10T04:34:41.115603Z","iopub.status.idle":"2026-01-10T04:34:44.646746Z","shell.execute_reply.started":"2026-01-10T04:34:41.115561Z","shell.execute_reply":"2026-01-10T04:34:44.645953Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"rouge_list = []\nbert_list = []\nldfacts_list = []\nsummaries_list = []\n\ndef to_full_text(data_dict):\n    text = ''\n    for key, value in data_dict.items(): \n        text += f\"{key}\\n\\n{value}\\n\\n\"\n    return text\n\n\nfor i in range(20):\n    print(f\"\\n=== Статья {i} ===\")\n    #извлекаем чанки, абстракт, названия разделов\n    article_dict = ast.literal_eval(df['dict_test'].iloc[i]) #словарь для i-той статьи \n    chunked_article = data['chunks'].iloc[i] #list(article_dict.values()) #список чанков !!!!ВАЖНО я поменял на новый list \n    # chapters_list = list(article_dict.keys()) #список глав\n    # print('Список разделов')\n    # display(pd.DataFrame(chapters_list))\n    abstract = df['abstract'].iloc[i] \n    article = to_full_text(article_dict) #полный текст статьи с ключами и значениями словаря\n    \n    print(f\"Токенов в статье: {len(tokenizer.encode('\\n'.join(chunked_article)))}\")\n    prompt_0 = \"You're a science editor. Briefly summarize this fragment of the scientific text in original language. Do not add information that is not in the source texts.Try to answer in 350 words. Your answer should be perfect. Do not add information that is not in the source texts\"\n    prompt_1 = \"You're a science editor. Based on the following summaries of the parts of the article, create a single, coherent and concise summary of the entire scientific article in original language, highlighting the common goal, methods, key results and conclusion. Try to answer with a length of 600 to 650 words. Your answer should be perfect. Do not add information that is not in the source texts\"\n    \n    summary_full = summarize(chunked_article, prompt_0 = prompt_0, prompt_1 = prompt_1, max_tokens_0 = 400, max_tokens_1 = 700, text_splitter = False)\n    print(f'\\nSummary article number {i}')\n    print(f\"Токенов в summary: {len(tokenizer.encode(summary_full))}\")\n    print(summary_full)\n    summaries_list.append({\n        'article_id': i,\n        'original_text': article,\n        'abstract': abstract,\n        'summary': summary_full\n    })\n    append_summary_to_file(f\"--- Summury #{i} ---\")\n    append_summary_to_file(summary_full)\n    append_summary_to_file(\"\")\n    \n    results = rouge.compute(\n    predictions=[summary_full],\n    references=[article],\n    use_stemmer=True\n    )\n    print(\"ROUGE Metrics:\")\n\n    rouge_dict = {}\n    for key, value in results.items():\n        print(f\"{key}: {value:.4f}\")\n        rouge_dict[key] = value\n    rouge_list.append(rouge_dict)\n\n    _, _, F1 = scorer.score([summary_full], [abstract])\n    bert_f1 = F1.item()\n    print(f\"\\nBERTScore: F1 = {bert_f1:.4f}\")\n    bert_list.append(bert_f1)\n\n    ldfacts_sum = ldfacts_scorer.score_src_hyp_long([article], [summary_full])\n    ldfacts_abs = ldfacts_scorer.score_src_hyp_long([article], [abstract])\n    print(f\"LongDocFACTScore for sum: {ldfacts_sum[0]}\")\n    print(f\"LongDocFACTScore for abstract: {ldfacts_abs[0]}\")\n    ldfacts_list.append({\n        'for_summary': ldfacts_sum[0],\n        'for_abstract': ldfacts_abs[0]\n    })","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T04:34:45.581314Z","iopub.execute_input":"2026-01-10T04:34:45.582096Z","iopub.status.idle":"2026-01-10T06:56:26.239204Z","shell.execute_reply.started":"2026-01-10T04:34:45.582067Z","shell.execute_reply":"2026-01-10T06:56:26.238247Z"}},"outputs":[{"name":"stdout","text":"\n=== Статья 0 ===\nТокенов в статье: 6901\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8572b3c92c954d36a234b274188189c2"}},"metadata":{}},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","output_type":"stream"},{"name":"stdout","text":"\nSummary article number 0\nТокенов в summary: 511\nThis article addresses the challenge of model order reduction (MOR) for linear time-invariant continuous-time systems with large order, where computational and storage demands hinder practical simulation and design. While standard balanced truncation (FIBT) preserves stability and offers a priori error bounds over the entire frequency range, it is suboptimal when input signals operate within a finite frequency interval [ω₁, ω₂]. To improve in-band approximation performance, several balancing-related methods are analyzed: singular perturbation approximation (SPA), frequency-weighted balanced truncation (FWBT), and frequency-limited Grammians balanced truncation (FGBT). However, these methods either lack error bounds for finite intervals, require iterative design, or fail due to non-positive semi-definite Grammians. A key observation is that existing methods evaluate performance using entire-frequency indices, despite the finite-frequency focus.\n\nTo address these limitations, the article proposes an interval-type frequency-dependent balanced truncation (FDBT) method specifically designed for finite-frequency intervals. Based on the Generalized KYP Lemma, the method constructs an extended system via Moebius transformation and defines interval-type frequency-dependent controllability and observability Gramians. A system realization is defined as interval-type frequency-dependent balanced if these Gramians are equal and diagonal. The reduced model is derived through coordinate transformation and truncation, preserving stability for stable original systems. The method provides both an interval-type error bound over [ω₁, ω₂] and an entire-frequency error bound. The interval-type error bound, derived via a dilated error system and symmetric Lyapunov variables, tends to zero as the frequency interval size shrinks, ensuring high in-band accuracy. The algorithm (Algorithm 2) solves Lyapov equations, performs diagonalization via a frequency-dependent transformation matrix, and truncates the system. It supports complex or real matrices and asymmetrical intervals, generating real reduced models for symmetric intervals.\n\nExperimental results on an RLC ladder circuit with complex pole-zero distribution demonstrate that FDBT outperforms FIBT, generalized SPA, and FGBT in both error bounds and in-band approximation, especially for small to medium frequency intervals. Notably, FIBT and generalized SPA fail to capture dynamics near ω = 0, even at high orders, while FDBT succeeds with reduced orders above 50. The proposed method thus provides a robust, stable, and practically applicable MOR approach tailored to finite-frequency applications, offering the first proven interval-type error bound for in-band performance.\nТекст добавлен в файл: /kaggle/working/all_summaries.txt\nТекст добавлен в файл: /kaggle/working/all_summaries.txt\nТекст добавлен в файл: /kaggle/working/all_summaries.txt\nROUGE Metrics:\nrouge1: 0.1338\nrouge2: 0.0667\nrougeL: 0.0847\nrougeLsum: 0.1155\n\nBERTScore: F1 = 0.6821\nLongDocFACTScore for sum: -4.260047599673271\nLongDocFACTScore for abstract: -5.1785284042358395\n\n=== Статья 1 ===\nТокенов в статье: 5945\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f64b1a2697914d67b59e5495478b8cc5"}},"metadata":{}},{"name":"stdout","text":"\nSummary article number 1\nТокенов в summary: 552\nThis article proposes a novel loss framework and architectural improvement for recurrent neural network language models (RNNLMs) aimed at addressing two fundamental limitations in existing models: the lack of a defined metric space for output classes and the isolation of input and output representations. The authors introduce an augmented loss function that extends classical cross-entropy by incorporating a term minimizing the Kullback-Leibler (KL) divergence between model predictions and a target distribution derived from word embedding similarities. This target distribution, estimated via softmax over inner products of word embeddings normalized by temperature τ, reflects semantic proximity rather than one-hot labels, thereby leveraging the natural geometric structure of word embeddings. The framework further introduces a synergistic architectural improvement by tying the input and output embedding matrices together—specifically, by setting the output projection matrix W equal to the transpose of the input embedding matrix Lᵀ—thus reusing the input embedding matrix as the output classification matrix. This design reduces model complexity, eliminates redundant parameters, and enforces alignment between input and output representations within the shared embedding space.\n\nTheoretical analysis shows that under conditions of equal embedding and hidden state dimensions (dₓ = dₕ), zero bias, and high temperature τ, the augmented loss drives model logits to align with the target distribution derived from embedding similarities. First-order approximations demonstrate that the gradient update leads to a linear transformation h → Ah, with the output space constrained to match the input embedding subspace. Empirical validation on the Penn Treebank (PTB) and Wikitext-2 datasets using 2-layer LSTMs with varying hidden units confirms that all modified models outperform the baseline VD-LSTM. The augmented loss (AL) alone improves performance, especially in small networks, by providing more informative gradient updates through semantic similarity. Reusing embeddings (RE) significantly improves performance in larger models by reducing parameter count and enforcing structural alignment. The combined approach (REAL) achieves the best results, outperforming prior methods including large ensembles. On PTB, AL outperforms RE; on Wikitext-2, the advantage of AL diminishes with larger datasets, suggesting that semantic similarity becomes more reliable with sufficient data. Experiments with small networks retrained on split Wikitext-2 confirm AL’s ability to extract meaningful information even with larger vocabularies. The model also shows reduced <unk> generation and improved predictions for semantically related words due to embedding-based similarity. The framework is not limited to language modeling and is expected to benefit tasks like machine translation and speech recognition, particularly with large vocabularies. The results demonstrate that the proposed method—by integrating semantic structure into the loss and aligning input and output spaces—leads to more efficient, accurate, and interpretable language modeling.\nТекст добавлен в файл: /kaggle/working/all_summaries.txt\nТекст добавлен в файл: /kaggle/working/all_summaries.txt\nТекст добавлен в файл: /kaggle/working/all_summaries.txt\nROUGE Metrics:\nrouge1: 0.1513\nrouge2: 0.0623\nrougeL: 0.0801\nrougeLsum: 0.1167\n\nBERTScore: F1 = 0.6817\nLongDocFACTScore for sum: -4.415717393159866\nLongDocFACTScore for abstract: -4.481736040115356\n\n=== Статья 2 ===\nТокенов в статье: 6253\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e03fb43ab7244b39bcff0f41b2726a6"}},"metadata":{}},{"name":"stdout","text":"\nSummary article number 2\nТокенов в summary: 600\nThis paper proposes a multi-view learning framework for acoustic and orthographic word embeddings that jointly captures pronunciation and spelling information. The goal is to learn shared, low-dimensional vector representations of words such that acoustic and character sequences of the same word are close in embedding space, while those of different words are separated. Unlike prior single-view methods—such as DTW-based or autoencoder-based approaches—the proposed method jointly learns embeddings from both audio (MFCCs with first and second derivatives) and character sequences using bidirectional long-short term memory (LSTM) networks. Each view is processed by stacked bidirectional LSTMs, with intermediate layer outputs concatenated and final outputs combined into 1024-dimensional embeddings used to compute cosine distances. The model employs four contrastive losses: fixed-margin losses based on matched pair distances and edit distances, and a novel cost-sensitive loss where the margin scales with the Levenshtein edit distance between character sequences. This ensures that embedding distances reflect phonetic and orthographic similarity.\n\nThe method is evaluated on three surrogate tasks: acoustic word discrimination (determining if two audio segments represent the same word), cross-view discrimination (matching written words to spoken segments), and word similarity (measuring alignment between embedding distances and edit distances). Performance is measured by average precision (AP) in classification and Spearman’s ρ in similarity tasks. The best-performing model, using fixed-margin (obj 0) and cost-sensitive (obj 2) losses, achieves AP of 0.806 and 0.892 on acoustic and cross-view tasks, significantly outperforming prior methods (e.g., DTW-based baseline: 0.214; learned autoencoders: 0.469–0.497). The cost-sensitive loss improves alignment between embedding distances and orthographic edit distances (ρ = 0.270 for text embeddings vs. 0.240 for fixed-margin), while acoustic embeddings show strong alignment with phonetic edit distances (ρ = 0.226). Despite access to text data, text embeddings do not surpass acoustic ones due to limited data diversity—fewer than 2% of word pairs have edit distances below five characters, indicating insufficient similar word pairs in the dataset.\n\nt-SNE visualizations show that both acoustic and character embeddings cluster tightly and align well, with unseen words clustering similarly to seen ones, suggesting good generalization. Bidirectional LSTMs outperform unidirectional ones, and two-layer models significantly outperform single-layer ones, with no further gains beyond two layers. Training was conducted for up to 1000 epochs, with hyperparameters tuned for dropout, learning rate, and margins. The study concludes that the multi-view framework enables more accurate and consistent representations across spoken and written queries, supporting downstream applications like spoken term detection and query-by-example search. Future work should incorporate phonetic knowledge and expand training to include non-word segments.\nТекст добавлен в файл: /kaggle/working/all_summaries.txt\nТекст добавлен в файл: /kaggle/working/all_summaries.txt\nТекст добавлен в файл: /kaggle/working/all_summaries.txt\nROUGE Metrics:\nrouge1: 0.1550\nrouge2: 0.0786\nrougeL: 0.0849\nrougeLsum: 0.1356\n\nBERTScore: F1 = 0.6900\nLongDocFACTScore for sum: -4.455914959311485\nLongDocFACTScore for abstract: -4.3108125030994415\n\n=== Статья 3 ===\nТокенов в статье: 8152\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13b142eb107e4822b5ee8c0db0b0aa72"}},"metadata":{}},{"name":"stdout","text":"\nSummary article number 3\nТокенов в summary: 541\nThis paper proposes a novel structured stationary kernel (SMD kernel) that generalizes the classical spectral mixture (SM) kernel by explicitly modeling inter-component dependencies and incorporating time-phase delays. The core goal is to overcome the limitations of existing SM variants—such as SMP, NSM, and GSM—which assume independence among spectral components and fail to capture meaningful cross-component interactions. The authors introduce a dependency structure via a complex-valued Gaussian mixture model, where cross-covariances between components are derived from the Fourier domain using Bienaymé’s identity, ensuring the resulting kernel remains positive definite. A key innovation is the use of time and phase delays (TP delays) to model temporal interference between frequency components, captured through complex exponential modulation in the spectral density. This enables a more accurate representation of real-world signals with dynamic temporal dependencies. The SMD kernel generalizes the standard SM kernel by reducing to it when dependencies are ignored, preserving compatibility while enhancing expressiveness.\n\nTo enable efficient and interpretable learning, the authors propose a Structure Adaptation (SA) algorithm that includes hyperparameter initialization via bootstrap-based histogram initialization (BHI), component pruning based on component weights, and sparsity induction through dependency intensity quantification. The SA algorithm reduces hyperparameter explosion, improves initialization, and enables scalable inference. Dependency strength is quantified by a normalized correlation coefficient γ_ij ∈ [−1,1], with γ_ij = ±1 indicating strong inter-component relationships. Performance is evaluated on synthetic and real datasets—including monthly river flow and yearly sunspot data—across interpolation, extrapolation, scalability, and structural sparsity. Results show that SMD outperforms SM and other baselines in both interpolation and extrapolation, achieving tighter 95% confidence intervals and lower mean squared error. In the river flow dataset, SMD achieves higher compression and sparsity ratios (CR = 38.9%, SR = 89.3%), indicating effective pruning of irrelevant dependencies. Posterior analysis reveals more meaningful and structured cross-covariances in SMD compared to the unstructured correlations in SM. On the abalone dataset, SMD demonstrates superior scalability and efficiency due to sparsity, with convergence rates above 30% and significantly better performance than NSM, which suffers from hyperparameter explosion. The SMD kernel with non-zero time and phase delays (θ ≠ 0, ϕ ≠ 0) consistently outperforms all alternatives. The work concludes that the proposed SMD kernel, combined with the SA algorithm, enables interpretable, expressive, and scalable modeling of time series with complex inter-component dynamics, while addressing key limitations in existing kernel designs through a principled, data-driven dependency structure.\nТекст добавлен в файл: /kaggle/working/all_summaries.txt\nТекст добавлен в файл: /kaggle/working/all_summaries.txt\nТекст добавлен в файл: /kaggle/working/all_summaries.txt\nROUGE Metrics:\nrouge1: 0.1104\nrouge2: 0.0445\nrougeL: 0.0612\nrougeLsum: 0.0900\n\nBERTScore: F1 = 0.6974\nLongDocFACTScore for sum: -4.871068179607391\nLongDocFACTScore for abstract: -4.574126243591309\n\n=== Статья 4 ===\nТокенов в статье: 13916\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13bea2034e9d45fb9458bc42254e7bdd"}},"metadata":{}},{"name":"stdout","text":"\nSummary article number 4\nТокенов в summary: 570\nThe article addresses the problem of solving systems of multivariate polynomial equations over \\( \\mathbb{F}_2 \\), which is known to be NP-complete. The central goal is to analyze the computational complexity of quantum algorithms that solve such systems via the HHL quantum linear system (QLS) approach, specifically by linearizing the polynomial system into a large sparse linear system using the Macaulay matrix. The authors demonstrate that while the HHL algorithm can efficiently prepare the linear system and extract solutions through a generalized quantum coupon collector, its practicality is limited by a fundamental issue: the condition number \\( \\kappa \\) of the Macaulay matrix. They prove an exponential lower bound on \\( \\kappa \\), specifically \\( \\kappa_{\\vec{b}}(M) \\geq \\frac{1}{2}(2^h - 1) \\), where \\( h \\) is the Hamming weight of the solution. This implies that the HHL-based quantum algorithm requires time \\( \\Omega(\\kappa) \\), which is exponential in \\( h \\), even for standard and truncated QLS variants. The bound is improved to \\( \\Omega(2^{h/2}) \\) using a refined \"Boolean Macaulay matrix\" derived via Gaussian elimination, which is smaller and preserves the solution set. For solutions with \\( h = \\Theta(\\log n) \\), this yields a polynomial lower bound, suggesting a potential superpolynomial quantum speedup over classical brute-force. However, classical Grover-based search over all assignments of Hamming weight \\( h \\) achieves time \\( O(\\sqrt{n^h}) \\), which matches or outperforms the quantum algorithm when \\( d + h \\geq n \\) or \\( h \\) is large. The analysis shows that when multiple solutions exist with the same \\( h \\), Grover search performs as well as classical methods. The authors also prove that the Boolean Macaulay matrix has full column rank and is sparse, enabling unique solution extraction via quantum state measurement. Crucially, they show that the condition number lower bound applies even under max degree, and that solution multiplicity does not reduce the condition number. The work concludes that while quantum speedup is theoretically possible for sparse solution sets with small Hamming weight, the exponential dependence on \\( h \\) limits practical advantages. For cryptographic applications, where solutions are typically unique and of high Hamming weight, the quantum algorithm does not offer a significant advantage over classical methods. The full column rank and structural correctness of the linear system are established via induction and ideal theory, confirming the validity of the reduction. Thus, the article establishes a rigorous lower bound on quantum algorithmic complexity for Boolean polynomial systems, showing that the HHL approach is not universally superior to classical or Grover-based methods.\nТекст добавлен в файл: /kaggle/working/all_summaries.txt\nТекст добавлен в файл: /kaggle/working/all_summaries.txt\nТекст добавлен в файл: /kaggle/working/all_summaries.txt\nROUGE Metrics:\nrouge1: 0.0730\nrouge2: 0.0394\nrougeL: 0.0430\nrougeLsum: 0.0594\n\nBERTScore: F1 = 0.6768\nLongDocFACTScore for sum: -4.305095386505127\nLongDocFACTScore for abstract: -4.2445260882377625\n\n=== Статья 5 ===\nТокенов в статье: 7446\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98bc544bd47b40708d6bed236bec0c06"}},"metadata":{}},{"name":"stdout","text":"\nSummary article number 5\nТокенов в summary: 658\nThis submission to the SIMMC 2.0 challenge presents a conversational agent designed to assist in shopping by interpreting task-oriented dialogues grounded in immersive visual scenes, focusing on disambiguation and coreference resolution as key sub-tasks. The primary goal is to enable accurate interpretation of user intent and object references in real-world, multi-modal dialogues. The authors develop two transformer-based models: one for disambiguation prediction using a TOD-BERT variant, and another for coreference resolution leveraging an LXMERT encoder with dialogue and visual context. The dataset comprises 11,244 dialogues in fashion and furniture domains, collected via self-play and human annotation, split into train/dev/devtest/test-std (65%/5%/15%/15%), with final performance evaluated on the unseen test-std split.\n\nFor disambiguation (Sub-Task #1), the model predicts whether clarification is needed in a user utterance based on dialogue history and scene image. The baseline GPT-2 classifier achieves 73.9% accuracy, which is improved to 88.03% by adjusting batch size and removing dialogue history. Surprisingly, models without multi-modal inputs—such as TOD-BERT and LXMERT—perform competitively, suggesting that syntactic patterns (e.g., wh-questions, prepositions, adjectives) and sentence length are strong predictors of ambiguity. A significant correlation exists between preposition/adjective usage and non-disambiguation (r = 0.402, 0.291), while wh-questions show a positive link to disambiguation (r = 0.323). Mean sentence length is shorter in disambiguation cases (13.94 vs. 18.66), indicating that language structure alone suffices for disambiguation prediction without visual or dialogue context.\n\nFor coreference resolution (Sub-Task #2), the model predicts which objects in a scene are referenced using a multi-modal LXMERT architecture. It constructs natural language sequences combining user utterances, object descriptions (derived from Detectron2 metadata), and previously mentioned objects. Object positional IDs and visual features (RoI embeddings) are integrated to identify referenced objects. Ablation studies show that language and object descriptions are most critical; removing visual features reduces F1 by 3.39%, while bounding boxes have no effect. Object position counts improve F1 by 0.05%, and global object tokens (based on prefab model IDs) boost performance from 60.83% to 68.53%. Despite the absence of visual context, the model achieves 60.83% object F1, with baseline models showing limited gains from dialogue history.\n\nThe key conclusion is that unimodal language models can effectively solve both disambiguation and coreference tasks, leveraging syntactic and lexical biases and object metadata. Vision plays a limited role due to scene clutter and insufficient object variation. Object IDs—especially when used as global, consistent tokens—enable robust coreference resolution. The results suggest that future systems should prioritize disentangled, structured object representations to improve real-world dialogue understanding.\nТекст добавлен в файл: /kaggle/working/all_summaries.txt\nТекст добавлен в файл: /kaggle/working/all_summaries.txt\nТекст добавлен в файл: /kaggle/working/all_summaries.txt\nROUGE Metrics:\nrouge1: 0.1318\nrouge2: 0.0627\nrougeL: 0.0755\nrougeLsum: 0.1149\n\nBERTScore: F1 = 0.6659\nLongDocFACTScore for sum: -4.399820227372019\nLongDocFACTScore for abstract: -5.499554531914847\n\n=== Статья 6 ===\nТокенов в статье: 9620\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41470964353c4b4ebefa4fdde886a147"}},"metadata":{}},{"name":"stdout","text":"\nSummary article number 6\nТокенов в summary: 534\nThis study presents MICDIR, a novel unsupervised deep learning method for deformable image registration in 3D brain MRI, aiming to improve both intramodal (e.g., T1w to T1w) and intermodal (e.g., T1w to T2w) registration by integrating structural brain connectivity and accommodating large deformations. The common goal is to achieve accurate, fast, and robust image alignment without manual annotations or supervised training data. The method leverages a multi-scale UNet architecture (MSCGUNet) with a self-constructing graph network (SCGNet) to encode global anatomical semantics and capture both small and large deformations. The network processes fixed and moving images, concatenates them, and predicts dense displacement vector fields (DVF) via a spatial transformer. A multi-scale design enables deep supervision at original and downsampled resolutions, improving gradient flow and stability during training. The SCGNet dynamically constructs a graph from feature maps, reducing computational load while preserving global structural relationships—critical for capturing long-range anatomical dependencies in brain images. Inverse consistency is enforced through a cycle consistency loss to ensure deformation symmetry and realistic alignment regardless of direction. The model is trained on 600 brain MRI volumes from healthy subjects using two preprocessing pipelines, with performance evaluated on 200 training and 50 test pairs. Registration is assessed using Dice coefficient (for tissue overlap), Kullback-Leibler distance (KLD), Pearson correlation, and SSIM (intramodally), with KLD used exclusively for intermodal cases due to intensity independence. Results show MICDIR outperforms ANTS (non-deep learning), ICNet, Voxelmorph, and ADMIR in four out of five metrics, with statistically significant gains in intramodal registration. It matches RMSPROP-based direct optimisation on one metric and achieves the second-highest Pearson correlation. Visual analysis confirms superior anatomical alignment, especially in cerebrospinal fluid (CSF) and brain boundary preservation. Ablation studies validate the contributions of multi-scale supervision, inverse consistency, and SCGNet: each improves performance, with MSS and IC showing significant gains and SCGNet enhancing generalization across preprocessing pipelines. MICDIR also demonstrates strong inverse consistency, with no significant direction-dependent degradation, unlike other deep learning methods. The study concludes that MICDIR effectively integrates structural connectivity via SCGNet, enables robust multi-scale deformation modeling, and achieves state-of-the-art performance in both intramodal and intermodal brain MRI registration, offering a scalable, unsupervised solution with improved generalization and anatomical fidelity.\nТекст добавлен в файл: /kaggle/working/all_summaries.txt\nТекст добавлен в файл: /kaggle/working/all_summaries.txt\nТекст добавлен в файл: /kaggle/working/all_summaries.txt\nROUGE Metrics:\nrouge1: 0.0890\nrouge2: 0.0378\nrougeL: 0.0510\nrougeLsum: 0.0725\n\nBERTScore: F1 = 0.7035\nLongDocFACTScore for sum: -4.840327167510987\nLongDocFACTScore for abstract: -4.316442532972856\n\n=== Статья 7 ===\nТокенов в статье: 11988\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/13 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5669dd826df42bfb02c936c668e63c5"}},"metadata":{}},{"name":"stdout","text":"\nSummary article number 7\nТокенов в summary: 536\nThis tutorial provides a comprehensive overview of the application of Transformer architectures in time-series analysis, focusing on their structural foundations, methodological adaptations, and performance advantages over traditional models. The core goal is to enable efficient, accurate, and scalable forecasting and classification of time-series data—such as stock prices, sensor readings, and biomedical signals—by leveraging the parallelizable self-attention mechanism that overcomes the limitations of recurrent models like RNNs, LSTMs, and GRUs, which suffer from vanishing gradients and poor parallelization.\n\nThe original Transformer architecture, introduced by Vaswani et al., relies on self-attention, scaled dot-product attention, multi-head attention, and sinusoidal positional encoding to preserve sequence order and enable parallel computation. Input sequences are embedded into vectors, with positional encoding—using sinusoidal functions—encoding relative temporal positions without requiring absolute indexing. Multi-head attention allows the model to capture diverse temporal relationships, such as distinguishing different meanings of repeated words or events in time-series data. Encoder and decoder blocks, with residual connections and layer normalization, ensure stable training and efficient gradient flow. The decoder’s masked self-attention prevents future information leakage, ensuring causality during training.\n\nKey architectural modifications have been developed to enhance efficiency and scalability for time-series tasks. Models such as Informer, LogSparse Transformers, and TCCT introduce sparse attention mechanisms (e.g., ProbSparse, cross-stage partial attention), dilated convolutions, and temporal encoding to reduce computational complexity from O(L²) to O(L log L) or O(L log² L), enabling long-sequence modeling. Gating mechanisms and convolutional layers (e.g., in TFT, TabAConvBERT) improve feature extraction and temporal modeling. Some models, like the Traffic Transformer, integrate graph neural networks for spatial dependencies with Transformers for temporal dynamics.\n\nEmpirical results show that Transformers outperform traditional methods (ARIMA, LSTMs, attention-based sequence models) in forecasting tasks such as influenza prediction and traffic flow, achieving lower RMSE. In classification tasks, such as vegetation type identification from satellite images, Transformers surpass LSTM-RNN, MS-ResNet, and random forest methods. Performance is robust across diverse data types, though pre-processing (e.g., time-delay embedding) can influence outcomes. Training challenges—including gradient instability and memory cost—are addressed via adaptive optimizers, layer normalization, warm-up strategies, and distributed training.\n\nIn conclusion, the Transformer architecture provides a powerful, parallelizable, and scalable framework for time-series modeling, outperforming classical methods in both forecasting and classification. Its adaptability through architectural modifications and preprocessing strategies makes it a leading approach in modern time-series analysis.\nТекст добавлен в файл: /kaggle/working/all_summaries.txt\nТекст добавлен в файл: /kaggle/working/all_summaries.txt\nТекст добавлен в файл: /kaggle/working/all_summaries.txt\nROUGE Metrics:\nrouge1: 0.0763\nrouge2: 0.0378\nrougeL: 0.0431\nrougeLsum: 0.0695\n\nBERTScore: F1 = 0.6837\nLongDocFACTScore for sum: -4.655131297952988\nLongDocFACTScore for abstract: -4.566188255945842\n\n=== Статья 8 ===\nТокенов в статье: 12330\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/13 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab2a544f126f4394a08526ffc50c162e"}},"metadata":{}},{"name":"stdout","text":"\nSummary article number 8\nТокенов в summary: 662\nThe article presents BASALISC, a hardware accelerator for Fully Homomorphic Encryption (FHE) based on the BGV scheme, designed to enable efficient, high-performance homomorphic computation. The common goal is to overcome the computational and noise management challenges in FHE, particularly the high cost of bootstrapping and the need for scalable, low-latency operations. The BGV scheme operates in the ring \\( R = \\mathbb{Z}[X]/(X^N + 1) \\), with plaintexts in \\( R_t \\) and ciphertexts in \\( R_{2q} \\), where noise accumulation limits operation depth. To manage noise, key switching and modulus switching are used, while bootstrapping restores noise via homomorphic decryption. The core challenge lies in reducing computational overhead and memory footprint, especially during multiplication and permutation operations, which dominate execution time and data movement.\n\nBASALISC accelerates five fundamental operations—addition, multiplication, permutation, modulus switching, and key switching—using a hybrid architecture combining residue number systems (RNS) and the Number-Theoretic Transform (NTT). It leverages RNS to split modular arithmetic into smaller 32-bit components, reducing complexity and enabling efficient parallelism. Polynomial operations are accelerated via NTT, implemented with a radix-256 butterfly structure optimized for coefficient-level parallelism. The design features a conflict-free content-addressable memory (CTB) layout and a generalized permutation PE supporting both data reordering and ring automorphisms (e.g., \\( X \\mapsto X^k \\)) without additional hardware. A key innovation is the use of NTT-friendly primes (≥26 bits), enabling efficient hardware multipliers and reducing area and power consumption by 40% and 46%, respectively.\n\nTo manage noise and enable deep computation, BASALISC implements a generalized bootstrapping procedure tailored to NTT-friendly primes, using fast base extension and small Montgomery reduction to lift moduli and correct overflows. This maintains the same computational cost as conventional bootstrapping while enabling hardware implementation. The system supports up to 128-bit security with ring dimension \\( N \\in [2^{14}, 2^{16}] \\), enabling 31 multiplicative levels without bootstrapping. Performance is evaluated on benchmarks including NTT, database lookup, and thin bootstrapping, achieving speedups of up to 8,700×, 4,000×, and 2,600× compared to HElib software on Intel Xeon.\n\nThe architecture is implemented in a 150 mm² footprint using standard commercial technologies (DDR4, PCIe, RISC-V), with a hierarchical structure including a system board, ASIC, and FHE core. It features a massively parallel, multicore design with dedicated PEs for MAC, permutation, and NTT operations, optimized for coefficient-level parallelism. The design is verified via formal methods (Cryptol, SAW, Synopsys Formality) and validated through hardware emulation and performance benchmarks. Unlike prior accelerators, BASALISC supports full BGV operations—including automorphisms and packing—without requiring large SRAM or host processor interaction, offering superior efficiency and scalability.\nТекст добавлен в файл: /kaggle/working/all_summaries.txt\nТекст добавлен в файл: /kaggle/working/all_summaries.txt\nТекст добавлен в файл: /kaggle/working/all_summaries.txt\nROUGE Metrics:\nrouge1: 0.0907\nrouge2: 0.0391\nrougeL: 0.0470\nrougeLsum: 0.0802\n\nBERTScore: F1 = 0.6699\nLongDocFACTScore for sum: -4.612411472532484\nLongDocFACTScore for abstract: -5.4921878178914385\n\n=== Статья 9 ===\nТокенов в статье: 7207\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/6 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f10702eae794a6da7db8eff21541fd3"}},"metadata":{}},{"name":"stdout","text":"\nSummary article number 9\nТокенов в summary: 700\nThe article presents a comprehensive analysis of existing ternary logic circuit designs, with a focus on identifying and evaluating efficient, scalable, and power-optimized architectures. The primary goal is to identify and refine ternary adder and full-adder (TFA) designs that minimize transistor count, reduce power dissipation, and eliminate voltage division—common sources of power loss and performance degradation. The authors begin by reviewing 84 existing TFA designs, then systematically eliminate non-competitive categories: non-transistor-based circuits, multi-VDD designs, balanced ternary designs with −VDD requirements, CML designs due to high power consumption, RTL and pseudo-NMOS due to poor power efficiency, CTL due to noise sensitivity, and dynamic logic due to clock dependency. After filtering, 25 designs remain, with further reduction to 11 unique, non-redundant designs focused on single-VDD, static, full-swing operation. The analysis centers on circuit topology and logic style, with key styles including ternary CMOS, dynamic DCVSL, NT/PT functions, and MUX-based approaches.\n\nA core innovation is the development of partial TFA designs that exploit the binary nature of one input signal (c), which never takes the value '2'. This enables the elimination of redundant transistors: p-type transistors with low-V_T are replaced with wires (always ON), and high-V_T p-type transistors are replaced with open circuits (never ON). Crucially, the output carry voltage when carry = '1' is set to V_DD, eliminating internal voltage division and reducing power dissipation. This simplification reduces transistor count and improves power-delay product (PDP). For example, one complete TFA is transformed into a partial TFA with 32 fewer transistors and 70.2% lower PDP. Another design reduces by 56 transistors and improves PDP by 46.3%. In a separate modification, the original ternary adder cell is restructured: carry generation uses p-type transistors for full-swing operation, middle PTs are replaced with TGs, and low-V_T n-type transistors are upgraded to ultra-low-V_T variants. This preserves transistor count but improves performance. A simplified partial TFA with V(Carry='1') = V_DD eliminates three voltage divisions, reducing average power by 26.9%. One design replaces ternary gates with binary gates, removes redundant AND gates, and reduces transistor count from 1130 to 864, achieving 22.8% better power efficiency and 24.8% higher speed.\n\nAll designs are simulated using HSPICE with a 32nm CNFET library at 0.9V and 1GHz, using a ternary FO4 load. Key results show that binary-based designs offer better driving capability but longer critical paths; cascaded THAs perform well due to parallelism; and voltage division at ½V_DD is a major power sink. The DCVSL-based partial TFA achieves the best delay and PDP, while the minimal-transistor design consumes the least power. The study concludes that by leveraging input decoding, eliminating redundant transistors, and setting V(Carry='1') = V_DD, significant improvements in power efficiency, speed, and area can be achieved—demonstrating a clear path toward practical\nТекст добавлен в файл: /kaggle/working/all_summaries.txt\nТекст добавлен в файл: /kaggle/working/all_summaries.txt\nТекст добавлен в файл: /kaggle/working/all_summaries.txt\nROUGE Metrics:\nrouge1: 0.1597\nrouge2: 0.0677\nrougeL: 0.0917\nrougeLsum: 0.1349\n\nBERTScore: F1 = 0.6601\nLongDocFACTScore for sum: -4.973611630891499\nLongDocFACTScore for abstract: -5.229717884744916\n\n=== Статья 10 ===\nТокенов в статье: 7913\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"192be5b354804577aae33cebb114ef24"}},"metadata":{}},{"name":"stdout","text":"\nSummary article number 10\nТокенов в summary: 505\nThe AudioLM framework presents a hierarchical, hybrid tokenization approach to audio generation that simultaneously enables high-fidelity audio synthesis and long-term linguistic coherence. The core goal is to achieve both high-quality audio reconstruction and semantic consistency in speech and music, by disentangling acoustic and semantic content through a two-stage tokenization scheme. Input audio sequences are first tokenized into acoustic tokens via SoundStream, a neural codec that downsamples at 16 kHz to 50 Hz with residual vector quantization (RVQ), achieving a bitrate of 2000 bps and preserving waveform fidelity. Concurrently, semantic tokens are derived from the 7th layer of w2v-BERT using k-means clustering (K=1024), capturing linguistic structure with strong phonetic discriminability (low ABX error) but poor reconstruction quality. These two token types are combined in a three-stage autoregressive Transformer architecture: semantic tokens are predicted first to model long-term structure; coarse acoustic tokens (from 4 quantizer layers) are generated next to capture speaker identity and recording conditions; fine acoustic tokens (from 8 layers) are modeled last to refine residual details and eliminate compression artifacts. This hierarchical design reduces sequence length and improves scalability, while enabling efficient training on 16 TPUv4s with 256-batch size over 1M steps.  \n\nKey results show that semantic tokens effectively capture lexical and syntactic content, outperforming prior models in sWUGGY and sBLIMP metrics, and enabling generation of semantically correct, diverse speech. Acoustic tokens preserve speaker identity and recording conditions, as verified by speaker classification (92% accuracy in continuations) and subjective evaluation showing human indistinguishability from real speech (p=0.23). In music generation, AudioLM maintains melody, harmony, and rhythm in continuations, outperforming acoustic-only models in subjective preference (83.3% preference) despite equal audio quality. The framework is trained on 60k hours of English speech (unlab-60k) and tested on unseen speakers and music genres, demonstrating robust generalization. To mitigate misuse, a detection model achieves 98.6% accuracy in identifying synthetic audio. The study concludes that AudioLM’s hybrid tokenization and hierarchical modeling successfully balance reconstruction fidelity with long-term structural coherence, enabling coherent, high-quality audio generation across speech and music domains, with clear implications for assistive technologies and responsible AI deployment.\nТекст добавлен в файл: /kaggle/working/all_summaries.txt\nТекст добавлен в файл: /kaggle/working/all_summaries.txt\nТекст добавлен в файл: /kaggle/working/all_summaries.txt\nROUGE Metrics:\nrouge1: 0.1081\nrouge2: 0.0479\nrougeL: 0.0605\nrougeLsum: 0.0876\n\nBERTScore: F1 = 0.6906\nLongDocFACTScore for sum: -4.765718996524811\nLongDocFACTScore for abstract: -4.645268610545567\n\n=== Статья 11 ===\nТокенов в статье: 10101\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/9 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a4605bfea744e7e92888b562e9d5c5c"}},"metadata":{}},{"name":"stdout","text":"\nSummary article number 11\nТокенов в summary: 551\nThe article presents LIBRO, a novel framework that leverages large language models (LLMs) to generate and filter bug-reproducing test cases directly from natural language bug reports. The core goal is to address the underexplored problem of report-to-test generation—where bug reports are frequently used to create tests for debugging—by enabling automated, reliable test generation without requiring crash traces or prior test suites. The authors argue that such generation is both prevalent and essential, as analysis of 300 open-source Java projects shows that 28.4% of test additions are tied to issue-linked commits, indicating real-world relevance. Despite this, prior work focuses narrowly on crash reproduction or specific cases, missing the broader need for semantic bug validation.\n\nLIBRO uses pretrained LLMs (e.g., OpenAI Codex) to generate test methods from bug reports via carefully engineered prompts, including instructions and code scaffolding. Generated tests are postprocessed to resolve missing imports and dependencies, with a focus on integrating them into existing test classes. A test is considered a Bug Reproducing Test (BRT) only if it fails on the buggy version and passes on the fixed version, distinguishing it from crash-based methods. To avoid developer overload, LIBRO applies filtering based on output agreement (max_output_clus_size) and ranks test candidates using three heuristics: relevance to the bug report, consensus in failure output, and test length (shorter tests preferred). The framework is evaluated on Defects4J (750 bugs) and a new dataset, GHRB (31 recent bugs), with performance measured by reproduction rate, precision, and developer effort reduction.\n\nResults show LIBRO generates at least one BRT for 33.5% of Defects4J bugs and correctly identifies successful reproductions with 71.4% accuracy. It produces a first successful test suggestion for 149 bugs and reproduces 32.2% of bugs on GHRB, demonstrating robustness across datasets. The framework achieves a 33% reduction in wasted inspection effort and ranks tests more effectively than random baselines. While performance varies by project (e.g., lower in Closure due to complex structure), the selection and ranking heuristics remain valid across datasets. Case studies confirm that LIBRO can generate valid tests even from reports without code snippets, with 81% token overlap in cases with partial snippets. However, limitations persist in handling external dependencies, custom helper functions, and non-standard test infrastructures. The study concludes that LIBRO is the first method to generate general-purpose, semantically meaningful tests from bug reports, offering a practical, developer-friendly solution to a critical gap in automated debugging.\nТекст добавлен в файл: /kaggle/working/all_summaries.txt\nТекст добавлен в файл: /kaggle/working/all_summaries.txt\nТекст добавлен в файл: /kaggle/working/all_summaries.txt\nROUGE Metrics:\nrouge1: 0.0918\nrouge2: 0.0410\nrougeL: 0.0534\nrougeLsum: 0.0792\n\nBERTScore: F1 = 0.6889\nLongDocFACTScore for sum: -4.525179415941238\nLongDocFACTScore for abstract: -5.015513944625854\n\n=== Статья 12 ===\nТокенов в статье: 18119\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/14 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4026d8a219e24417b499b2ac176ce494"}},"metadata":{}},{"name":"stdout","text":"\nSummary article number 12\nТокенов в summary: 483\nThe paper addresses the long-standing challenge of proving convergence of iterative methods in parameter identification for partial differential equations (PDEs) based on boundary measurements, by establishing a general framework for range invariance of the linearized forward operator—without requiring restrictive nonlinearities or full observation. The central goal is to ensure convergence of regularization methods, such as Newton-type iterations, under practical conditions relevant to inverse problems in tomography and nondestructive testing. The authors introduce a condition of range invariance of the derivative \\( F' \\), which is independent of the observation operator \\( C \\) and thus directly applicable to boundary data. This condition is shown to be equivalent to the boundedness and bounded invertibility of the operator \\( F'(x)^{-1}F'(x) \\) in a neighborhood of the exact solution, and can be rephrased via a bounded invertible operator \\( R(x) \\) such that \\( F'(x) = F'(x_0)R(x) \\). To overcome the loss of unique identifiability that arises when extending the parameter space (e.g., to handle multiple coefficients or time dependence), the authors introduce a penalty functional \\( P(q) \\) that enforces the original parameter dependency in the limit. This penalty ensures that the reconstructed solution converges to the true solution in the original, smaller parameter space.\n\nThe analysis applies to both reduced and all-at-once formulations, with the reduced case offering greater flexibility in function space choices. The paper verifies range invariance in three representative examples: identification of a spatially varying potential in elliptic PDEs, time-dependent diffusion coefficients, and diffusion coefficients in steady-state tomography. In each case, the linearized operators are shown to satisfy range invariance via orthogonality arguments or spectral properties, with the nullspace of the derivative intersecting trivially with the penalty nullspace. Convergence of the regularized Newton-type iterates is proven under standard assumptions, including continuity of the forward operator and bounded invertibility of its derivative. The iterates remain in a neighborhood of the true solution and converge to it as the regularization parameter vanishes or the iteration count increases. The key result is that range invariance, combined with a properly designed penalty, enables convergence of iterative reconstruction methods for practical PDE-based inverse problems without requiring restrictive assumptions on nonlinearity or full observation.\nТекст добавлен в файл: /kaggle/working/all_summaries.txt\nТекст добавлен в файл: /kaggle/working/all_summaries.txt\nТекст добавлен в файл: /kaggle/working/all_summaries.txt\nROUGE Metrics:\nrouge1: 0.0598\nrouge2: 0.0330\nrougeL: 0.0373\nrougeLsum: 0.0511\n\nBERTScore: F1 = 0.6890\nLongDocFACTScore for sum: -4.248531540234883\nLongDocFACTScore for abstract: -4.684328715006511\n\n=== Статья 13 ===\nТокенов в статье: 10043\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b74fe50d411641d188fd51e12c13f95c"}},"metadata":{}},{"name":"stdout","text":"\nSummary article number 13\nТокенов в summary: 558\nThe paper presents a novel hybrid automaton (HA)-learning algorithm that infers hybrid systems from input-output trajectories by modeling both continuous dynamics and discrete mode transitions. The core goal is to learn a hybrid automaton (HA) that captures the system’s behavior, including distinct continuous dynamics in each location, nonlinear ordinary differential equations (ODEs), and both exogenous and endogenous transitions—features absent in prior methods. The algorithm operates offline and passively, without requiring system interaction or external intervention. It begins by segmenting trajectories into continuous dynamics segments using backward and forward backward differentiation formulas (BDF) to detect discontinuities in derivatives, identifying change points where derivative jumps occur. These segments are then clustered based on shape similarity using dynamic time warping (DTW), with clustering guided by DTW distance and correlation thresholds. Each cluster is interpreted as a discrete location in the HA, justified under the assumption of distinct ODEs per location. The continuous dynamics within each location are modeled via polynomial ODEs inferred through template-based linear regression, enabling support for nonlinear flows—unlike prior methods limited to linear ODEs.\n\nTransitions between locations are identified from chronological sequences of segments, with guards and assignments inferred data-driven. Guards are learned using support vector machines (SVM) with a polynomial kernel to classify transition points, forming inequality constraints that separate pre- and post-transition states. Assignments are inferred via linear regression, with variable types introduced to improve accuracy: \"no assignments\" for continuous variables, \"constant assignments\" for reset values, and \"constant pool\" for discrete-valued variables. These type annotations leverage domain knowledge to refine assignment inference. The method supports variable resets at transitions and handles both exogenous and endogenous mode changes, enabling accurate modeling of systems like a bouncing ball or excitable cells.\n\nThe algorithm is evaluated on five benchmarks—Ball, Tanks, Osci, Cells, and Engine—using DTW distance to measure trajectory similarity between original and learned models. Results show that the proposed method achieves significantly lower DTW distances, with minimal deviation between learned and original trajectories, outperforming POSEHAD in both accuracy and generalization. It also demonstrates superior scalability and speed: training completes in under one hour (vs. over 53 hours for POSEHAD), with faster convergence even at large data scales. The learned models accurately recover ODEs, guards, and assignments, with type annotations enabling exact trajectory reconstruction. The method is implemented in C++, Python, and MATLAB/Simulink/Stateflow, with code and artifact available online. The study concludes that the algorithm provides a robust, practical, and accurate framework for learning hybrid systems with nonlinear dynamics, endogenous transitions, and structured assignments—addressing key limitations of existing approaches.\nТекст добавлен в файл: /kaggle/working/all_summaries.txt\nТекст добавлен в файл: /kaggle/working/all_summaries.txt\nТекст добавлен в файл: /kaggle/working/all_summaries.txt\nROUGE Metrics:\nrouge1: 0.1033\nrouge2: 0.0436\nrougeL: 0.0564\nrougeLsum: 0.0866\n\nBERTScore: F1 = 0.6613\nLongDocFACTScore for sum: -5.239806903733148\nLongDocFACTScore for abstract: -5.03673513730367\n\n=== Статья 14 ===\nТокенов в статье: 6209\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ead961221f84e9ea3623a7b7c016996"}},"metadata":{}},{"name":"stdout","text":"\nSummary article number 14\nТокенов в summary: 604\nThis article addresses the challenge of real-time video super-resolution (VSR) in adaptive online streaming, where dynamic degradations caused by fluctuating bandwidth and variable quantization parameters (QP) severely degrade video quality. Unlike traditional static super-resolution methods, online streaming VSR must operate under strict latency and inference speed constraints, while adapting to unpredictable low-bitrate conditions. Existing approaches—particularly LUT-based methods—struggle due to fixed mappings and limited adaptability to dynamic degradations, while deep learning models like CNNs suffer from high computational cost and latency. To bridge this gap, the authors propose a novel framework based on *grouped expert Look-Up Tables (LUTs)*, designed specifically for real-time streaming environments.\n\nThe method introduces a set of expert LUTs, each trained on representative static degradations (defined by QP values) and specialized in handling specific types of compression artifacts. Each LUT maps low-resolution (LR) patches to high-resolution (HR) outputs via a 2×2 input and scale factor r=4, with a 4D lattice structure. To reduce storage overhead, LUTs are compressed using uniform sampling (interval size 16), balancing performance and efficiency. During inference, a lightweight pixel-wise weight predictor generates adaptive fusion weights based on local degradation, enabling spatial and temporal adaptation. A novel efficient interpolation scheme splits input pixel bits into high (MSB) and low (LSB) bits: MSBs locate the 4D LUT lattice, while LSBs determine interpolation via a pre-defined 4-bit permutation table, enabling parallel, CUDA-accelerated computation without complex control flow.\n\nThe model avoids heavy temporal modules (e.g., optical flow, deformable convolutions) due to latency constraints, instead using a temporal branch that fuses previous and current frames with motion vectors—available as prior knowledge in streaming systems—without extra computation. Experiments on the newly proposed LDV-WebRTC dataset (simulating real-world bandwidth variations at 100Kbps, 500Kbps, and 1Mbps) show that the method achieves superior performance in PSNR and SSIM compared to LUT-based and CNN-based baselines, especially under low-bandwidth conditions. It outperforms PAN and SR-LUT in both quality and latency, achieving up to 100 FPS with 720P resolution. While not surpassing large bi-directional models like TTVSR, it matches their quality with significantly lower latency. The spatial branch handles spatial degradation via adaptive LUT fusion, improving PSNR by 0.26 dB over a single LUT baseline. Temporal refinement further enhances quality at the cost of slight speed reduction. Storage increases linearly with expert LUT count (e.g., +8.52 MB), deemed acceptable for practical deployment. The method achieves a balance between performance, efficiency, and real-time adaptability, making it suitable for real-world online streaming applications.\nТекст добавлен в файл: /kaggle/working/all_summaries.txt\nТекст добавлен в файл: /kaggle/working/all_summaries.txt\nТекст добавлен в файл: /kaggle/working/all_summaries.txt\nROUGE Metrics:\nrouge1: 0.1544\nrouge2: 0.0602\nrougeL: 0.0820\nrougeLsum: 0.1295\n\nBERTScore: F1 = 0.6860\nLongDocFACTScore for sum: -4.745161308961756\nLongDocFACTScore for abstract: -4.606397390365601\n\n=== Статья 15 ===\nТокенов в статье: 5569\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/6 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e2e62447a4784ed487e64f8655d606a3"}},"metadata":{}},{"name":"stdout","text":"\nSummary article number 15\nТокенов в summary: 506\nThis study presents a kinodynamics-based pose optimization framework integrated with real-time model predictive control (MPC) for humanoid robots performing dynamic pushing tasks. The central goal is to enable autonomous, stable, and robust whole-body pushing of large, heavy, and ungraspable objects by mimicking human behavior—such as leaning forward, lowering the center of mass, and shifting feet backward—to enhance stability and force generation. Unlike prior methods that rely on trajectory optimization or neglect object-robot dynamics, the proposed approach explicitly incorporates object-robot interactions, kinematic constraints, and physical properties like friction and mass in a single, task-specific whole-body pose. The method formulates an optimal pushing pose—defined by body Euler angles and joint configurations—as a nonlinear programming (NLP) problem, solving for a static, single-configuration solution that avoids the computational complexity of time-discretized trajectory planning. This pose is derived from object parameters (size, mass, friction) and setup conditions (contact location, terrain), ensuring adaptability across diverse tasks. The resulting optimal pose is then fed into a real-time loco-manipulation MPC framework that coordinates locomotion and manipulation in a unified control loop. The MPC uses a simplified rigid-body dynamics model to track the desired pose, manage ground reaction forces, and enforce physical constraints such as friction limits, joint limits, and zero y-direction hand force to prevent slipping. The system is designed for double-leg support, enabling generalization across gaits and eliminating the need for explicit ground force transmission to the MPC. Real-time control is achieved through a 27-dimensional state-space model that includes robot and object states, with Cartesian PD controllers used for swing leg and hand position tracking. The framework is validated in high-fidelity MATLAB/Simulink simulations, demonstrating successful pushing of objects up to 1 m in size and 20 kg in mass (118% of robot mass) without exceeding motor limits. It also shows robust recovery from external disturbances—such as a 120 N lateral impulse for 0.3 s—and accurate correction of object yaw deviations. The method is extended to 3D, enabling trajectory-based pushing and yaw tracking. Key results confirm that the framework achieves stable pushing, generalizes across object parameters, and maintains stability under disturbances. The conclusion is that the proposed kinodynamic pose optimization combined with real-time MPC enables safe, efficient, and robust whole-body pushing in dynamic environments, offering a practical and scalable solution for humanoid robots in real-world manipulation tasks.\nТекст добавлен в файл: /kaggle/working/all_summaries.txt\nТекст добавлен в файл: /kaggle/working/all_summaries.txt\nТекст добавлен в файл: /kaggle/working/all_summaries.txt\nROUGE Metrics:\nrouge1: 0.1553\nrouge2: 0.0649\nrougeL: 0.0815\nrougeLsum: 0.1188\n\nBERTScore: F1 = 0.7224\nLongDocFACTScore for sum: -4.736291238239834\nLongDocFACTScore for abstract: -4.413513592311314\n\n=== Статья 16 ===\nТокенов в статье: 6719\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/6 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dcebd5842a3c4baf968d6af0a4871e45"}},"metadata":{}},{"name":"stdout","text":"\nSummary article number 16\nТокенов в summary: 588\nThe article proposes SwiftFormer, a novel hybrid vision architecture designed for efficient, real-time mobile deployment, addressing the long-standing trade-off between accuracy and computational efficiency in mobile vision models. The core goal is to enable effective global context modeling on mobile devices without the high latency and memory overhead of traditional self-attention mechanisms. Existing transformer-based models, though accurate, suffer from quadratic computational complexity (O(n²d)) due to dot-product operations in multi-head self-attention (MHSA), making them impractical for mobile inference. In contrast, convolutional neural networks (CNNs) offer speed and efficiency but struggle with long-range dependencies and local connectivity. Hybrid architectures attempt to bridge this gap by combining CNNs with self-attention in later stages, yet remain limited by the computational cost and stage-specific application of MHSA.\n\nTo overcome these limitations, the authors introduce **efficient additive attention**, a novel self-attention mechanism that replaces dot-product operations with element-wise multiplications and linear transformations, eliminating explicit key-value interactions and matrix multiplications. This design achieves linear computational complexity (O(nd)) with respect to token length, enabling scalable and fast global context modeling. Unlike prior methods that apply MHSA only in later stages, efficient additive attention is applied uniformly across all network stages, ensuring consistent local-global feature learning and better generalization at high resolutions. The mechanism generates global queries via projection and pooling, then computes attention through element-wise interaction with keys, followed by a linear transformation to produce contextual features.\n\nThe SwiftFormer architecture builds upon EfficientFormer by replacing ineffective pool mixers with depth-wise convolution (DWConv) encoders and integrating efficient additive attention in every stage. It consists of four hierarchical stages, each extracting features at different scales via patch embedding, DWConv, and efficient additive attention. Models are evaluated on ImageNet-1K, MS-COCO 2017, and ADE20K, demonstrating superior performance in speed-accuracy trade-offs. SwiftFormer-S achieves 78.5% top-1 accuracy with 0.8 ms latency on iPhone 14—1.6× faster than EfficientNet-b0 and 1.4% more accurate. SwiftFormer-L3 reaches 83.0% accuracy at 1.9 ms, outperforming ResNet-50 and ConvNeXt-T in both speed and accuracy. It is 2.6× faster than MobileViT-v2×2.0 and achieves 0.5% higher accuracy than DeiT-S despite being smaller. Error analysis shows improved detection of large objects and better localization. The model excels in small and medium object detection and achieves accurate segmentation across diverse scenes. By eliminating key-value interactions and positional encodings, the method reduces latency by 10% and improves robustness to input resolution. SwiftFormer is thus presented as the most efficient hybrid architecture for real-time mobile vision tasks.\nТекст добавлен в файл: /kaggle/working/all_summaries.txt\nТекст добавлен в файл: /kaggle/working/all_summaries.txt\nТекст добавлен в файл: /kaggle/working/all_summaries.txt\nROUGE Metrics:\nrouge1: 0.1514\nrouge2: 0.0781\nrougeL: 0.0827\nrougeLsum: 0.1283\n\nBERTScore: F1 = 0.6931\nLongDocFACTScore for sum: -4.564792783636796\nLongDocFACTScore for abstract: -3.747978130976359\n\n=== Статья 17 ===\nТокенов в статье: 9619\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/9 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d5b6e67b15440ca9d5d750812f72ce2"}},"metadata":{}},{"name":"stdout","text":"\nSummary article number 17\nТокенов в summary: 537\nThis study identifies a novel class of attacks that exploit imperceptible text encoding variations to manipulate search engine results and downstream AI systems, posing a significant threat to information integrity. The core goal is to demonstrate how adversaries can deceive search engines—both commercial (e.g., Google, Bing) and open-source (e.g., Elasticsearch)—by introducing subtle, visually undetectable perturbations in text encoding, without altering the visible content. These perturbations—such as invisible characters (e.g., Zero Width Space), homoglyphs (e.g., Latin H vs. Cyrillic Н), reordering (e.g., Right-to-Left Override), and deletions—produce identical rendered text but differ in binary encoding. By exploiting the lack of cross-representation awareness in text processing pipelines, adversaries can either hide malicious content from standard searches (hiding potential) or push false content into results (surfacing potential), effectively manipulating information retrieval.\n\nThe authors evaluate three key metrics—Disruption, Hiding, and Surfacing Potential—to quantify vulnerability. Experiments across Google, Bing, and Elasticsearch show that Bing is particularly vulnerable, with high success rates in both hiding and surfacing attacks, especially using reordering and homoglyphs. Google shows partial resilience, correctly hiding some perturbations (e.g., ZWJ, ZWNJ) and treating them as duplicates, but remains vulnerable to deletions, homoglyphs, and ZWSPs. Elasticsearch, due to open access, reveals full vulnerability, treating perturbed content as distinct unless specific control characters are ignored. The attacks extend to AI-powered search assistants (e.g., Bing’s GPT-4, Google’s Bard), where perturbations disrupt cited sources and degrade text output, with RLO and homoglyphs being most effective. Similarly, adversarial attacks on text summarization and plagiarism detection models (e.g., Grammarly) succeed in degrading output quality and inducing misclassification, with 100% success in plagiarism detection.\n\nThe findings confirm that even state-of-the-art search systems and AI models are vulnerable to imperceptible encoding manipulations. The authors recommend simple, practical defenses: visual indicators (e.g., warnings for suspicious characters) and input sanitization (e.g., removing invisible characters, resolving control sequences, mapping homoglyphs). These measures are feasible and effective. The study underscores a fundamental flaw in current text processing systems: the assumption that visual identity equals semantic identity. Without such safeguards, disinformation campaigns can exploit these vulnerabilities to distort public discourse, limit access to legal information, and manipulate public opinion through subtle, undetectable manipulations.\nТекст добавлен в файл: /kaggle/working/all_summaries.txt\nТекст добавлен в файл: /kaggle/working/all_summaries.txt\nТекст добавлен в файл: /kaggle/working/all_summaries.txt\nROUGE Metrics:\nrouge1: 0.0910\nrouge2: 0.0348\nrougeL: 0.0494\nrougeLsum: 0.0764\n\nBERTScore: F1 = 0.6706\nLongDocFACTScore for sum: -4.421796035766602\nLongDocFACTScore for abstract: -4.887451251347859\n\n=== Статья 18 ===\nТокенов в статье: 8568\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/9 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10a3d73500d64f3ea7c7e7752b795ff4"}},"metadata":{}},{"name":"stdout","text":"\nSummary article number 18\nТокенов в summary: 522\nThis article presents VoxTracer, a novel traceable voice conversion (VC) framework designed to enable robust speaker identity recovery from degraded, compressed, or processed speech—addressing critical gaps in current VC systems where source traceability is absent. The core goal is to enable reliable source speaker identification and original speech restoration in voice conversion, countering misuse such as deepfake fraud, reputational harm, and financial manipulation. Unlike traditional audio watermarking, which embeds fragile, error-prone signals and fails under compression or processing, VoxTracer integrates speaker identity imperceptibly into the VC process using speaker embeddings and a VAE-Glow architecture. It operates in four stages: voice conversion, identity hiding, transmission, and tracing. During hiding, a speaker embedding is encoded into a Gaussian-distributed latent code via a VAE encoder and embedded into the speech generator’s latent space without degrading perceptual quality. The system is designed to work across arbitrary VC architectures—both two-stage and one-stage—and major audio codecs (AAC, MP3, Opus, SILK) at low bitrates (as low as 16 kbps).\n\nVoxTracer achieves near-perfect traceability under real-world degradation. Experiments on VCTK and LibriSpeech datasets show 100% tracing accuracy (TA) and near-ideal mean cosine similarity (MCS) across all codecs and bitrates, even after severe compression and audio processing (e.g., noise, resampling, filtering). Its robustness is validated through extensive testing, including live-streaming scenarios with chunked speech, where accuracy reaches 99.87% with a single 0.74s chunk and approaches 100% with multiple chunks. Crucially, the system avoids identity leakage: only 1.84% of embeddings match source speakers by chance, confirming traceability stems from design, not residual information. The framework also enables full speech restoration using a content representation and recovered embedding, with restored speech achieving high intelligibility (MOS-I) and speaker similarity (MOS-S), even at low bitrates. The VAE-Glow structure is trained asynchronously—encoder and generator trained separately during hiding, inverter and decoder during tracing—ensuring stability and robustness against degradation. The system is efficient, with hiding time comparable to standard VC (0.27s), and supports real-time deployment. The results demonstrate that VoxTracer provides a practical, scalable, and secure solution for source traceability in voice conversion, meeting regulatory and judicial needs for accountability in audio content.\nТекст добавлен в файл: /kaggle/working/all_summaries.txt\nТекст добавлен в файл: /kaggle/working/all_summaries.txt\nТекст добавлен в файл: /kaggle/working/all_summaries.txt\nROUGE Metrics:\nrouge1: 0.0970\nrouge2: 0.0375\nrougeL: 0.0566\nrougeLsum: 0.0817\n\nBERTScore: F1 = 0.6836\nLongDocFACTScore for sum: -4.444999371256147\nLongDocFACTScore for abstract: -4.511798596382141\n\n=== Статья 19 ===\nТокенов в статье: 9629\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/9 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"62d87463726e40eaa5a447f62ef7ea31"}},"metadata":{}},{"name":"stdout","text":"\nSummary article number 19\nТокенов в summary: 621\nThe article presents TriMLP, a novel sequential recommendation model that addresses the fundamental incompatibility between fully-connected MLP architectures and auto-regressive training paradigms. While recent all-MLP models like MLP-Mixer and ResMLP have gained popularity for their computational efficiency, empirical evaluations on 12 public datasets—including Beauty, Sports, MovieLens, and LBSN benchmarks—reveal that standard MLPs underperform significantly, lagging NextItNet by up to 28.19% in HR@10. This degradation is attributed to the fully-connected design enabling antichronological interactions (future-to-past), which violates the autoregressive training principle where tokens should only attend to themselves and previous interactions. The authors demonstrate that MLPs are insensitive to sequence order, confirming their inability to model chronological user preferences effectively.\n\nTo resolve this, TriMLP introduces a triangular mixer architecture that enforces strict chronological interaction through masking lower-triangular weights, preventing future information leakage. The model employs two mixing layers: a global mixer for long-term preference modeling and a local mixer for short-term patterns via non-overlapping sessions of length *l*. The triangular structure ensures that each token interacts only with earlier or itself, preserving autoregressive training while maintaining global receptive fields. The mixing kernels are transformed into probability distributions via Softmax, enabling dropout and ensuring equal initial contributions through 1-0 initialization. Unlike self-attention, TriMLP uses static, independent weights, reducing parameters and computational overhead without positional encoding, which is redundant and detrimental in this context.\n\nExperiments show TriMLP achieves state-of-the-art performance, outperforming baselines (GRU4Rec, CNN-based, Transformer-based, and all-MLP variants) by an average of 15.57% in HR@5 and 18.35% in HR@10 across all datasets. It consistently surpasses NextItNet, the strongest baseline, and achieves significant gains in NDCG and HR metrics. Crucially, TriMLP offers superior efficiency: it reduces inference time by up to 23.73% and cuts MACs by 83.51%, with gains increasing with dataset scale. Ablation studies confirm that the triangular design is essential—SqrMLP (fully-connected) degrades performance by 77.99%, while both global and local mixing layers outperform baseline models. The serial global-to-local mixing structure yields the best results, with local sessions optimized per dataset (e.g., s=8 on NYC, s=1 on Gowalla). Cross-session variants degrade due to prior information leakage and pattern distortion. Auto-encoding experiments further validate the superiority of unidirectional, auto-regressive training in sparse sequential data.\n\nIn conclusion, the paper establishes that standard fully-connected MLPs are incompatible with autoregressive sequential modeling. TriMLP provides a simple, efficient, and effective alternative that enables MLPs to capture temporal dynamics through chronological token interactions, achieving competitive performance with significantly lower computational cost.\nТекст добавлен в файл: /kaggle/working/all_summaries.txt\nТекст добавлен в файл: /kaggle/working/all_summaries.txt\nТекст добавлен в файл: /kaggle/working/all_summaries.txt\nROUGE Metrics:\nrouge1: 0.1181\nrouge2: 0.0426\nrougeL: 0.0608\nrougeLsum: 0.1016\n\nBERTScore: F1 = 0.6927\nLongDocFACTScore for sum: -5.135932644208272\nLongDocFACTScore for abstract: -4.661999988555908\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"print(\"\\n--- ROUGE Metrics (средние значения) ---\")\nfor key in rouge_list[0].keys():\n    avg = sum(r[key] for r in rouge_list) / len(rouge_list)\n    print(f\"{key}: {avg:.4f}\")\n\navg_bert = sum(bert_list) / len(bert_list)\nprint(f\"\\n--- BERTScore F1 (среднее): {avg_bert:.4f} ---\")\n\navg_sum = sum(l['for_summary'] for l in ldfacts_list) / len(ldfacts_list)\navg_abs = sum(l['for_abstract'] for l in ldfacts_list) / len(ldfacts_list)\n\nprint(f\"\\n--- LongDocFACTScore (средние) ---\")\nprint(f\"Для суммаризаций: {avg_sum:.4f}\")\nprint(f\"Для абстрактов: {avg_abs:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T06:57:07.480052Z","iopub.execute_input":"2026-01-10T06:57:07.480392Z","iopub.status.idle":"2026-01-10T06:57:07.486649Z","shell.execute_reply.started":"2026-01-10T06:57:07.480345Z","shell.execute_reply":"2026-01-10T06:57:07.486032Z"}},"outputs":[{"name":"stdout","text":"\n--- ROUGE Metrics (средние значения) ---\nrouge1: 0.1151\nrouge2: 0.0510\nrougeL: 0.0641\nrougeLsum: 0.0965\n\n--- BERTScore F1 (среднее): 0.6845 ---\n\n--- LongDocFACTScore (средние) ---\nДля суммаризаций: -4.6309\nДля абстрактов: -4.7052\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"pd.DataFrame(summaries_list).to_csv('summaries.csv')\n\ndata_load = pd.read_csv('/kaggle/working/summaries.csv')\ndata_load.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T07:09:33.13316Z","iopub.execute_input":"2026-01-10T07:09:33.133435Z","iopub.status.idle":"2026-01-10T07:09:33.178023Z","shell.execute_reply.started":"2026-01-10T07:09:33.133415Z","shell.execute_reply":"2026-01-10T07:09:33.177455Z"}},"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"   Unnamed: 0  article_id                                      original_text  \\\n0           0           0  I. INTRODUCTION AND PROBLEM FORMULATIONS\\n\\nWe...   \n1           1           1  INTRODUCTION\\n\\nNeural network models have rec...   \n2           2           2  INTRODUCTION\\n\\nWord embeddings-continuous-val...   \n3           3           3  Introduction\\n\\nGaussian processes (GPs) const...   \n4           4           4  Introduction\\n\\nSolving systems of multivariat...   \n\n                                            abstract  \\\n0  this paper discusses model order reduction of ...   \n1  recurrent neural networks have been very succe...   \n2  recent work has begun exploring neural acousti...   \n3  spectral mixture (sm) kernels comprise a power...   \n4  recently chen and gao~\\cite{chengao2017} propo...   \n\n                                             summary  \n0  This article addresses the challenge of model ...  \n1  This article proposes a novel loss framework a...  \n2  This paper proposes a multi-view learning fram...  \n3  This paper proposes a novel structured station...  \n4  The article addresses the problem of solving s...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>article_id</th>\n      <th>original_text</th>\n      <th>abstract</th>\n      <th>summary</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>I. INTRODUCTION AND PROBLEM FORMULATIONS\\n\\nWe...</td>\n      <td>this paper discusses model order reduction of ...</td>\n      <td>This article addresses the challenge of model ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>1</td>\n      <td>INTRODUCTION\\n\\nNeural network models have rec...</td>\n      <td>recurrent neural networks have been very succe...</td>\n      <td>This article proposes a novel loss framework a...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>2</td>\n      <td>INTRODUCTION\\n\\nWord embeddings-continuous-val...</td>\n      <td>recent work has begun exploring neural acousti...</td>\n      <td>This paper proposes a multi-view learning fram...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>3</td>\n      <td>Introduction\\n\\nGaussian processes (GPs) const...</td>\n      <td>spectral mixture (sm) kernels comprise a power...</td>\n      <td>This paper proposes a novel structured station...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>4</td>\n      <td>Introduction\\n\\nSolving systems of multivariat...</td>\n      <td>recently chen and gao~\\cite{chengao2017} propo...</td>\n      <td>The article addresses the problem of solving s...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":32}]}