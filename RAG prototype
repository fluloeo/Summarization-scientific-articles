{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14601595,"sourceType":"datasetVersion","datasetId":9326791}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/fluloeo/rag-prototype?scriptVersionId=295126842\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"!pip install \"protobuf<5\" vllm transformers chromadb langchain -U -q","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T11:01:55.720287Z","iopub.execute_input":"2026-01-31T11:01:55.720491Z","iopub.status.idle":"2026-01-31T11:08:02.22392Z","shell.execute_reply.started":"2026-01-31T11:01:55.72047Z","shell.execute_reply":"2026-01-31T11:08:02.22315Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.0/52.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.9/87.9 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m509.2/509.2 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m192.6/192.6 kB\u001b[0m \u001b[31m142.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m63.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.0/111.0 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.4/45.4 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m58.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m899.7/899.7 MB\u001b[0m \u001b[31m937.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.0/8.0 MB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.9/34.9 MB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:02\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.3/322.3 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.7/124.7 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.5/170.5 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m47.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.8/108.8 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m397.9/397.9 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.2/96.2 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m45.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m490.2/490.2 kB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m157.4/157.4 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.7/105.7 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 MB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.0/64.0 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.7/149.7 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.5/66.5 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m456.8/456.8 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m285.4/285.4 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.0/149.0 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.9/224.9 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m517.7/517.7 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.3/46.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 MB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.6/58.6 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.3/74.3 kB\u001b[0m \u001b[31m78.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.7/320.7 kB\u001b[0m \u001b[31m946.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m342.0/342.0 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.6/212.6 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.1/16.1 MB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m821.6/821.6 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m959.8/959.8 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngoogle-adk 1.22.1 requires google-cloud-bigquery-storage>=2.0.0, which is not installed.\nbigframes 2.26.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\na2a-sdk 0.3.22 requires protobuf>=5.29.5, but you have protobuf 4.25.8 which is incompatible.\ngoogle-adk 1.22.1 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.27.0 which is incompatible.\ngoogle-adk 1.22.1 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.27.0 which is incompatible.\nopentelemetry-exporter-gcp-logging 1.11.0a0 requires opentelemetry-api>=1.35.0, but you have opentelemetry-api 1.27.0 which is incompatible.\nopentelemetry-exporter-gcp-logging 1.11.0a0 requires opentelemetry-sdk<1.39.0,>=1.35.0, but you have opentelemetry-sdk 1.27.0 which is incompatible.\ngoogle-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.47.0 which is incompatible.\ngoogle-colab 1.0.0 requires jupyter-server==2.14.0, but you have jupyter-server 2.12.5 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\ncudf-cu12 25.6.0 requires cuda-python<13.0a0,>=12.6.2, but you have cuda-python 13.1.1 which is incompatible.\ncudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ngradio 5.49.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.5 which is incompatible.\ncuml-cu12 25.6.0 requires cuda-python<13.0a0,>=12.6.2, but you have cuda-python 13.1.1 which is incompatible.\nydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\nbigframes 2.26.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\npylibraft-cu12 25.6.0 requires cuda-python<13.0a0,>=12.6.2, but you have cuda-python 13.1.1 which is incompatible.\nopentelemetry-exporter-gcp-trace 1.10.0 requires opentelemetry-api~=1.30, but you have opentelemetry-api 1.27.0 which is incompatible.\nopentelemetry-exporter-gcp-trace 1.10.0 requires opentelemetry-sdk~=1.30, but you have opentelemetry-sdk 1.27.0 which is incompatible.\nopentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-exporter-otlp-proto-common==1.37.0, but you have opentelemetry-exporter-otlp-proto-common 1.27.0 which is incompatible.\nopentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-proto==1.37.0, but you have opentelemetry-proto 1.27.0 which is incompatible.\nopentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.27.0 which is incompatible.\nopentelemetry-resourcedetector-gcp 1.10.0a0 requires opentelemetry-api~=1.30, but you have opentelemetry-api 1.27.0 which is incompatible.\nopentelemetry-resourcedetector-gcp 1.10.0a0 requires opentelemetry-sdk~=1.30, but you have opentelemetry-sdk 1.27.0 which is incompatible.\ncuvs-cu12 25.6.1 requires cuda-python<13.0a0,>=12.6.2, but you have cuda-python 13.1.1 which is incompatible.\nopentelemetry-exporter-gcp-monitoring 1.10.0a0 requires opentelemetry-api~=1.30, but you have opentelemetry-api 1.27.0 which is incompatible.\nopentelemetry-exporter-gcp-monitoring 1.10.0a0 requires opentelemetry-sdk~=1.30, but you have opentelemetry-sdk 1.27.0 which is incompatible.\nfastai 2.8.4 requires fastcore<1.9,>=1.8.0, but you have fastcore 1.11.3 which is incompatible.\nfastai 2.8.4 requires torch<2.9,>=1.10, but you have torch 2.9.1 which is incompatible.\nrmm-cu12 25.6.0 requires cuda-python<13.0a0,>=12.6.2, but you have cuda-python 13.1.1 which is incompatible.\ngrpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.8 which is incompatible.\npylibcudf-cu12 25.6.0 requires cuda-python<13.0a0,>=12.6.2, but you have cuda-python 13.1.1 which is incompatible.\npylibcudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.10.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## Преобразование статей из postgres в векторную бд chroma для прототипирования RAG","metadata":{}},{"cell_type":"markdown","source":"я пока не задумывался над тем как там все векторизуется и фрагментируется на чанки","metadata":{}},{"cell_type":"code","source":"import psycopg2\nfrom kaggle_secrets import UserSecretsClient\nimport chromadb\nfrom IPython.display import display, Markdown\nimport numpy as np \nimport pandas as pd \nimport os\nimport ast\nfrom tqdm import tqdm\nfrom typing import List, Dict, Tuple\nfrom vllm import LLM, SamplingParams\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom langchain_core.runnables import RunnableLambda\nfrom transformers import AutoTokenizer\nimport torch\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n\nsecrets = UserSecretsClient()\ndb_params = {\n    \"dbname\": \"arxivdb\",\n    \"user\": secrets.get_secret(\"DB_USER\"),\n    \"password\": secrets.get_secret(\"DB_PASSWORD\"),\n    \"host\": secrets.get_secret(\"DB_HOST\"),\n    \"port\": \"5433\"  \n}\n\nwith psycopg2.connect(**db_params) as conn:\n    with conn.cursor() as cursor:\n        print(\"Выгрузка данных из Postgres...\")\n        cursor.execute(\"SELECT id, title, abstract, clean_text FROM arxivdb.public.articles LIMIT 1000\")\n        rows = cursor.fetchall()\n        \nclient = chromadb.PersistentClient(path=\"./my_rag_db\")\n\ntry:\n    client.delete_collection(name=\"scientific_papers\")\nexcept:\n    pass\ncollection = client.create_collection(name=\"scientific_papers\")\n\ndb_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=1500, \n    chunk_overlap=200,\n    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n)\n\nall_ids = []\nall_documents = []\nall_metadatas = []\n\nprint(\"Разбивка на чанки...\")\nfor row in rows:\n    article_id, title, abstract, full_text = str(row[0]), row[1], row[2], row[3]\n    \n    if not full_text or len(full_text) < 10: \n        continue\n        \n    article_chunks = db_splitter.split_text(full_text)\n    \n    for i, chunk in enumerate(article_chunks):\n        all_ids.append(f\"{article_id}_ch_{i}\")\n        all_documents.append(chunk)\n        all_metadatas.append({\n            \"article_id\": article_id, \n            \"title\": title,\n            \"abstract\": abstract\n        })\n\nprint(f\"Индексация в ChromaDB ({len(all_documents)} фрагментов)...\")\nbatch_size = 1000\nfor i in tqdm(range(0, len(all_documents), batch_size)):\n    collection.add(\n        ids=all_ids[i : i + batch_size],\n        documents=all_documents[i : i + batch_size],\n        metadatas=all_metadatas[i : i + batch_size]\n    )\n\nprint(f\"Готово! Всего в ChromaDB: {collection.count()} записей.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T11:09:25.661188Z","iopub.execute_input":"2026-01-31T11:09:25.66152Z","iopub.status.idle":"2026-01-31T11:28:04.952395Z","shell.execute_reply.started":"2026-01-31T11:09:25.661484Z","shell.execute_reply":"2026-01-31T11:28:04.951508Z"}},"outputs":[{"name":"stdout","text":"Выгрузка данных из Postgres...\nРазбивка на чанки...\nИндексация в ChromaDB (26561 фрагментов)...\n","output_type":"stream"},{"name":"stderr","text":"/root/.cache/chroma/onnx_models/all-MiniLM-L6-v2/onnx.tar.gz: 100%|██████████| 79.3M/79.3M [00:00<00:00, 101MiB/s] \n","output_type":"stream"},{"name":"stdout","text":"Готово! Всего в ChromaDB: 26561 записей.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"Проверка","metadata":{}},{"cell_type":"code","source":"results = collection.get(\n    limit=300, \n    include=[\"metadatas\"]\n)\nunique_articles = {}\nfor meta in results['metadatas']:\n    a_id = meta['article_id']\n    title = meta['title']\n    if a_id not in unique_articles:\n        unique_articles[a_id] = title\n    if len(unique_articles) >= 10:\n        break\nprint(\"--- Список первых 10 уникальных статей в ChromaDB ---\")\nfor i, (a_id, title) in enumerate(unique_articles.items(), 1):\n    print(f\"{i}. [ID: {a_id}] {title}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T11:30:39.410725Z","iopub.execute_input":"2026-01-31T11:30:39.411061Z","iopub.status.idle":"2026-01-31T11:30:39.436229Z","shell.execute_reply.started":"2026-01-31T11:30:39.411033Z","shell.execute_reply":"2026-01-31T11:30:39.435621Z"}},"outputs":[{"name":"stdout","text":"--- Список первых 10 уникальных статей в ChromaDB ---\n1. [ID: 2203.14864] chess is hard even for a single player\n2. [ID: 2203.15173] an evaluation dataset for legal word embedding: a case study on chinese codex\n3. [ID: 2203.15287] accelerating code search with deep hashing and code classification\n4. [ID: 2112.01766] unsupervised low-light image enhancement via histogram equalization prior\n5. [ID: 1311.4818] routing diverse evacuees with cognitive packets\n6. [ID: 1411.1132] are you going to the party: depends, who else is coming? [learning hidden group dynamics via conditional latent tree models]\n7. [ID: 2307.13080] tolerance to asynchrony of an algorithm for gathering myopic robots on an infinite triangular grid\n8. [ID: 2405.09960] a unified deep transfer learning model for accurate iot localization in diverse environments\n9. [ID: 2007.09343] mtl2l: a context aware neural optimiser\n10. [ID: 2503.02526] a theory of initialisation's impact on specialisation\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"## Импорты и загрузка модели","metadata":{}},{"cell_type":"code","source":"\nmodel_id = \"Qwen/Qwen3-4B-Instruct-2507\"\nllm = LLM(\n    model= model_id,\n    tensor_parallel_size=2,  \n    max_model_len=16384,     \n    gpu_memory_utilization=0.85,\n    attention_backend=\"TRITON_ATTN\",\n    dtype=\"float16\" \n)\ntokenizer = AutoTokenizer.from_pretrained(model_id)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-31T11:31:23.013165Z","iopub.execute_input":"2026-01-31T11:31:23.013499Z","iopub.status.idle":"2026-01-31T11:35:16.718971Z","shell.execute_reply.started":"2026-01-31T11:31:23.013472Z","shell.execute_reply":"2026-01-31T11:35:16.71798Z"}},"outputs":[{"name":"stderr","text":"2026-01-31 11:31:31.778721: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1769859092.077988      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1769859092.172199      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1769859092.812409      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1769859092.812448      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1769859092.812451      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1769859092.812454      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"name":"stdout","text":"/kaggle/input/df_dict_test.csv\nINFO 01-31 11:31:50 [utils.py:261] non-default args: {'dtype': 'float16', 'max_model_len': 16384, 'tensor_parallel_size': 2, 'gpu_memory_utilization': 0.85, 'disable_log_stats': True, 'attention_backend': 'TRITON_ATTN', 'model': 'Qwen/Qwen3-4B-Instruct-2507'}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/727 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d3001c7006a241e399e2925e197085dd"}},"metadata":{}},{"name":"stdout","text":"INFO 01-31 11:32:08 [model.py:541] Resolved architecture: Qwen3ForCausalLM\nWARNING 01-31 11:32:08 [model.py:1885] Casting torch.bfloat16 to torch.float16.\nINFO 01-31 11:32:08 [model.py:1561] Using max model len 16384\nINFO 01-31 11:32:08 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=8192.\nINFO 01-31 11:32:08 [vllm.py:624] Asynchronous scheduling is enabled.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/238 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"699c66c579bc43e4ab48ebb0c9633871"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eafd7fd5e040443494e096e8282d9f5c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd02e16115234367bdfefd11c74d06f8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"06eae895be0d4619bfd65b75429edb71"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"abd0c4040ff341e5acd93ed6173144b4"}},"metadata":{}},{"name":"stdout","text":"WARNING 01-31 11:32:11 [system_utils.py:140] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized\n","output_type":"stream"},{"name":"stderr","text":"2026-01-31 11:32:16.567943: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1769859136.595371     315 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1769859136.603003     315 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1769859136.621516     315 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1769859136.621560     315 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1769859136.621564     315 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1769859136.621568     315 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[0;36m(EngineCore_DP0 pid=315)\u001b[0;0m INFO 01-31 11:32:24 [core.py:96] Initializing a V1 LLM engine (v0.15.0) with config: model='Qwen/Qwen3-4B-Instruct-2507', speculative_config=None, tokenizer='Qwen/Qwen3-4B-Instruct-2507', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=16384, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=Qwen/Qwen3-4B-Instruct-2507, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer', 'vllm::rocm_aiter_sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}\n\u001b[0;36m(EngineCore_DP0 pid=315)\u001b[0;0m WARNING 01-31 11:32:24 [multiproc_executor.py:910] Reducing Torch parallelism from 2 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n","output_type":"stream"},{"name":"stderr","text":"2026-01-31 11:32:29.504336: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1769859149.529437     340 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1769859149.536925     340 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1769859149.555835     340 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1769859149.555869     340 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1769859149.555872     340 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1769859149.555874     340 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n2026-01-31 11:32:29.641981: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1769859149.667350     339 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1769859149.674958     339 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1769859149.692961     339 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1769859149.692991     339 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1769859149.692996     339 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1769859149.692999     339 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"name":"stdout","text":"INFO 01-31 11:32:40 [parallel_state.py:1212] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:55865 backend=nccl\nINFO 01-31 11:32:40 [parallel_state.py:1212] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:55865 backend=nccl\nINFO 01-31 11:32:40 [pynccl.py:111] vLLM is using nccl==2.27.5\nWARNING 01-31 11:32:41 [symm_mem.py:67] SymmMemCommunicator: Device capability 7.5 not supported, communicator is not available.\nWARNING 01-31 11:32:41 [symm_mem.py:67] SymmMemCommunicator: Device capability 7.5 not supported, communicator is not available.\nWARNING 01-31 11:32:41 [custom_all_reduce.py:165] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.\nWARNING 01-31 11:32:41 [custom_all_reduce.py:165] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.\nINFO 01-31 11:32:41 [parallel_state.py:1423] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank N/A\nINFO 01-31 11:32:41 [parallel_state.py:1423] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A\n\u001b[0;36m(Worker_TP0 pid=339)\u001b[0;0m INFO 01-31 11:32:42 [gpu_model_runner.py:4021] Starting to load model Qwen/Qwen3-4B-Instruct-2507...\n\u001b[0;36m(Worker_TP0 pid=339)\u001b[0;0m INFO 01-31 11:32:42 [cuda.py:328] Using AttentionBackendEnum.TRITON_ATTN backend.\n\u001b[0;36m(Worker_TP1 pid=340)\u001b[0;0m INFO 01-31 11:32:42 [cuda.py:328] Using AttentionBackendEnum.TRITON_ATTN backend.\n\u001b[0;36m(Worker_TP0 pid=339)\u001b[0;0m INFO 01-31 11:33:37 [weight_utils.py:527] Time spent downloading weights for Qwen/Qwen3-4B-Instruct-2507: 26.680979 seconds\n","output_type":"stream"},{"name":"stderr","text":"Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]\nLoading safetensors checkpoint shards:  33% Completed | 1/3 [00:10<00:21, 10.59s/it]\nLoading safetensors checkpoint shards:  67% Completed | 2/3 [00:12<00:05,  5.75s/it]\nLoading safetensors checkpoint shards: 100% Completed | 3/3 [00:13<00:00,  3.18s/it]\nLoading safetensors checkpoint shards: 100% Completed | 3/3 [00:13<00:00,  4.36s/it]\n\u001b[0;36m(Worker_TP0 pid=339)\u001b[0;0m \n","output_type":"stream"},{"name":"stdout","text":"\u001b[0;36m(Worker_TP0 pid=339)\u001b[0;0m INFO 01-31 11:33:50 [default_loader.py:291] Loading weights took 13.07 seconds\n\u001b[0;36m(Worker_TP0 pid=339)\u001b[0;0m INFO 01-31 11:33:51 [gpu_model_runner.py:4118] Model loading took 3.87 GiB memory and 68.235243 seconds\n\u001b[0;36m(Worker_TP0 pid=339)\u001b[0;0m INFO 01-31 11:34:09 [backends.py:805] Using cache directory: /root/.cache/vllm/torch_compile_cache/940cfad38a/rank_0_0/backbone for vLLM's torch.compile\n\u001b[0;36m(Worker_TP0 pid=339)\u001b[0;0m INFO 01-31 11:34:09 [backends.py:865] Dynamo bytecode transform time: 15.96 s\n","output_type":"stream"},{"name":"stderr","text":"\u001b[0;36m(Worker_TP0 pid=339)\u001b[0;0m [rank0]:W0131 11:34:20.180000 339 torch/_inductor/utils.py:1613] Not enough SMs to use max_autotune_gemm mode\n\u001b[0;36m(Worker_TP1 pid=340)\u001b[0;0m [rank1]:W0131 11:34:22.146000 340 torch/_inductor/utils.py:1613] Not enough SMs to use max_autotune_gemm mode\n","output_type":"stream"},{"name":"stdout","text":"\u001b[0;36m(Worker_TP0 pid=339)\u001b[0;0m INFO 01-31 11:34:30 [backends.py:302] Cache the graph of compile range (1, 8192) for later use\n\u001b[0;36m(Worker_TP1 pid=340)\u001b[0;0m INFO 01-31 11:34:30 [backends.py:302] Cache the graph of compile range (1, 8192) for later use\n\u001b[0;36m(Worker_TP0 pid=339)\u001b[0;0m INFO 01-31 11:34:42 [backends.py:319] Compiling a graph for compile range (1, 8192) takes 25.25 s\n\u001b[0;36m(Worker_TP0 pid=339)\u001b[0;0m INFO 01-31 11:34:42 [monitor.py:34] torch.compile takes 41.21 s in total\n\u001b[0;36m(Worker_TP0 pid=339)\u001b[0;0m INFO 01-31 11:34:45 [gpu_worker.py:356] Available KV cache memory: 7.05 GiB\n\u001b[0;36m(EngineCore_DP0 pid=315)\u001b[0;0m INFO 01-31 11:34:45 [kv_cache_utils.py:1307] GPU KV cache size: 102,720 tokens\n\u001b[0;36m(EngineCore_DP0 pid=315)\u001b[0;0m INFO 01-31 11:34:45 [kv_cache_utils.py:1312] Maximum concurrency for 16,384 tokens per request: 6.27x\n","output_type":"stream"},{"name":"stderr","text":"Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:08<00:00,  6.24it/s]\nCapturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:15<00:00,  2.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"\u001b[0;36m(Worker_TP0 pid=339)\u001b[0;0m INFO 01-31 11:35:10 [gpu_model_runner.py:5051] Graph capturing finished in 25 secs, took 1.28 GiB\n\u001b[0;36m(EngineCore_DP0 pid=315)\u001b[0;0m INFO 01-31 11:35:10 [core.py:272] init engine (profile, create kv cache, warmup model) took 78.06 seconds\n\u001b[0;36m(EngineCore_DP0 pid=315)\u001b[0;0m INFO 01-31 11:35:15 [vllm.py:624] Asynchronous scheduling is enabled.\nINFO 01-31 11:35:15 [llm.py:343] Supported tasks: ['generate']\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"## Проверка запуска модели","metadata":{}},{"cell_type":"code","source":"# sampling_params = SamplingParams(temperature = 0, max_tokens=512)\n# messages = [{\"role\": \"user\", \"content\": \"Кратко расскажи о больших языковых моделях.\"}]\n\n# prompt = tokenizer.apply_chat_template(\n#     messages, \n#     tokenize=False, \n#     add_generation_prompt=True\n# )\n\n# outputs = llm.generate([prompt], sampling_params)\n# for output in outputs:\n#     print(f\"Ответ: {output.outputs[0].text}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T10:42:45.854902Z","iopub.execute_input":"2026-01-31T10:42:45.855642Z","iopub.status.idle":"2026-01-31T10:42:45.862414Z","shell.execute_reply.started":"2026-01-31T10:42:45.855593Z","shell.execute_reply":"2026-01-31T10:42:45.860955Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"## Минимальная обработка чанков датасета (здесь не используется, так как я взял из бд полные текста статей)","metadata":{}},{"cell_type":"code","source":"def to_full_text(data_dict):\n    \"\"\"Превращает словарь разделов в единый текст.\"\"\"\n    return \"\\n\\n\".join([f\"{k}\\n{v}\" for k, v in data_dict.items()])\n\ndef get_token_length(text: str) -> int:\n    return len(tokenizer.encode(text))\n\n# def merge_small_chunks_by_tokens(chunks: List[str], min_tokens: int) -> List[str]:\n#     \"\"\"\n#     Объединяет чанки, если их длина в ТОКЕНАХ меньше min_tokens.\n#     Слияние происходит с наименьшим соседом (по количеству токенов).\n#     \"\"\"\n#     processed_chunks = chunks[:]\n#     separator = \" \" \n    \n#     i = 0\n#     while i < len(processed_chunks):\n#         current_chunk = processed_chunks[i]\n#         current_len = get_token_length(current_chunk)\n#         if current_len >= min_tokens:\n#             i += 1\n#             continue\n#         if len(processed_chunks) == 1:\n#             break\n#         if i > 0:\n#             left_len = get_token_length(processed_chunks[i-1])\n#         else:\n#             left_len = float('inf')\n\n#         if i < len(processed_chunks) - 1:\n#             right_len = get_token_length(processed_chunks[i+1])\n#         else:\n#             right_len = float('inf')\n#         if left_len < right_len:\n#             processed_chunks[i-1] = processed_chunks[i-1] + separator + processed_chunks[i]\n#             processed_chunks.pop(i)\n#             i -= 1 \n#         else:\n#             processed_chunks[i] = processed_chunks[i] + separator + processed_chunks[i+1]\n#             processed_chunks.pop(i+1)\n#     return processed_chunks\n    \n# def merge_chunks_in_dataset(df, min_tokens=100, n=100, n_threshold = 10):\n#     data = pd.DataFrame(columns=['chunks'])\n#     max_len_chunks = []\n#     max_size_chunks = []\n#     min_size_chunks = []\n#     for i in range(n):\n#         data_dict = ast.literal_eval(df['dict_test'].iloc[i])\n#         tokens_chunks_list_0 = list(map(lambda x: get_token_length(x), list(data_dict.values())))\n#         if len(tokens_chunks_list_0)>n_threshold or min(tokens_chunks_list_0)<min_tokens:\n#             final_chunks = merge_small_chunks_by_tokens(list(data_dict.values()), min_tokens=min_tokens)\n#             # print(f\"\\nКонечное количество чанков: {len(final_chunks)}\\n\")\n#             tokens_chunks_list = list(map(lambda x: get_token_length(x), final_chunks))\n#             # print('Максимальный размер чанка',max(tokens_chunks_list),'\\n')\n#             max_len_chunks.append(len(final_chunks))\n#             max_size_chunks.append(max(tokens_chunks_list))\n#             min_size_chunks.append(min(tokens_chunks_list))\n#             data.loc[len(data)] = [final_chunks]\n#         else:\n#             print(\"Ничего не меняем, кол-во чанков, мин токенов\", len(tokens_chunks_list_0),min(tokens_chunks_list_0))\n#             data.loc[len(data)] = [data_dict]\n#     print('Максимальное Кол-во чанков из всех статей\\n',(max(max_len_chunks)))\n#     print('Минимальное Кол-во чанков из всех статей\\n',(min(max_len_chunks)))\n#     print('Максимальный размер чанка из всех статей\\n',max(max_size_chunks))\n#     print('Минимальный размер чанка из всех статей\\n',min(min_size_chunks))\n#     return data\n# df = pd.read_csv('/kaggle/input/df-dict-test/df_dict_test.csv')\n# data = merge_chunks_in_dataset(df=df, min_tokens=700, n=20,n_threshold = 10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T11:35:29.0585Z","iopub.execute_input":"2026-01-31T11:35:29.05924Z","iopub.status.idle":"2026-01-31T11:35:29.069619Z","shell.execute_reply.started":"2026-01-31T11:35:29.059194Z","shell.execute_reply":"2026-01-31T11:35:29.068052Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def fetch_article_by_id(article_id: str) -> Dict:\n    \"\"\"Получает полные данные статьи из PostgreSQL.\"\"\"\n    with psycopg2.connect(**db_params) as conn:\n        with conn.cursor() as cur:\n            cur.execute(\"SELECT id, title, abstract, clean_text FROM articles WHERE id = %s\", (article_id,))\n            row = cur.fetchone()\n            if row:\n                return {\"id\": row[0], \"title\": row[1], \"abstract\": row[2], \"text\": row[3]}\n    return None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T11:35:33.445212Z","iopub.execute_input":"2026-01-31T11:35:33.445646Z","iopub.status.idle":"2026-01-31T11:35:33.453414Z","shell.execute_reply.started":"2026-01-31T11:35:33.445601Z","shell.execute_reply":"2026-01-31T11:35:33.452556Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def vllm_gen(prompts, max_new_tokens):\n    \"\"\"Принимает список промптов и лимит токенов.\"\"\"\n    if not prompts: return []\n    current_params = SamplingParams(\n        temperature=0, \n        max_tokens=max_new_tokens\n    )\n    \n    outputs = llm.generate(prompts, current_params)\n    return [output.outputs[0].text for output in outputs]\n\ndef process_chunks(pre_parsed_chunks, max_chunk_tokens=3000):\n    \"\"\"\n    Проверяет готовые чанки. Если чанк слишком большой, дробит его.\n    \"\"\"\n    splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n        tokenizer=tokenizer,\n        chunk_size=max_chunk_tokens,\n        chunk_overlap=200\n    )\n    \n    final_chunks = []\n    for chunk in pre_parsed_chunks:\n        token_count = len(tokenizer.encode(chunk))\n        if token_count <= max_chunk_tokens:\n            final_chunks.append(chunk)\n        else:\n            sub_chunks = splitter.split_text(chunk)\n            final_chunks.extend(sub_chunks)\n            \n    return final_chunks\n\ndef prepare_prompt(text, system_prompt):\n    messages = [{\"role\": \"user\", \"content\": f\"{system_prompt}:\\n\\n{text}\"}]\n    return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n\ndef run_summarization_pipeline(pre_parsed_chunks, p0, p1):\n    ready_chunks = process_chunks(pre_parsed_chunks, max_chunk_tokens=3500)\n\n    map_prompts = [prepare_prompt(c, p0) for c in ready_chunks]\n    chunk_summaries = gen_chunks_runnable.invoke(map_prompts)\n    \n    combined_text = \"\\n\".join(chunk_summaries)\n    \n    reduce_prompt = prepare_prompt(combined_text, p1)\n    final_summaries = gen_final_runnable.invoke([reduce_prompt])\n    return final_summaries[0]\n\nsummaries_list = []\ngen_chunks_runnable = RunnableLambda(lambda x: vllm_gen(x, max_new_tokens=512))\ngen_final_runnable = RunnableLambda(lambda x: vllm_gen(x, max_new_tokens=1280))\nprompt_0 = \"\"\"Act as a professional Science Editor. \nYour task is to extract key information from this fragment of a scientific paper.\n1. Identify specific objectives, technical methodologies, and key data mentioned.\n2. Maintain technical terminology and the original language.\n3. Be objective: do not add interpretations or info not present in the text.\n4. Output format: Use a dense bulleted list of the most important points.\n5. Avoid any introductory or concluding remarks—just the facts.\"\"\"\nprompt_1 = \"\"\"Act as a Senior Science Editor. \nTask: Synthesize a single, coherent executive summary based on the provided fragment summaries.\n\nStructure your response strictly as follows:\n- **Goal**: Clear statement of the research objective and problem.\n- **Methods**: Summary of the technical approach, experiments, or theoretical framework.\n- **Results**: Detailed overview of key findings and performance improvements.\n- **Conclusion**: The overall significance and final takeaway.\n\nRequirements:\n- Style: Academic, professional, and precise.\n- Formulas: Do not use any mathematical formulas or LaTeX. Describe results in words.\n- Length: Aim for 500-650 words. Ensure information density is high.\n- Integrity: Ensure the text ends with a complete, definitive concluding sentence.\n- Do not add any text outside of the headers. Final summary only.\"\"\"\n\n# for i in range(20): #len(data)\n#     torch.cuda.empty_cache()\n#     print(f\"\\n=== Обработка статьи {i} ===\")\n#     current_article_chunks = data['chunks'].iloc[i] \n#     try:\n#         summary = run_summarization_pipeline(current_article_chunks, prompt_0, prompt_1)\n#         article = to_full_text(ast.literal_eval(df['dict_test'].iloc[i]))\n#         print(f\"Статья {i} готова. Итоговых токенов: {len(tokenizer.encode(summary))}\")\n#         summaries_list.append({\n#             'article_id': i,\n#             'summary': summary,\n#             'abstract': df['abstract'].iloc[i],\n#             'article': article\n#         })\n#     except Exception as e:\n#         print(f\"Ошибка на статье {i}: {e}\")\n\n# pd.DataFrame(summaries_list).to_csv('summaries.csv')\n# data_load = pd.read_csv('/kaggle/working/summaries.csv')\n# data_load.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T11:35:41.026254Z","iopub.execute_input":"2026-01-31T11:35:41.027326Z","iopub.status.idle":"2026-01-31T11:35:41.043942Z","shell.execute_reply.started":"2026-01-31T11:35:41.027267Z","shell.execute_reply":"2026-01-31T11:35:41.042947Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"class ScientificSearch:\n    def __init__(self):\n        self.intent_prompt = \"\"\"Analyze the user query. Classify it into one of two categories:\n1. SUMMARIZE: The user wants a summary of a specific article (often mentions an ID or asks for a general summary).\n2. QA: The user is asking a factual question about scientific topics and needs information from multiple sources.\n\nAnswer only with the word 'SUMMARIZE' or 'QA'.\"\"\"\n\n    def classify_intent(self, query: str) -> str:\n        prompt = tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": f\"{self.intent_prompt}\\nQuery: {query}\"}], tokenize=False, add_generation_prompt=True)\n        return vllm_gen([prompt], 10)[0].strip().upper()\n\n    def handle_qa(self, query: str):\n        \"\"\"RAG сценарий: поиск по всей базе ChromaDB.\"\"\"\n        results = collection.query(query_texts=[query], n_results=3)\n        context = \"\\n\\n\".join(results['documents'][0])\n        \n        qa_prompt = f\"Using the following scientific context, answer the question: {query}\\n\\nContext:\\n{context}\"\n        prompt = tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": qa_prompt}], tokenize=False, add_generation_prompt=True)\n        answer = vllm_gen([prompt], 700)[0]\n        \n        sources = [m['title'] for m in results['metadatas'][0]]\n        return answer, sources\n\n    def handle_summarize(self, query: str):\n        # 1. Поиск ближайшего чанка\n        results = collection.query(query_texts=[query], n_results=1)\n        if not results['ids'][0]:\n            return \"No matching articles found in Vector DB.\", []\n\n        # Вытаскиваем статью из метаданных чанка\n        meta = results['metadatas'][0][0]\n        article_id = meta['article_id']\n        title = meta['title']\n        \n        print(f\"[*] Found article: '{title}' (ID: {article_id}). Fetching full text...\")\n        \n        # 2. Получение текста\n        article_data = fetch_article_by_id(str(article_id))\n        \n        if not article_data or not article_data['text']:\n            return f\"Error: Full text for '{title}' not found in Postgres.\", []\n\n        # 3. Суммаризация\n        # Используем RecursiveCharacterTextSplitter для нарезки полного текста\n        full_text_splitter = RecursiveCharacterTextSplitter(chunk_size=3000, chunk_overlap=200)\n        chunks = full_text_splitter.split_text(article_data['text'])\n        \n        print(f\"[*] Summarizing {len(chunks)} chunks...\")\n        summary = run_summarization_pipeline(chunks, prompt_0, prompt_1)\n        \n        return summary, [title]\n\n    def run(self, query: str):\n        intent = self.classify_intent(query)\n        print(f\"[*] Detected Intent: {intent}\")\n        \n        if \"SUMMARIZE\" in intent:\n            return self.handle_summarize(query)\n        else:\n            return self.handle_qa(query)\n\n# --- 4. ЗАПУСК ---\nSearch = ScientificSearch()\n\n# Пример 1: Вопрос (QA / RAG)\nq1 = \"How do proximal gradient algorithms handle errors?\"\nans1, src1 = Search.run(q1)\ndisplay(Markdown(f\"\\n[QA Answer]: {ans1}\\n[Sources]: {src1}\"))\n\n# Пример 2: Суммаризация (Summarization)\nq2 = \"Summarize the paper about style transfer and image robustness.\"\nans2, src2 = Search.run(q2)\ndisplay(Markdown(f\"\\n[Summary]: {ans2}\\n[Article]: {src2}\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T11:43:50.688388Z","iopub.execute_input":"2026-01-31T11:43:50.689244Z","iopub.status.idle":"2026-01-31T11:44:55.643888Z","shell.execute_reply.started":"2026-01-31T11:43:50.689182Z","shell.execute_reply":"2026-01-31T11:44:55.641935Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"faf5330b25974245a74fb69df6ed3f35"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"840d6ac170fd493ba7d7583abc9eeb28"}},"metadata":{}},{"name":"stdout","text":"[*] Detected Intent: QA\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6354671419fb40798aaa8c04f10d694c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a5011e039cd4486a2b860a1f5e8df7a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"\n[QA Answer]: Proximal gradient algorithms **handle errors** by incorporating a **proximal mapping** that stabilizes and regularizes the optimization process, particularly in the presence of noisy or non-ideal data — such as in stochastic or non-convex settings common in reinforcement learning (RL) and large-scale machine learning.\n\nHere’s how they handle errors, based on the provided scientific context:\n\n---\n\n### 1. **Error Mitigation via Proximal Mapping**\nIn proximal gradient methods, each iteration consists of:\n- A **gradient step** (to move in the direction of steepest descent of the smooth part of the objective),\n- A **proximal mapping step** (to handle the non-smooth or structured component of the objective).\n\nThe proximal operator acts as a **regularizing or smoothing mechanism**. It \"corrects\" or dampens the impact of errors in the gradient or in the data by enforcing constraints or promoting sparsity, structure, or stability — for example, in logistic regression or L1-regularized problems.\n\n> This makes the algorithm robust to noise or inaccuracies in the gradient estimates, especially when the gradient is not perfectly known (e.g., in stochastic settings).\n\n---\n\n### 2. **Stability in Stochastic and Non-Convex Settings**\nIn reinforcement learning (RL), data is collected under different policies over time, leading to **evolving TD errors** (a form of error in value estimation). Standard SGD can become unstable due to this drift.\n\nProximal gradient methods **handle such errors** by:\n- Using **damped or proximal iterations** that are inherently more stable than standard gradient descent.\n- Ensuring convergence to a **stationary point** even when the objective is non-convex or the gradients are noisy.\n- Providing **convergence guarantees** under mild assumptions, even when the data distribution changes (a key issue in RL).\n\n> This stability helps the algorithm maintain performance despite persistent or drifting errors in the value function estimates.\n\n---\n\n### 3. **Error Propagation Control via Firm Nonexpansion**\nThe proximal operator is a **firm nonexpansion**, meaning that it contracts distances between points in a controlled way:\n> For any $ x, y $, $ \\|\\text{prox}_{\\lambda f}(x) - \\text{prox}_{\\lambda f}(y)\\| \\leq \\|x - y\\| $\n\nThis property ensures that **errors in the input (e.g., noisy gradients or misestimated values) do not grow unboundedly** during iterations — they are damped or bounded.\n\n---\n\n### 4. **Incremental Updates and Memory of Gradients (e.g., Piag)**\nIn methods like **Proximal Incremental Aggregated Gradient (Piag)**:\n- Only one component function is updated per iteration, reducing per-iteration cost.\n- The algorithm maintains a **memory of past gradients** to approximate the full gradient.\n- This helps **smooth out noisy or erratic gradient signals** (errors) by averaging over time.\n\n> This reduces the impact of transient errors in individual components, leading to more stable and reliable convergence.\n\n---\n\n### Summary: How Proximal Gradient Algorithms Handle Errors\n| Type of Error | How Proximal Gradient Methods Handle It |\n|-------------|----------------------------------------|\n| **Noisy gradients** | Proximal mapping regularizes and smooths gradient updates. |\n| **Drifting data (e.g., in\n[Sources]: ['proximal deterministic policy gradient', 'proximal deterministic policy gradient', 'asynchronous iterations in optimization: new sequence results and sharper algorithmic guarantees']"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a475b1d894bf4060bbf37021a4c7ea9c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9825248dce3a423ea6e0c6ca4ba885b7"}},"metadata":{}},{"name":"stdout","text":"[*] Detected Intent: SUMMARIZE\n[*] Found article: 'stytr$^2$: image style transfer with transformers' (ID: 2105.14576). Fetching full text...\n[*] Summarizing 10 chunks...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"21c07e766bd64f41b9f91523ec3e7584"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Processed prompts:   0%|          | 0/10 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"86e787e6674045eeae715a466cef23f7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b8f6f43d9754b8fa6616e2f657133fd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a92b54aa0344facacc3b42f86750e9c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"\n[Summary]: **Goal**:  \nThe research aims to address the fundamental limitations of existing image style transfer methods—particularly content leakage, loss of fine details, and poor handling of long-range dependencies—by developing a more robust and semantically aware framework that preserves both content structure and style fidelity during stylization. Traditional CNN-based approaches, despite their efficiency, fail to capture global image relationships and exhibit degradation in content quality over repeated stylization, while existing transformer-based methods lack effective positional encoding strategies that are scale-invariant and content-aware.\n\n**Methods**:  \nA novel transformer-based framework named StyTr 2 is proposed, which formulates style transfer as a sequence-to-sequence generation task of image patches. The input content and style images are partitioned into non-overlapping patches of size 8×8, which are then embedded into a sequence of feature vectors. Two separate transformer encoders—dedicated to content and style domains—extract hierarchical, long-range dependencies through multi-head self-attention mechanisms. Content embeddings are enhanced with a content-aware positional encoding (CAPE), which dynamically learns positional information based on semantic content and is invariant to image scale changes. This contrasts with traditional sinusoidal or learnable positional encodings, which suffer from positional deviation under resolution scaling. The transformer decoder generates stylized image patches by attending to both content and style sequences, with outputs refined via a three-layer CNN decoder that performs spatial upscaling. The model is trained using a composite loss function comprising perceptual content loss, perceptual style loss, and identity losses to ensure fidelity to both input content and style references.\n\n**Results**:  \nStyTr 2 achieves superior visual and quantitative performance compared to state-of-the-art methods including AdaIN, IEST, StyleFormer, and ArtFlow. It preserves fine content details and structural integrity even after 20 rounds of repeated stylization, significantly outperforming CNN-based models in mitigating content leakage. Quantitatively, it achieves the lowest content loss and among the lowest style losses, demonstrating balanced alignment with both input content and style. In ablation studies, CAPE is shown to be essential for maintaining consistent stylization across varying resolutions, with no vertical track artifacts even at 512×512 input size. A user study with 100 participants confirms that StyTr 2 is perceived as superior in overall quality, content preservation, and style consistency across all evaluation criteria.\n\n**Conclusion**:  \nStyTr 2 establishes a new benchmark in image style transfer by combining transformer-based global feature modeling with a semantics-aware, scale-invariant positional encoding mechanism, effectively resolving long-standing issues of content degradation and style misalignment. This work provides a foundational advancement in the field, demonstrating that transformer architectures can achieve both high fidelity and robustness in visual generation tasks.\n[Article]: ['stytr$^2$: image style transfer with transformers']"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"# Пример 1: Технический вопрос по оптимизации (Статья №9)\nq1 = \"What is MTL2L and how does it use context for optimization?\"\nans1, src1 = Search.run(q1)\ndisplay(Markdown(f\"### Тест 1: QA (Оптимизация)\\n**Запрос:** {q1}\\n\\n[QA Answer]: {ans1}\\n\\n**Источники:** {src1}\"))\n\n# Пример 2: Вопрос по компьютерному зрению (Статья №4)\nq2 = \"How does histogram equalization prior help in unsupervised low-light image enhancement?\"\nans2, src2 = Search.run(q2)\ndisplay(Markdown(f\"### Тест 2: QA (Computer Vision)\\n**Запрос:** {q2}\\n\\n[QA Answer]: {ans2}\\n\\n**Источники:** {src2}\"))\n\n# Пример 3: Вопрос по робототехнике и алгоритмам (Статья №7)\nq3 = \"What are the challenges of gathering myopic robots on a triangular grid under asynchrony?\"\nans3, src3 = Search.run(q3)\ndisplay(Markdown(f\"### Тест 3: QA (Робототехника)\\n**Запрос:** {q3}\\n\\n[QA Answer]: {ans3}\\n\\n**Источники:** {src3}\"))\n\n# --- ПРОВЕРКА СУММАРИЗАЦИИ ---\n\n# Пример 4: Суммаризация статьи про юриспруденцию (Статья №2)\nq4 = \"Summarize the case study on Chinese codex regarding legal word embeddings.\"\nans4, src4 = Search.run(q4)\ndisplay(Markdown(f\"### Тест 4: Суммаризация (Legal NLP)\\n**Запрос:** {q4}\\n\\n[Summary]: {ans4}\\n\\n**Статья:** {src4}\"))\n\n# Пример 5: Суммаризация статьи про поиск кода (Статья №3)\nq5 = \"Provide a summary of the paper about accelerating code search using deep hashing.\"\nans5, src5 = Search.run(q5)\ndisplay(Markdown(f\"### Тест 5: Суммаризация (Software Engineering)\\n**Запрос:** {q5}\\n\\n[Summary]: {ans5}\\n\\n**Статья:** {src5}\"))\n\n# Пример 6: Сложный вопрос про теорию нейросетей (Статья №10)\nq6 = \"What is the impact of initialization on specialization according to the theory proposed?\"\nans6, src6 = Search.run(q6)\ndisplay(Markdown(f\"### Тест 6: QA (DL Theory)\\n**Запрос:** {q6}\\n\\n[QA Answer]: {ans6}\\n\\n**Источники:** {src6}\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T11:45:41.420046Z","iopub.execute_input":"2026-01-31T11:45:41.420758Z","iopub.status.idle":"2026-01-31T11:48:18.100626Z","shell.execute_reply.started":"2026-01-31T11:45:41.420698Z","shell.execute_reply":"2026-01-31T11:48:18.098916Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8412e1717a9d40ce9023dbac2efc3e74"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72235ce27a9f4575ab7ef8a29041e449"}},"metadata":{}},{"name":"stdout","text":"[*] Detected Intent: QA\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc7df700cbfd477ab10819e4fdb9f5a0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3194c241453a489ca1a4a814f71cd745"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"### Тест 1: QA (Оптимизация)\n**Запрос:** What is MTL2L and how does it use context for optimization?\n\n[QA Answer]: **What is MTL2L and how does it use context for optimization?**\n\n**MTL2L** (Multi-Task Learning with Context-Aware Synapses) is a **context-aware neural meta-optimizer** designed to adapt its optimization strategy dynamically based on the characteristics of the input data or task structure. It builds upon the concept of **HyperNetworks** and leverages **Singular Value Decomposition (SVD)** to create self-modifying synapses that allow the system to \"learn to learn\" in a dataset-agnostic and multi-task manner.\n\n---\n\n### 🔍 What is MTL2L?\n\nMTL2L is a **meta-learning framework** where the optimizer (a neural network) learns how to optimize base learners (e.g., MLPs) across different tasks. Unlike traditional optimizers that use fixed parameters, MTL2L uses **context-sensitive synapses** that change their behavior depending on the input data.\n\nKey components of MTL2L:\n- **Synapses are represented via SVD**: Each synapse is decomposed into:\n  - A **fixed orthonormal matrix** (purple terms): These are constant and represent stable, generalizable components.\n  - An **orange diagonal matrix** of eigenvalues: These are learned and vary based on context.\n- The eigenvalues of the diagonal matrix are **inferred by a hypernetwork** $ N = N(x_t) $, where $ x_t $ is the input data or task context.\n\nThis structure allows MTL2L to **self-modify its internal parameters** in response to the specific structure of the task or dataset.\n\n---\n\n### 📚 How does MTL2L use context for optimization?\n\nMTL2L uses **context** (i.e., the structure of the input data or task) to dynamically adjust its optimization rules through the following mechanism:\n\n1. **Context Input → Hypernetwork Output**  \n   When a new dataset or task is presented (at meta-training or meta-testing), the context $ x_t $ is fed into a **hypernetwork** $ N $.  \n   → The hypernetwork outputs a set of **eigenvalues** that define the diagonal matrix in the SVD decomposition.\n\n2. **Dynamic Synapse Adjustment**  \n   These eigenvalues modify the effective strength and behavior of the synapses (i.e., how weights are updated).  \n   → Different datasets lead to different eigenvalue combinations → different optimization dynamics.\n\n3. **Task Structure Recognition**  \n   During **meta-training**, MTL2L is exposed to various datasets to learn how different task structures (e.g., complexity, distribution, patterns) correspond to specific eigenvalue configurations.  \n   → This enables it to **recognize and generalize** across tasks.\n\n4. **Meta-Testing Adaptation**  \n   During **meta-testing**, when a new unseen dataset is presented:\n   - MTL2L compares the task structure of the new data to those learned during training.\n   - It adjusts the eigenvalues (via the hypernetwork) to match the most similar learned configuration.\n   - This allows it to **update the base learner's weights** using context-aware optimization rules — effectively \"learning to learn\" in a task-specific way.\n\n5. **Multi-Task Learning Support**  \n   MTL2L is naturally suited for multi-task learning because:\n   - Each task has a unique eigenvalue profile.\n   - The hypernetwork learns to map\n\n**Источники:** ['mtl2l: a context aware neural optimiser', 'mtl2l: a context aware neural optimiser', 'mtl2l: a context aware neural optimiser']"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01dd1f7976fe4805b2e3786726df300c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0e2a9224ba84cba83c93b52b7e5def3"}},"metadata":{}},{"name":"stdout","text":"[*] Detected Intent: QA\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5a89e06a81143e0ad1abde88ae056ca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6cb36e55f89f471f9788a2b6a9a236b9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"### Тест 2: QA (Computer Vision)\n**Запрос:** How does histogram equalization prior help in unsupervised low-light image enhancement?\n\n[QA Answer]: The **histogram equalization prior (HEP)** helps in **unsupervised low-light image enhancement** by providing a **strong, generalizable, and perceptually meaningful regularization signal** during the training process. Here's how it works and why it is effective:\n\n---\n\n### 🔍 **How the Histogram Equalization Prior (HEP) Helps:**\n\n1. **Captures Abundant Texture and Luminance Information**  \n   - Histogram equalization enhances dark images by stretching their dynamic range, making them brighter and more visually interpretable.\n   - Although traditional histogram equalization can cause artifacts (e.g., over-exposure, noise amplification), the **feature maps** (e.g., from VGG) of the equalized images still contain **rich texture and luminance details**.\n   - The key observation is that **the feature maps of histogram equalization-enhanced images are perceptually similar to those of ground-truth images** — especially in low-light scenarios.\n\n2. **Provides a Generic Prior for Training**  \n   - Since this similarity holds across many low-light images, HEP acts as a **generalizable prior** that does not require paired clean images or multi-exposure training data.\n   - This makes the method **robust and applicable to diverse real-world images** without relying on expensive or hard-to-obtain training data.\n\n3. **Regularizes Feature Similarity Between Enhanced and Restored Images**  \n   - During training, the model is encouraged to make the **feature maps of the restored (enhanced) image** as similar as possible to those of the **histogram equalization-enhanced version**.\n   - This ensures that the output image not only improves brightness but also **preserves fine textures, structures, and visual coherence** — avoiding unrealistic or over-enhanced regions.\n\n4. **Guides the Decomposition Process (Retinex-based)**  \n   - The enhancement pipeline splits the low-light image into **illumination** and **reflectance** maps.\n   - The HEP helps ensure that the **reflectance map** (which represents the true content of the scene) is restored with **natural textures and realistic details**, while the illumination map is smoothed to avoid noise and artifacts.\n\n5. **Improves Performance Without Supervision**  \n   - By leveraging this prior, the model learns to enhance images in a way that mimics the visual quality of histogram equalization — a common and intuitive enhancement technique — without needing labeled clean images.\n   - This leads to **better brightness recovery** and **preservation of structural details**, outperforming many unsupervised methods.\n\n---\n\n### ✅ Summary:\n> The **histogram equalization prior (HEP)** helps unsupervised low-light image enhancement by **regularizing the feature similarity** between the restored image and a histogram equalization-enhanced version of the input. This leverages the fact that such enhanced images contain rich, perceptually meaningful texture and luminance information. By enforcing this prior, the model learns to produce brighter, more natural-looking images while preserving fine details — all without requiring paired training data or complex exposure setups.\n\nThis makes HEP a **powerful, generalizable, and effective prior** for unsupervised low-light enhancement, especially in real-world scenarios where labeled data is scarce.\n\n**Источники:** ['unsupervised low-light image enhancement via histogram equalization prior', 'unsupervised low-light image enhancement via histogram equalization prior', 'unsupervised low-light image enhancement via histogram equalization prior']"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"997258f530c24b90ab5143bde49f1325"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8472593017074d2fa8546ec7e88ff8ce"}},"metadata":{}},{"name":"stdout","text":"[*] Detected Intent: QA\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"37201efe220b4c7d8f75ecfe5db38479"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0cc5144447b04b1f9c21d210ef7c9bf7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"### Тест 3: QA (Робототехника)\n**Запрос:** What are the challenges of gathering myopic robots on a triangular grid under asynchrony?\n\n[QA Answer]: The challenges of gathering **myopic robots on a triangular grid under asynchrony** can be understood by analyzing the constraints and limitations described in the provided scientific context. Here's a structured breakdown of the key challenges:\n\n---\n\n### 1. **Myopic Sensing Limitation**\n- **Challenge**: Each robot can only detect whether another robot is present at an adjacent vertex (i.e., it has no global view of the configuration).\n- **Implication**: Robots cannot determine their absolute position, the number of robots, or the global structure of the system. This makes coordination extremely difficult.\n- **Impact under asynchrony**: Without global visibility, robots cannot coordinate movements or avoid collisions effectively, especially when actions occur at irregular intervals (asynchrony). They may repeatedly move into or out of conflict zones without knowing if others are doing the same.\n\n---\n\n### 2. **Lack of Communication**\n- **Challenge**: Robots do not exchange information directly. They rely solely on local sensing.\n- **Implication**: There is no way to broadcast messages, share positions, or synchronize decisions.\n- **Impact under asynchrony**: Even if robots move in a coordinated pattern, the absence of communication means that any coordination must emerge purely from local rules and deterministic behavior, which is fragile and error-prone in asynchronous settings.\n\n---\n\n### 3. **Asynchrony in Movement and Decision-Making**\n- **Challenge**: Robots do not operate in synchronized rounds. Their actions (e.g., moving or staying) occur at arbitrary times.\n- **Implication**: The timing of robot movements is unpredictable. A robot may move when another is still waiting, leading to inconsistent configurations and potential divergence.\n- **Impact**: Without synchronization, it is hard to ensure that all robots eventually reach the same point. The system may oscillate or take longer to converge, especially if robots make conflicting decisions due to delayed sensing or movement.\n\n---\n\n### 4. **No Global Coordinates or IDs**\n- **Challenge**: Robots are oblivious — they do not know their position or have unique identifiers.\n- **Implication**: They cannot distinguish between different parts of the grid or track progress toward a meeting point.\n- **Impact under asynchrony**: Without a shared reference frame or identity, robots may repeatedly explore the same areas or move in cycles, making convergence unreliable.\n\n---\n\n### 5. **Need for Global Agreement on an Axis**\n- **Challenge**: All robots must agree on a common axis (direction and orientation) to align their movement.\n- **Implication**: Without this agreement, movement directions could be inconsistent, leading to scattering rather than convergence.\n- **Impact under asynchrony**: If robots reach agreement on the axis at different times, or if one robot changes its axis decision while others don’t, the system may fail to converge.\n\n---\n\n### 6. **Ensuring Convergence Despite Asynchrony**\n- **Challenge**: The algorithm must guarantee that all robots eventually gather at one point, regardless of the timing of individual actions.\n- **Implication**: The system must be robust to delays, missed opportunities, and unpredictable movement sequences.\n- **Impact**: Even though the paper shows **lattice-linearity** (a key property that ensures deterministic convergence under asynchrony), achieving this requires careful design. The challenge lies in proving that despite the lack of synchronization, the system still progresses toward a unique gathering point due to the **total order of impedensable robots** and deterministic behavior.\n\n---\n\n###\n\n**Источники:** ['tolerance to asynchrony of an algorithm for gathering myopic robots on an infinite triangular grid', 'tolerance to asynchrony of an algorithm for gathering myopic robots on an infinite triangular grid', 'tolerance to asynchrony of an algorithm for gathering myopic robots on an infinite triangular grid']"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ddb78a202504c93a2731bdf894636a4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f11483a070a1461585d18dab99435836"}},"metadata":{}},{"name":"stdout","text":"[*] Detected Intent: SUMMARIZE\n[*] Found article: 'an evaluation dataset for legal word embedding: a case study on chinese codex' (ID: 2203.15173). Fetching full text...\n[*] Summarizing 9 chunks...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Adding requests:   0%|          | 0/9 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"87ee7abd0748484cbb72607a2975da4d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Processed prompts:   0%|          | 0/9 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f6555eead9794758b3aeae7cf2fc9250"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fae605558d334e3faaf149cc0dc74734"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e4961e6dcc95441eb028abe815f26f59"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"### Тест 4: Суммаризация (Legal NLP)\n**Запрос:** Summarize the case study on Chinese codex regarding legal word embeddings.\n\n[Summary]: **Goal**:  \nThis study addresses the critical gap in Chinese natural language processing (NLP) by developing a domain-specific word embedding model and a dedicated evaluation benchmark for legal texts. General-purpose word embedding models, evaluated on datasets like Google’s, fail to capture domain-specific linguistic structures—particularly in specialized fields such as law—due to lexical incompatibilities, domain-specific terminology, and the absence of relevant training data. The objective is to create a robust, legally grounded word embedding model and a corresponding evaluation framework that reflects the semantic and structural complexity of legal language.\n\n**Methods**:  \nA corpus of 2,388 Chinese legal code articles was compiled from the \"Laws and Regulations Database of The Republic of China.\" Preprocessing involved sentence segmentation using CKIPTagger, a tool proven superior to Jieba in handling traditional Chinese legal texts. Words with frequency below five were manually reviewed to ensure lexical quality. Two word embedding models were trained using the skip-gram architecture with a window size of seven and minimum token size of one character. Hyperparameters including vector dimension (ranging from 100 to 1,000) and training iterations (100 to 1,000) were systematically varied. Gensim was used for model training and evaluation, with TensorBoard employed to visualize word relevance and guide manual refinement of segmentation and legal term relationships. A Legal Analogical Reasoning Questions Set (LARQS), comprising 1,134 questions organized into five legal relationship categories (\"Prime and Deputy,\" \"Rights and Obligations,\" \"Execute and Staff,\" \"Operation,\" \"Rights\"), was manually constructed through expert review of legal provisions. Model performance was assessed using Mikolov’s algebraic analogical reasoning method, comparing results across three benchmarks: the Google dataset (translated by CKIP), the CA8 dataset, and the newly developed LARQS.\n\n**Results**:  \nThe Chinese legal codex-based word embedding model achieved an average accuracy of 65% on the LARQS benchmark, with peak performance of 67.02% at 850 dimensions. This outperformed the Google translated dataset by 27.58% and the CA8 dataset by 8.2%, demonstrating superior capture of legal semantic relations. Legal relations were found to be ubiquitous in both the legal codex model and the general Chinese Wikipedia model, indicating that domain-specific structures are embedded even in broader corpora. However, the Google benchmark, limited to 14 general linguistic categories, fails to represent legal-specific relationships, rendering it unsuitable for legal NLP evaluation. Accuracy was positively correlated with embedding dimension and training iterations, but performance declined at 1,000 iterations, suggesting optimal training at 100–200 iterations.\n\n**Conclusion**:  \nThis work establishes a novel, domain-specific Chinese legal word embedding model and a dedicated evaluation benchmark, LARQS, that enables more accurate and meaningful assessment of semantic relationships in legal texts. The findings underscore the necessity of domain-specific data and expert-driven dataset construction in Chinese NLP, and lay the foundation for future applications in automated contract review, document classification, and machine translation. The developed model and dataset provide a critical resource for advancing legal AI in Chinese contexts.\n\n**Статья:** ['an evaluation dataset for legal word embedding: a case study on chinese codex']"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b582c17c9534b2b9d4bf785079a4b2e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e044fd3b0554099862c10fe4fe5633c"}},"metadata":{}},{"name":"stdout","text":"[*] Detected Intent: SUMMARIZE\n[*] Found article: 'accelerating code search with deep hashing and code classification' (ID: 2203.15287). Fetching full text...\n[*] Summarizing 10 chunks...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ef4d50830ce4500970240835fec84be"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Processed prompts:   0%|          | 0/10 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4766e20020ab49649f288bb2ba1f83ad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a426812243164627b80ef185ea3634c7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d04629f406184d12b587db93660c24ca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"### Тест 5: Суммаризация (Software Engineering)\n**Запрос:** Provide a summary of the paper about accelerating code search using deep hashing.\n\n[Summary]: **Goal**:  \nThe research addresses the critical challenge of computational inefficiency in deep learning-based code search systems, where full-scan retrieval over large-scale code corpora leads to prohibitively high computational costs. Despite advances in retrieval accuracy, existing models suffer from poor efficiency due to the need to compute pairwise similarities between high-dimensional representation vectors and perform exhaustive searches. The objective is to develop a scalable, efficient retrieval framework that preserves the accuracy of state-of-the-art models while drastically reducing inference time.\n\n**Methods**:  \nThe proposed approach, CoSHC, introduces a two-stage retrieval paradigm—recall and re-rank—integrated with deep hashing and code classification. In the offline stage, code and query embeddings from a baseline deep code search model are clustered using K-Means to form semantic categories. A deep hashing module generates compact binary hash codes from these embeddings, minimizing the discrepancy between Hamming distances in the hash space and Euclidean distances in the original embedding space. To address vanishing gradients in binary hashing, a scaled tanh function approximates the sign function during training. In the online stage, a query is first classified into categories via a category prediction module, which outputs a normalized probability distribution. The number of code candidates per category is dynamically determined based on this distribution, ensuring balanced and relevant recall. Candidates are retrieved via fast Hamming distance computation, then re-ranked using the original model’s cosine similarity to refine final results.\n\n**Results**:  \nCoSHC achieves a 94–95% reduction in total retrieval time across both Python and Java datasets, with over 97% reduction in vector similarity computation and significant gains in sorting efficiency. It retains at least 99.2% of the original models’ R@1, R@5, and R@10 accuracy, outperforming baselines in several cases, particularly on UNIF, RNN, and CodeBERTa. The classification module, though with less than 75% accuracy, improves final retrieval performance by filtering out misaligned or irrelevant candidates. When the recall strategy uses probability-based candidate selection instead of hard category assignment, performance improves substantially, demonstrating that the classification mechanism enhances retrieval quality even with imperfect predictions.\n\n**Conclusion**:  \nCoSHC establishes a novel, generalizable framework that effectively decouples retrieval efficiency from accuracy in deep code search, enabling real-time performance on large-scale codebases without sacrificing precision. It represents a significant advancement in scalable code search by combining deep hashing with intelligent classification, offering a practical and high-performance solution for industrial deployment.\n\n**Статья:** ['accelerating code search with deep hashing and code classification']"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"44ba939b839e447faf5b780c90f8ac98"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc51c66ee96945c5a19dbaab4c26ea77"}},"metadata":{}},{"name":"stdout","text":"[*] Detected Intent: QA\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"12b3133deaf3465caa8a3a521866b45d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f8a62f6c46624f7aaa76a29d19cc1473"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"### Тест 6: QA (DL Theory)\n**Запрос:** What is the impact of initialization on specialization according to the theory proposed?\n\n[QA Answer]: The provided scientific context does **not** contain a direct or coherent theory that links **initialization** to **specialization** in a way that allows for a clear answer to the question: *\"What is the impact of initialization on specialization according to the theory proposed?\"*\n\nLet’s break this down:\n\n- The context discusses **initialization in neural networks**, particularly in the context of **implicit-scheme-informed neural networks** and **generalisation error**.\n- It references how improved initializations can lead to cost reduction in Newton’s method and how neural network approximations (like Φ*τ) of implicit rules (like the midpoint rule) affect generalization performance.\n- There is a discussion of **generalisation error** as a measure of how well an initialisation performs on unseen data — which relates to **initialisation quality**, but not directly to **specialization**.\n- The term **\"specialization\"** is not explicitly defined or used in the context. While the paper \"ProtoPShare\" discusses prototype merging and classification, the term \"specialization\" does not appear in the provided excerpts.\n- The theoretical results focus on function classes (A_D, P_D), factorizations, and generalization bounds — not on how initialization influences the network’s ability to specialize (e.g., learn fine-grained distinctions or adapt to specific tasks).\n\n### Conclusion:\n**There is no explicit theory in the provided context that describes the impact of initialization on specialization.**\n\nTherefore, based solely on the given scientific context:\n\n> ❌ The impact of initialization on specialization according to the theory proposed **cannot be determined** — the context does not contain a theory linking initialization to specialization.\n\nIf you are referring to a different or more specific theory (e.g., in the ProtoPShare paper or in a broader neural network specialization framework), additional context would be needed. As it stands, the provided text lacks the necessary theoretical framework to answer the question.\n\n**Источники:** ['a fast neural hybrid newton solver adapted to implicit methods for nonlinear dynamics', 'relaxations of associativity and preassociativity for variadic functions', 'protopshare: prototype sharing for interpretable image classification and similarity discovery']"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}